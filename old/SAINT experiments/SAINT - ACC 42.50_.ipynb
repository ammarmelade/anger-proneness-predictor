{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNgKQpOG42+4JggzLtXaB8/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Preliminaries"],"metadata":{"id":"6ZiHKXJ0JQ2y"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"],"metadata":{"id":"irMRi0gI9vH_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686511634676,"user_tz":-420,"elapsed":20237,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"ee91fec2-931d-4b6f-fd21-fad85808c732"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wF_EZuxx8-UB","executionInfo":{"status":"ok","timestamp":1686511638636,"user_tz":-420,"elapsed":3979,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"82e292da-426b-42b4-c974-9a117c93ba61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'saint'...\n","remote: Enumerating objects: 664, done.\u001b[K\n","remote: Total 664 (delta 0), reused 0 (delta 0), pack-reused 664\u001b[K\n","Receiving objects: 100% (664/664), 17.00 MiB | 13.06 MiB/s, done.\n","Resolving deltas: 100% (364/364), done.\n"]}],"source":["!git clone https://github.com/ogunlao/saint.git"]},{"cell_type":"code","source":["!pip install pytorch-lightning==1.3.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZ4VpFXm7GDp","executionInfo":{"status":"ok","timestamp":1686511933129,"user_tz":-420,"elapsed":29249,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"8daaf278-a83e-40e9-9087-6f10a1f52078"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-lightning==1.3.2\n","  Downloading pytorch_lightning-1.3.2-py3-none-any.whl (805 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.7/805.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (1.22.4)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2.0.1+cu118)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (0.18.3)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (4.65.0)\n","Collecting PyYAML<=5.4.1,>=5.1 (from pytorch-lightning==1.3.2)\n","  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2023.4.0)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2.12.2)\n","Collecting torchmetrics>=0.2.0 (from pytorch-lightning==1.3.2)\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyDeprecate==0.3.0 (from pytorch-lightning==1.3.2)\n","  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2.27.1)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.54.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.4.3)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->pytorch-lightning==1.3.2) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->pytorch-lightning==1.3.2) (16.0.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->pytorch-lightning==1.3.2) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.2.2)\n","Building wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=529feeaabf25b7c7f6d7a46356df4ab5afde9325370bb1e8ec24e63ba58fc473\n","  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n","Successfully built PyYAML\n","Installing collected packages: PyYAML, pyDeprecate, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","Successfully installed PyYAML-5.4.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 pyDeprecate-0.3.0 pytorch-lightning-1.3.2 torchmetrics-0.11.4 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["# !pip3 install -r \"/content/saint/requirements.txt\""],"metadata":{"id":"0ECBnzW59KNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scikit-learn==0.24.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czKgW7eW7lTi","executionInfo":{"status":"ok","timestamp":1686512351596,"user_tz":-420,"elapsed":418490,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"f9d27d57-20b0-408f-e639-6a7b34639f87"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-learn==0.24.2\n","  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.22.4)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.10.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (3.1.0)\n","Building wheels for collected packages: scikit-learn\n","  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install scipy==1.5.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdZC8Xhx7rEc","executionInfo":{"status":"ok","timestamp":1686512524014,"user_tz":-420,"elapsed":172463,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"3ab6a22c-681e-4647-f5c6-474371bcbcbb"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scipy==1.5.4\n","  Downloading scipy-1.5.4.tar.gz (25.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/25.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"code","source":["!pip install tensorboard==2.4.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"QzrvzYGU7q_C","executionInfo":{"status":"ok","timestamp":1686512539785,"user_tz":-420,"elapsed":15828,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"9a150161-c891-40ab-e533-ea35fe95c054"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboard==2.4.1\n","  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.54.0)\n","Collecting google-auth<2,>=1.6.3 (from tensorboard==2.4.1)\n","  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.4.1)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (3.4.3)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.22.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (2.27.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (67.7.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.16.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (0.40.0)\n","Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard==2.4.1)\n","  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.1) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.1) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.1) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard==2.4.1) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.4.1) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.1) (3.2.2)\n","Installing collected packages: cachetools, google-auth, google-auth-oauthlib, tensorboard\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.3.0\n","    Uninstalling cachetools-5.3.0:\n","      Successfully uninstalled cachetools-5.3.0\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 2.17.3\n","    Uninstalling google-auth-2.17.3:\n","      Successfully uninstalled google-auth-2.17.3\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.2\n","    Uninstalling tensorboard-2.12.2:\n","      Successfully uninstalled tensorboard-2.12.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n","google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 1.35.0 which is incompatible.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cachetools-4.2.4 google-auth-1.35.0 google-auth-oauthlib-0.4.6 tensorboard-2.4.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install tensorboard-plugin-wit==1.8.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wNz-mi0s7q0l","executionInfo":{"status":"ok","timestamp":1686512545473,"user_tz":-420,"elapsed":5728,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"fd5cfa31-74d0-4c94-c7fd-0bb711cec1e2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboard-plugin-wit==1.8.0\n","  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.2/781.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard-plugin-wit\n","  Attempting uninstall: tensorboard-plugin-wit\n","    Found existing installation: tensorboard-plugin-wit 1.8.1\n","    Uninstalling tensorboard-plugin-wit-1.8.1:\n","      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tensorboard-plugin-wit-1.8.0\n"]}]},{"cell_type":"code","source":["!pip install torch==1.8.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sk55Z5-d75xp","executionInfo":{"status":"ok","timestamp":1686512547264,"user_tz":-420,"elapsed":1819,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"53639bc9-8e75-4a29-d463-6f5df05bc954"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.1\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install torchmetrics==0.3.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"832RK6OB78_Y","executionInfo":{"status":"ok","timestamp":1686512554994,"user_tz":-420,"elapsed":7752,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"1f3b9e64-e084-41f9-ec03-849fff94a978"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics==0.3.2\n","  Downloading torchmetrics-0.3.2-py3-none-any.whl (274 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/274.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/274.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.3.2) (2.0.1+cu118)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.3.2) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.1->torchmetrics==0.3.2) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.1->torchmetrics==0.3.2) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.1->torchmetrics==0.3.2) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.1->torchmetrics==0.3.2) (1.3.0)\n","Installing collected packages: torchmetrics\n","  Attempting uninstall: torchmetrics\n","    Found existing installation: torchmetrics 0.11.4\n","    Uninstalling torchmetrics-0.11.4:\n","      Successfully uninstalled torchmetrics-0.11.4\n","Successfully installed torchmetrics-0.3.2\n"]}]},{"cell_type":"code","source":["!pip install torchtext==0.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cBUbpbK7_ji","executionInfo":{"status":"ok","timestamp":1686512562513,"user_tz":-420,"elapsed":7543,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"64664873-04c0-4c19-f101-7c7b74c77142"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.6.0\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.6.0)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.15.2\n","    Uninstalling torchtext-0.15.2:\n","      Successfully uninstalled torchtext-0.15.2\n","Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"]}]},{"cell_type":"code","source":["!pip install torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIYK5adF7_et","executionInfo":{"status":"ok","timestamp":1686512568601,"user_tz":-420,"elapsed":6104,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"31bc02a2-3d66-41d6-98e9-06d02c73662b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install hydra-core"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"S6QGqYdU8Frx","executionInfo":{"status":"ok","timestamp":1686512580186,"user_tz":-420,"elapsed":11605,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"1976a2da-85d4-40d8-9d98-650dc098e33a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hydra-core\n","  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting omegaconf<2.4,>=2.2 (from hydra-core)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (23.1)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (5.4.1)\n","Building wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=d6fd23553251f35bea6e5a04251513a7012aa75c5b20fdaa42c2b8627f9a6704\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n","Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install ruamel_yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZpR1cNo8IRD","executionInfo":{"status":"ok","timestamp":1686512591463,"user_tz":-420,"elapsed":11316,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"ecde7ac3-1875-408d-ec41-87fea57d3754"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ruamel_yaml\n","  Downloading ruamel.yaml-0.17.31-py3-none-any.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel_yaml)\n","  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel_yaml\n","Successfully installed ruamel.yaml.clib-0.2.7 ruamel_yaml-0.17.31\n"]}]},{"cell_type":"code","source":["# !pip install torch --upgrade torch\n","# !pip install torch --upgrade pytorch-lightning"],"metadata":{"id":"RMjvVdfYITxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !nvcc --version"],"metadata":{"id":"WEdpJXk_I_lg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","\n","print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9VEnYWP1KH_Z","executionInfo":{"status":"ok","timestamp":1686512597082,"user_tz":-420,"elapsed":5680,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"44197c26-bc2f-4c00-f1e2-ccafdd88781d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["\n","# Setting up parameters"],"metadata":{"id":"Q5gQs6SaILhx"}},{"cell_type":"code","source":["from ruamel.yaml import YAML "],"metadata":{"id":"VnOHG2rCImIq","executionInfo":{"status":"ok","timestamp":1686512597086,"user_tz":-420,"elapsed":44,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["**IMPORTANT : EDIT FILE-FILE INI DULU SEBELUM RUN**\n","\n","Cara editnya : double-click file yg mau di edit di \"Files\" (tab kiri colab) ato klik link-link dibawah, terus ctrl-s buat save.\n","\n","```1. /content/saint/configs/config.yaml, line 46```\n","\n","```\n","trainer:\n","  max_epochs: 100 # default is 100\n","  # gpus: 0\n","  accelerator: auto\n","  deterministic: true\n","  default_root_dir: null\n","  # resume_from_checkpoint: null\n","```\n","\n","```2. /content/saint/configs/data/bank_ssl.yaml, line 10```\n","```\n","data_stats:\n","  no_cat: 1\n","  no_num: 49\n","  cats: [1]\n","```\n","\n","```3. /content/saint/configs/data/bank_sup.yaml, line 10```\n","\n","```\n","data_stats:\n","  no_cat: 1\n","  no_num: 49\n","  cats: [1]\n","```\n","\n","```4. /content/saint/configs/experiment/supervised.yaml```\n","\n","```\n","experiment: supervised\n","task: classification # {classification, regression}\n","model: saint\n","num_output: 5 # no of output neurons: 1 for binary classification num of classes in target for multiclass}\n","freeze_encoder: false # freeze transformer layer\n","pretrained_checkpoint: null #checkpoints/lightning_logs/version_7/checkpoints/epoch=0-step=1.ckpt\n","```"],"metadata":{"id":"fdYj59RZzVWT"}},{"cell_type":"code","source":["config_path = 'saint/configs/config.yaml'\n","\n","yaml = YAML(typ='safe')\n","with open(config_path) as f:\n","  args = yaml.load(f)\n","\n","print(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3yBXKWlIOjL","executionInfo":{"status":"ok","timestamp":1686512597646,"user_tz":-420,"elapsed":26,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"fe402d77-a90c-464b-d9cf-5b5b5d8600bc"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["{'defaults': ['_self_', {'experiment': 'supervised'}, {'data': 'bank_sup'}], 'seed': 1234, 'transformer': {'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'dropout_ff': 0.1, 'embed_dim': 32, 'd_ff': 32, 'cls_token_idx': 0}, 'augmentation': {'prob_cutmix': 0.3, 'alpha': 0.2, 'lambda_pt': 10}, 'optimizer': {'temperature': 0.7, 'proj_head_dim': 128, 'beta_1': 0.9, 'beta_2': 0.99, 'lr': 0.0001, 'weight_decay': 0.01, 'optim': 'adamw', 'metric': 'auroc'}, 'preproc': {'data_folder': None, 'train_split': 0.65, 'validation_split': 0.15, 'test_split': 0.2, 'num_supervised_train_data': None}, 'callback': {'monitor': 'val_loss', 'mode': 'min', 'auto_insert_metric_name': False}, 'trainer': {'max_epochs': 100, 'gpus': 0, 'deterministic': True, 'default_root_dir': None, 'resume_from_checkpoint': None}, 'dataloader': {'shuffle_val': False, 'train_bs': 32, 'val_bs': 32, 'test_bs': 16, 'num_workers': 2, 'pin_memory': False}, 'metric': '${optimizer.metric}', 'print_config': False}\n"]}]},{"cell_type":"markdown","source":["# Data preprocessing"],"metadata":{"id":"mIFvOAGlIueV"}},{"cell_type":"code","source":["data_folder = \"/content/saint/data\"\n","\n","os.mkdir(\"/content/saint/data\")"],"metadata":{"id":"b2yZLsEkI1Ex","executionInfo":{"status":"ok","timestamp":1686512597648,"user_tz":-420,"elapsed":20,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_csv(\"/content/drive/MyDrive/SEM4/Research Method/RM Kel 19 Experiment/data-final.csv\", sep='\\t')\n","\n","print(dataset.shape)\n","dataset.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"0NCa-gD39lta","executionInfo":{"status":"ok","timestamp":1686512623813,"user_tz":-420,"elapsed":26184,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"b2e722d1-db91-4745-9793-8ea0aed7013f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["(1015341, 110)\n"]},{"output_type":"execute_result","data":{"text/plain":["   EXT1  EXT2  EXT3  EXT4  EXT5  EXT6  EXT7  EXT8  EXT9  EXT10  ...  \\\n","0   4.0   1.0   5.0   2.0   5.0   1.0   5.0   2.0   4.0    1.0  ...   \n","1   3.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0   1.0    5.0  ...   \n","2   2.0   3.0   4.0   4.0   3.0   2.0   1.0   3.0   2.0    5.0  ...   \n","3   2.0   2.0   2.0   3.0   4.0   2.0   2.0   4.0   1.0    4.0  ...   \n","4   3.0   3.0   3.0   3.0   5.0   3.0   3.0   5.0   3.0    4.0  ...   \n","\n","              dateload  screenw  screenh  introelapse  testelapse  endelapse  \\\n","0  2016-03-03 02:01:01    768.0   1024.0          9.0       234.0          6   \n","1  2016-03-03 02:01:20   1360.0    768.0         12.0       179.0         11   \n","2  2016-03-03 02:01:56   1366.0    768.0          3.0       186.0          7   \n","3  2016-03-03 02:02:02   1920.0   1200.0        186.0       219.0          7   \n","4  2016-03-03 02:02:57   1366.0    768.0          8.0       315.0         17   \n","\n","   IPC  country  lat_appx_lots_of_err  long_appx_lots_of_err  \n","0    1       GB               51.5448                 0.1991  \n","1    1       MY                3.1698                101.706  \n","2    1       GB               54.9119                -1.3833  \n","3    1       GB                 51.75                  -1.25  \n","4    2       KE                   1.0                   38.0  \n","\n","[5 rows x 110 columns]"],"text/html":["\n","  <div id=\"df-58fa9dc2-9dd9-40ae-b152-a79b07a00913\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EXT1</th>\n","      <th>EXT2</th>\n","      <th>EXT3</th>\n","      <th>EXT4</th>\n","      <th>EXT5</th>\n","      <th>EXT6</th>\n","      <th>EXT7</th>\n","      <th>EXT8</th>\n","      <th>EXT9</th>\n","      <th>EXT10</th>\n","      <th>...</th>\n","      <th>dateload</th>\n","      <th>screenw</th>\n","      <th>screenh</th>\n","      <th>introelapse</th>\n","      <th>testelapse</th>\n","      <th>endelapse</th>\n","      <th>IPC</th>\n","      <th>country</th>\n","      <th>lat_appx_lots_of_err</th>\n","      <th>long_appx_lots_of_err</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:01</td>\n","      <td>768.0</td>\n","      <td>1024.0</td>\n","      <td>9.0</td>\n","      <td>234.0</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>51.5448</td>\n","      <td>0.1991</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:20</td>\n","      <td>1360.0</td>\n","      <td>768.0</td>\n","      <td>12.0</td>\n","      <td>179.0</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>MY</td>\n","      <td>3.1698</td>\n","      <td>101.706</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:56</td>\n","      <td>1366.0</td>\n","      <td>768.0</td>\n","      <td>3.0</td>\n","      <td>186.0</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>54.9119</td>\n","      <td>-1.3833</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:02:02</td>\n","      <td>1920.0</td>\n","      <td>1200.0</td>\n","      <td>186.0</td>\n","      <td>219.0</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>51.75</td>\n","      <td>-1.25</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:02:57</td>\n","      <td>1366.0</td>\n","      <td>768.0</td>\n","      <td>8.0</td>\n","      <td>315.0</td>\n","      <td>17</td>\n","      <td>2</td>\n","      <td>KE</td>\n","      <td>1.0</td>\n","      <td>38.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 110 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58fa9dc2-9dd9-40ae-b152-a79b07a00913')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-58fa9dc2-9dd9-40ae-b152-a79b07a00913 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-58fa9dc2-9dd9-40ae-b152-a79b07a00913');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["data = dataset.drop(list(dataset)[50:], axis=1)\n","\n","print(data.shape)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"kKU71FbL-CH1","executionInfo":{"status":"ok","timestamp":1686512623821,"user_tz":-420,"elapsed":120,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"d49c29cd-6c75-4ae5-d3a9-56978a7bac56"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["(1015341, 50)\n"]},{"output_type":"execute_result","data":{"text/plain":["   EXT1  EXT2  EXT3  EXT4  EXT5  EXT6  EXT7  EXT8  EXT9  EXT10  ...  OPN1  \\\n","0   4.0   1.0   5.0   2.0   5.0   1.0   5.0   2.0   4.0    1.0  ...   5.0   \n","1   3.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0   1.0    5.0  ...   1.0   \n","2   2.0   3.0   4.0   4.0   3.0   2.0   1.0   3.0   2.0    5.0  ...   5.0   \n","3   2.0   2.0   2.0   3.0   4.0   2.0   2.0   4.0   1.0    4.0  ...   4.0   \n","4   3.0   3.0   3.0   3.0   5.0   3.0   3.0   5.0   3.0    4.0  ...   5.0   \n","\n","   OPN2  OPN3  OPN4  OPN5  OPN6  OPN7  OPN8  OPN9  OPN10  \n","0   1.0   4.0   1.0   4.0   1.0   5.0   3.0   4.0    5.0  \n","1   2.0   4.0   2.0   3.0   1.0   4.0   2.0   5.0    3.0  \n","2   1.0   2.0   1.0   4.0   2.0   5.0   3.0   4.0    4.0  \n","3   2.0   5.0   2.0   3.0   1.0   4.0   4.0   3.0    3.0  \n","4   1.0   5.0   1.0   5.0   1.0   5.0   3.0   5.0    5.0  \n","\n","[5 rows x 50 columns]"],"text/html":["\n","  <div id=\"df-d988beb9-bc6b-4beb-8165-1c5b11910a5a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EXT1</th>\n","      <th>EXT2</th>\n","      <th>EXT3</th>\n","      <th>EXT4</th>\n","      <th>EXT5</th>\n","      <th>EXT6</th>\n","      <th>EXT7</th>\n","      <th>EXT8</th>\n","      <th>EXT9</th>\n","      <th>EXT10</th>\n","      <th>...</th>\n","      <th>OPN1</th>\n","      <th>OPN2</th>\n","      <th>OPN3</th>\n","      <th>OPN4</th>\n","      <th>OPN5</th>\n","      <th>OPN6</th>\n","      <th>OPN7</th>\n","      <th>OPN8</th>\n","      <th>OPN9</th>\n","      <th>OPN10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 50 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d988beb9-bc6b-4beb-8165-1c5b11910a5a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d988beb9-bc6b-4beb-8165-1c5b11910a5a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d988beb9-bc6b-4beb-8165-1c5b11910a5a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["for i in data.columns:\n","  data = data[(data[i].notna()) & (data[i] != 0)]\n","\n","print(data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z75-zCjK-IiC","executionInfo":{"status":"ok","timestamp":1686512632430,"user_tz":-420,"elapsed":8682,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"d7ad9669-216c-476f-b590-9877f52c779f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["(874434, 50)\n"]}]},{"cell_type":"code","source":["data = data.astype(int)"],"metadata":{"id":"v6-PfEfr9vjd","executionInfo":{"status":"ok","timestamp":1686512632431,"user_tz":-420,"elapsed":19,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["data['EST9'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fK-gKGep-GKA","executionInfo":{"status":"ok","timestamp":1686512632432,"user_tz":-420,"elapsed":16,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"02898698-f101-45ca-dc84-49d0e0a0aec9"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4    247851\n","2    199050\n","3    182001\n","5    133152\n","1    112380\n","Name: EST9, dtype: int64"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["data = data[:1000]"],"metadata":{"id":"rOAEEVuG1oC5","executionInfo":{"status":"ok","timestamp":1686512632434,"user_tz":-420,"elapsed":16,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["x = data.drop(columns=['EST9'])\n","y = data['EST9']"],"metadata":{"id":"f9Arorto-KIy","executionInfo":{"status":"ok","timestamp":1686512632435,"user_tz":-420,"elapsed":15,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["y = y - 1"],"metadata":{"id":"QN7RPmy9S8X9","executionInfo":{"status":"ok","timestamp":1686512632436,"user_tz":-420,"elapsed":15,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from saint.src.dataset import generate_splits, preprocess"],"metadata":{"id":"XYfjIiOeJe91","executionInfo":{"status":"ok","timestamp":1686512634026,"user_tz":-420,"elapsed":1604,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["num_supervised_train_data = 200\n","\n","sup_train_indices, val_indices, test_indices, ssl_train_indices = generate_splits(len(x), num_supervised_train_data, args['preproc']['validation_split'], args['preproc']['test_split'], args['seed'],)"],"metadata":{"id":"sBdu-PJLJjt_","executionInfo":{"status":"ok","timestamp":1686512634027,"user_tz":-420,"elapsed":34,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["x_proc, y_proc, no_num, no_cat, cats  = preprocess(x, y, args['transformer']['cls_token_idx'])"],"metadata":{"id":"1ldx0xi8J9mT","executionInfo":{"status":"ok","timestamp":1686512634028,"user_tz":-420,"elapsed":32,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["print('no of numerical columns: ', no_num)\n","print('no of categorical columns: ', no_cat)\n","\n","print('list of categories in each categorical column: ', cats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99TtxxqOKEMK","executionInfo":{"status":"ok","timestamp":1686512634029,"user_tz":-420,"elapsed":31,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"5d31e387-35ec-44f5-8b3c-b86f3bf643e4"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["no of numerical columns:  49\n","no of categorical columns:  1\n","list of categories in each categorical column:  [1]\n"]}]},{"cell_type":"code","source":["train_df, train_y   = x_proc.iloc[sup_train_indices], y_proc.iloc[sup_train_indices]\n","val_df, val_y       = x_proc.iloc[val_indices], y_proc.iloc[val_indices]\n","test_df, test_y     = x_proc.iloc[test_indices], y_proc.iloc[test_indices]"],"metadata":{"id":"zO_CGazRKIlS","executionInfo":{"status":"ok","timestamp":1686512634033,"user_tz":-420,"elapsed":31,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["train_ssl, train_ssl_y = None, None\n","\n","if num_supervised_train_data != 'all':\n","    train_ssl, train_ssl_y = x_proc.iloc[ssl_train_indices], y_proc.iloc[ssl_train_indices]"],"metadata":{"id":"xdM8JYynKciy","executionInfo":{"status":"ok","timestamp":1686512634034,"user_tz":-420,"elapsed":29,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["train_df.to_csv('/content/saint/data/train.csv' , index=False)\n","train_y.to_csv('/content/saint/data/train_y.csv' , index=False)\n","val_df.to_csv('/content/saint/data/val.csv' , index=False)\n","val_y.to_csv('/content/saint/data/val_y.csv' , index=False)\n","test_df.to_csv('/content/saint/data/test.csv' , index=False)\n","test_y.to_csv('/content/saint/data/test_y.csv' , index=False)\n","\n","if train_ssl is not None:\n","   train_ssl.to_csv('/content/saint/data/train_ssl.csv' , index=False)\n","\n","if train_ssl_y is not None:\n","  train_ssl_y.to_csv('/content/saint/data/train_ssl_y.csv' , index=False)"],"metadata":{"id":"xn-T7swhKpmA","executionInfo":{"status":"ok","timestamp":1686512634035,"user_tz":-420,"elapsed":28,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["# SAINT training"],"metadata":{"id":"XK3XHgVCGfOm"}},{"cell_type":"code","source":["num_gpus = 1"],"metadata":{"id":"mqJKcN-ZNr7i","executionInfo":{"status":"ok","timestamp":1686512634035,"user_tz":-420,"elapsed":27,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["os.environ[\"HYDRA_FULL_ERROR\"] = \"1\""],"metadata":{"id":"uMwBMOpT51E3","executionInfo":{"status":"ok","timestamp":1686512634036,"user_tz":-420,"elapsed":27,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["## Self-supervised learning + supervised learning"],"metadata":{"id":"rZyNJ8mpOm6A"}},{"cell_type":"code","source":["# Self-Supervised Learning\n","\n","!python /content/saint/main.py experiment=self-supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_ssl"],"metadata":{"id":"tVpqTt4RLUV7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686513700752,"user_tz":-420,"elapsed":816523,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"913a7c97-e63e-46aa-e9e6-46ea564f256c"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 19:48:07.648750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-11 19:48:08.964488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/main.py:11: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Global seed set to 1234\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","\n","  | Name                | Type            | Params\n","--------------------------------------------------------\n","0 | transformer         | Encoder         | 65.0 K\n","1 | embedding           | Embedding       | 3.2 K \n","2 | contrastive_loss_fn | ContrastiveLoss | 409 K \n","3 | denoising_loss_fn   | DenoisingLoss   | 1.6 K \n","--------------------------------------------------------\n","479 K     Trainable params\n","0         Non-trainable params\n","479 K     Total params\n","1.918     Total estimated model params size (MB)\n","Global seed set to 1234\n","Epoch 0:  75% 15/20 [00:07<00:02,  1.88it/s, loss=624, v_num=0, val_loss_epoch=678.0, train_loss_step=620.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  85% 17/20 [00:08<00:01,  2.06it/s, loss=624, v_num=0, val_loss_epoch=678.0, train_loss_step=620.0]\n","Validating:  40% 2/5 [00:00<00:00,  4.92it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:08<00:00,  2.22it/s, loss=624, v_num=0, val_loss_epoch=678.0, train_loss_step=620.0]\n","Validating:  80% 4/5 [00:00<00:00,  5.96it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:08<00:00,  2.25it/s, loss=624, v_num=0, val_loss_epoch=599.0, train_loss_step=308.0, train_loss_epoch=645.0, val_loss_step=628.0]\n","Epoch 1:  80% 16/20 [00:06<00:01,  2.59it/s, loss=577, v_num=0, val_loss_epoch=599.0, train_loss_step=520.0, train_loss_epoch=645.0, val_loss_step=628.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.69it/s]\u001b[A\n","Epoch 1:  90% 18/20 [00:06<00:00,  2.72it/s, loss=577, v_num=0, val_loss_epoch=599.0, train_loss_step=520.0, train_loss_epoch=645.0, val_loss_step=628.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.40it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:06<00:00,  2.88it/s, loss=577, v_num=0, val_loss_epoch=599.0, train_loss_step=520.0, train_loss_epoch=645.0, val_loss_step=628.0]\n","Epoch 1: 100% 20/20 [00:07<00:00,  2.82it/s, loss=577, v_num=0, val_loss_epoch=593.0, train_loss_step=397.0, train_loss_epoch=596.0, val_loss_step=620.0]\n","Epoch 2:  80% 16/20 [00:07<00:01,  2.05it/s, loss=564, v_num=0, val_loss_epoch=593.0, train_loss_step=575.0, train_loss_epoch=596.0, val_loss_step=620.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.70it/s]\u001b[A\n","Epoch 2:  90% 18/20 [00:08<00:00,  2.19it/s, loss=564, v_num=0, val_loss_epoch=593.0, train_loss_step=575.0, train_loss_epoch=596.0, val_loss_step=620.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.51it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:08<00:00,  2.34it/s, loss=564, v_num=0, val_loss_epoch=593.0, train_loss_step=575.0, train_loss_epoch=596.0, val_loss_step=620.0]\n","Epoch 2: 100% 20/20 [00:08<00:00,  2.30it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=391.0, train_loss_epoch=590.0, val_loss_step=619.0]\n","Epoch 3:  80% 16/20 [00:06<00:01,  2.63it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=553.0, train_loss_epoch=590.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.74it/s]\u001b[A\n","Epoch 3:  90% 18/20 [00:06<00:00,  2.76it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=553.0, train_loss_epoch=590.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.36it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:06<00:00,  2.93it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=553.0, train_loss_epoch=590.0, val_loss_step=619.0]\n","Epoch 3: 100% 20/20 [00:07<00:00,  2.85it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=338.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Epoch 4:  80% 16/20 [00:07<00:01,  2.10it/s, loss=559, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.86it/s]\u001b[A\n","Epoch 4:  90% 18/20 [00:08<00:00,  2.23it/s, loss=559, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.42it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:08<00:00,  2.39it/s, loss=559, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Epoch 4: 100% 20/20 [00:08<00:00,  2.34it/s, loss=559, v_num=0, val_loss_epoch=591.0, train_loss_step=357.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Epoch 5:  80% 16/20 [00:05<00:01,  2.68it/s, loss=587, v_num=0, val_loss_epoch=591.0, train_loss_step=616.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.75it/s]\u001b[A\n","Epoch 5:  90% 18/20 [00:06<00:00,  2.81it/s, loss=587, v_num=0, val_loss_epoch=591.0, train_loss_step=616.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.53it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:06<00:00,  2.98it/s, loss=587, v_num=0, val_loss_epoch=591.0, train_loss_step=616.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Epoch 5: 100% 20/20 [00:06<00:00,  2.91it/s, loss=587, v_num=0, val_loss_epoch=591.0, train_loss_step=823.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Epoch 6:  80% 16/20 [00:07<00:01,  2.08it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=662.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.87it/s]\u001b[A\n","Epoch 6:  90% 18/20 [00:08<00:00,  2.22it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=662.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.36it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:08<00:00,  2.37it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=662.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Epoch 6: 100% 20/20 [00:08<00:00,  2.32it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=319.0, train_loss_epoch=589.0, val_loss_step=618.0]\n","Epoch 7:  80% 16/20 [00:06<00:01,  2.65it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=553.0, train_loss_epoch=589.0, val_loss_step=618.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.77it/s]\u001b[A\n","Epoch 7:  90% 18/20 [00:06<00:00,  2.78it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=553.0, train_loss_epoch=589.0, val_loss_step=618.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.33it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:06<00:00,  2.94it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=553.0, train_loss_epoch=589.0, val_loss_step=618.0]\n","Epoch 7: 100% 20/20 [00:06<00:00,  2.87it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=736.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 8:  80% 16/20 [00:07<00:01,  2.06it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.14it/s]\u001b[A\n","Epoch 8:  90% 18/20 [00:08<00:00,  2.13it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:01<00:00,  2.25it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:09<00:00,  2.17it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 8: 100% 20/20 [00:09<00:00,  2.13it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=554.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 9:  80% 16/20 [00:06<00:01,  2.60it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=554.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.62it/s]\u001b[A\n","Epoch 9:  90% 18/20 [00:06<00:00,  2.73it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=554.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.31it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:06<00:00,  2.89it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=554.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 9: 100% 20/20 [00:07<00:00,  2.82it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=416.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 10:  80% 16/20 [00:07<00:01,  2.09it/s, loss=573, v_num=0, val_loss_epoch=591.0, train_loss_step=611.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.73it/s]\u001b[A\n","Epoch 10:  90% 18/20 [00:08<00:00,  2.22it/s, loss=573, v_num=0, val_loss_epoch=591.0, train_loss_step=611.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.15it/s]\u001b[A\n","Epoch 10: 100% 20/20 [00:08<00:00,  2.37it/s, loss=573, v_num=0, val_loss_epoch=591.0, train_loss_step=611.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 10: 100% 20/20 [00:08<00:00,  2.32it/s, loss=573, v_num=0, val_loss_epoch=591.0, train_loss_step=452.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Epoch 11:  80% 16/20 [00:06<00:01,  2.67it/s, loss=571, v_num=0, val_loss_epoch=591.0, train_loss_step=547.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.73it/s]\u001b[A\n","Epoch 11:  90% 18/20 [00:06<00:00,  2.80it/s, loss=571, v_num=0, val_loss_epoch=591.0, train_loss_step=547.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.44it/s]\u001b[A\n","Epoch 11: 100% 20/20 [00:06<00:00,  2.96it/s, loss=571, v_num=0, val_loss_epoch=591.0, train_loss_step=547.0, train_loss_epoch=589.0, val_loss_step=619.0]\n","Epoch 11: 100% 20/20 [00:06<00:00,  2.89it/s, loss=571, v_num=0, val_loss_epoch=591.0, train_loss_step=387.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 12:  80% 16/20 [00:07<00:01,  2.08it/s, loss=570, v_num=0, val_loss_epoch=591.0, train_loss_step=573.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.73it/s]\u001b[A\n","Epoch 12:  90% 18/20 [00:08<00:00,  2.21it/s, loss=570, v_num=0, val_loss_epoch=591.0, train_loss_step=573.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.31it/s]\u001b[A\n","Epoch 12: 100% 20/20 [00:08<00:00,  2.37it/s, loss=570, v_num=0, val_loss_epoch=591.0, train_loss_step=573.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 12: 100% 20/20 [00:08<00:00,  2.32it/s, loss=570, v_num=0, val_loss_epoch=591.0, train_loss_step=337.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 13:  80% 16/20 [00:06<00:01,  2.64it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=587.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.66it/s]\u001b[A\n","Epoch 13:  90% 18/20 [00:06<00:00,  2.77it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=587.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.40it/s]\u001b[A\n","Epoch 13: 100% 20/20 [00:06<00:00,  2.94it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=587.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 13: 100% 20/20 [00:06<00:00,  2.87it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=384.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 14:  80% 16/20 [00:07<00:01,  2.08it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=589.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.64it/s]\u001b[A\n","Epoch 14:  90% 18/20 [00:08<00:00,  2.22it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=589.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.35it/s]\u001b[A\n","Epoch 14: 100% 20/20 [00:08<00:00,  2.37it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=589.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 14: 100% 20/20 [00:08<00:00,  2.32it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=368.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 15:  80% 16/20 [00:06<00:01,  2.62it/s, loss=559, v_num=0, val_loss_epoch=591.0, train_loss_step=662.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.74it/s]\u001b[A\n","Epoch 15:  90% 18/20 [00:06<00:00,  2.75it/s, loss=559, v_num=0, val_loss_epoch=591.0, train_loss_step=662.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.35it/s]\u001b[A\n","Epoch 15: 100% 20/20 [00:06<00:00,  2.92it/s, loss=559, v_num=0, val_loss_epoch=591.0, train_loss_step=662.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 15: 100% 20/20 [00:07<00:00,  2.85it/s, loss=559, v_num=0, val_loss_epoch=591.0, train_loss_step=249.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 16:  80% 16/20 [00:08<00:02,  1.96it/s, loss=565, v_num=0, val_loss_epoch=591.0, train_loss_step=588.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.51it/s]\u001b[A\n","Epoch 16:  90% 18/20 [00:08<00:00,  2.05it/s, loss=565, v_num=0, val_loss_epoch=591.0, train_loss_step=588.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.75it/s]\u001b[A\n","Epoch 16: 100% 20/20 [00:09<00:00,  2.17it/s, loss=565, v_num=0, val_loss_epoch=591.0, train_loss_step=588.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 16: 100% 20/20 [00:09<00:00,  2.11it/s, loss=565, v_num=0, val_loss_epoch=591.0, train_loss_step=379.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 17:  80% 16/20 [00:06<00:01,  2.32it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=515.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.66it/s]\u001b[A\n","Epoch 17:  90% 18/20 [00:07<00:00,  2.45it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=515.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.29it/s]\u001b[A\n","Epoch 17: 100% 20/20 [00:07<00:00,  2.61it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=515.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 100% 5/5 [00:00<00:00,  6.10it/s]\u001b[AEpoch 00018: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 17: 100% 20/20 [00:07<00:00,  2.55it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=692.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 18:  80% 16/20 [00:07<00:01,  2.07it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=614.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.59it/s]\u001b[A\n","Epoch 18:  90% 18/20 [00:08<00:00,  2.20it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=614.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.25it/s]\u001b[A\n","Epoch 18: 100% 20/20 [00:08<00:00,  2.35it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=614.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 18: 100% 20/20 [00:08<00:00,  2.31it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=375.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 19:  80% 16/20 [00:06<00:01,  2.60it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.62it/s]\u001b[A\n","Epoch 19:  90% 18/20 [00:06<00:00,  2.72it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.06it/s]\u001b[A\n","Epoch 19: 100% 20/20 [00:06<00:00,  2.88it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 19: 100% 20/20 [00:07<00:00,  2.80it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=577.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 20:  80% 16/20 [00:07<00:01,  2.11it/s, loss=606, v_num=0, val_loss_epoch=591.0, train_loss_step=623.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.65it/s]\u001b[A\n","Epoch 20:  90% 18/20 [00:08<00:00,  2.24it/s, loss=606, v_num=0, val_loss_epoch=591.0, train_loss_step=623.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.33it/s]\u001b[A\n","Epoch 20: 100% 20/20 [00:08<00:00,  2.40it/s, loss=606, v_num=0, val_loss_epoch=591.0, train_loss_step=623.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 20: 100% 20/20 [00:08<00:00,  2.35it/s, loss=606, v_num=0, val_loss_epoch=591.0, train_loss_step=943.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 21:  80% 16/20 [00:06<00:01,  2.58it/s, loss=597, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.60it/s]\u001b[A\n","Epoch 21:  90% 18/20 [00:06<00:00,  2.70it/s, loss=597, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  4.64it/s]\u001b[A\n","Epoch 21: 100% 20/20 [00:07<00:00,  2.82it/s, loss=597, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 21: 100% 20/20 [00:07<00:00,  2.73it/s, loss=597, v_num=0, val_loss_epoch=591.0, train_loss_step=368.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 22:  80% 16/20 [00:07<00:01,  2.08it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=655.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.66it/s]\u001b[A\n","Epoch 22:  90% 18/20 [00:08<00:00,  2.21it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=655.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.19it/s]\u001b[A\n","Epoch 22: 100% 20/20 [00:08<00:00,  2.36it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=655.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 22: 100% 20/20 [00:08<00:00,  2.32it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=570.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 23:  80% 16/20 [00:06<00:01,  2.55it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.62it/s]\u001b[A\n","Epoch 23:  90% 18/20 [00:06<00:00,  2.61it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.77it/s]\u001b[A\n","Epoch 23: 100% 20/20 [00:07<00:00,  2.72it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 23: 100% 20/20 [00:07<00:00,  2.63it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=639.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 24:  80% 16/20 [00:07<00:01,  2.19it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=616.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.69it/s]\u001b[A\n","Epoch 24:  90% 18/20 [00:07<00:00,  2.33it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=616.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.29it/s]\u001b[A\n","Epoch 24: 100% 20/20 [00:08<00:00,  2.48it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=616.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 24: 100% 20/20 [00:08<00:00,  2.43it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=597.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 25:  80% 16/20 [00:06<00:01,  2.55it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=589.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.56it/s]\u001b[A\n","Epoch 25:  90% 18/20 [00:06<00:00,  2.61it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=589.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.88it/s]\u001b[A\n","Epoch 25: 100% 20/20 [00:07<00:00,  2.73it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=589.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 25: 100% 20/20 [00:07<00:00,  2.64it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=514.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 26:  80% 16/20 [00:07<00:01,  2.26it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=656.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.46it/s]\u001b[A\n","Epoch 26:  90% 18/20 [00:07<00:00,  2.39it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=656.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.23it/s]\u001b[A\n","Epoch 26: 100% 20/20 [00:07<00:00,  2.55it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=656.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 26: 100% 20/20 [00:08<00:00,  2.49it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=365.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 27:  80% 16/20 [00:06<00:01,  2.54it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=578.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.63it/s]\u001b[A\n","Epoch 27:  90% 18/20 [00:06<00:00,  2.61it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=578.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.93it/s]\u001b[A\n","Epoch 27: 100% 20/20 [00:07<00:00,  2.72it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=578.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 27: 100% 20/20 [00:07<00:00,  2.63it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=538.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 28:  80% 16/20 [00:07<00:01,  2.27it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=583.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.49it/s]\u001b[A\n","Epoch 28:  90% 18/20 [00:07<00:00,  2.40it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=583.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.27it/s]\u001b[A\n","Epoch 28: 100% 20/20 [00:07<00:00,  2.56it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=583.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 100% 5/5 [00:00<00:00,  6.23it/s]\u001b[AEpoch 00029: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 28: 100% 20/20 [00:08<00:00,  2.50it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=482.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 29:  80% 16/20 [00:06<00:01,  2.45it/s, loss=570, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.67it/s]\u001b[A\n","Epoch 29:  90% 18/20 [00:07<00:00,  2.52it/s, loss=570, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.92it/s]\u001b[A\n","Epoch 29: 100% 20/20 [00:07<00:00,  2.64it/s, loss=570, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 29: 100% 20/20 [00:07<00:00,  2.56it/s, loss=570, v_num=0, val_loss_epoch=591.0, train_loss_step=332.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 30:  80% 16/20 [00:07<00:01,  2.28it/s, loss=571, v_num=0, val_loss_epoch=591.0, train_loss_step=567.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.82it/s]\u001b[A\n","Epoch 30:  90% 18/20 [00:07<00:00,  2.42it/s, loss=571, v_num=0, val_loss_epoch=591.0, train_loss_step=567.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.50it/s]\u001b[A\n","Epoch 30: 100% 20/20 [00:07<00:00,  2.58it/s, loss=571, v_num=0, val_loss_epoch=591.0, train_loss_step=567.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 30: 100% 20/20 [00:07<00:00,  2.53it/s, loss=571, v_num=0, val_loss_epoch=591.0, train_loss_step=472.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 31:  80% 16/20 [00:06<00:01,  2.44it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=612.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.57it/s]\u001b[A\n","Epoch 31:  90% 18/20 [00:07<00:00,  2.51it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=612.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.71it/s]\u001b[A\n","Epoch 31: 100% 20/20 [00:07<00:00,  2.61it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=612.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 31: 100% 20/20 [00:07<00:00,  2.53it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=419.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 32:  80% 16/20 [00:06<00:01,  2.31it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=581.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.66it/s]\u001b[A\n","Epoch 32:  90% 18/20 [00:07<00:00,  2.44it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=581.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.27it/s]\u001b[A\n","Epoch 32: 100% 20/20 [00:07<00:00,  2.60it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=581.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 32: 100% 20/20 [00:07<00:00,  2.55it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=564.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 33:  80% 16/20 [00:06<00:01,  2.39it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=559.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.55it/s]\u001b[A\n","Epoch 33:  90% 18/20 [00:07<00:00,  2.46it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=559.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.90it/s]\u001b[A\n","Epoch 33: 100% 20/20 [00:07<00:00,  2.58it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=559.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 33: 100% 20/20 [00:07<00:00,  2.50it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=734.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 34:  80% 16/20 [00:06<00:01,  2.33it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.76it/s]\u001b[A\n","Epoch 34:  90% 18/20 [00:07<00:00,  2.46it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.34it/s]\u001b[A\n","Epoch 34: 100% 20/20 [00:07<00:00,  2.62it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 34: 100% 20/20 [00:07<00:00,  2.57it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=372.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 35:  80% 16/20 [00:06<00:01,  2.35it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=549.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.65it/s]\u001b[A\n","Epoch 35:  90% 18/20 [00:07<00:00,  2.43it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=549.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.78it/s]\u001b[A\n","Epoch 35: 100% 20/20 [00:07<00:00,  2.53it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=549.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 35: 100% 20/20 [00:08<00:00,  2.46it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=445.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 36:  80% 16/20 [00:06<00:01,  2.41it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.48it/s]\u001b[A\n","Epoch 36:  90% 18/20 [00:07<00:00,  2.54it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.19it/s]\u001b[A\n","Epoch 36: 100% 20/20 [00:07<00:00,  2.69it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 36: 100% 20/20 [00:07<00:00,  2.63it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=610.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 37:  80% 16/20 [00:06<00:01,  2.32it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=558.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.72it/s]\u001b[A\n","Epoch 37:  90% 18/20 [00:07<00:00,  2.39it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=558.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.78it/s]\u001b[A\n","Epoch 37: 100% 20/20 [00:07<00:00,  2.51it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=558.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 37: 100% 20/20 [00:08<00:00,  2.43it/s, loss=588, v_num=0, val_loss_epoch=591.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 38:  80% 16/20 [00:06<00:01,  2.36it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=581.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.56it/s]\u001b[A\n","Epoch 38:  90% 18/20 [00:07<00:00,  2.43it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=581.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.70it/s]\u001b[A\n","Epoch 38: 100% 20/20 [00:07<00:00,  2.54it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=581.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 38: 100% 20/20 [00:08<00:00,  2.46it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=535.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 39:  80% 16/20 [00:08<00:02,  1.86it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=538.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.52it/s]\u001b[A\n","Epoch 39:  90% 18/20 [00:09<00:01,  1.95it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=538.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  4.28it/s]\u001b[A\n","Epoch 39: 100% 20/20 [00:09<00:00,  2.09it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=538.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 100% 5/5 [00:01<00:00,  5.69it/s]\u001b[AEpoch 00040: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 39: 100% 20/20 [00:09<00:00,  2.06it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=491.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 40:  80% 16/20 [00:06<00:01,  2.59it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.44it/s]\u001b[A\n","Epoch 40:  90% 18/20 [00:06<00:00,  2.71it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.22it/s]\u001b[A\n","Epoch 40: 100% 20/20 [00:06<00:00,  2.88it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 40: 100% 20/20 [00:07<00:00,  2.81it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=599.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 41:  80% 16/20 [00:07<00:01,  2.04it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=580.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.24it/s]\u001b[A\n","Epoch 41:  90% 18/20 [00:08<00:00,  2.16it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=580.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.03it/s]\u001b[A\n","Epoch 41: 100% 20/20 [00:08<00:00,  2.32it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=580.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 41: 100% 20/20 [00:08<00:00,  2.27it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=295.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 42:  80% 16/20 [00:06<00:01,  2.62it/s, loss=558, v_num=0, val_loss_epoch=591.0, train_loss_step=558.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.71it/s]\u001b[A\n","Epoch 42:  90% 18/20 [00:06<00:00,  2.76it/s, loss=558, v_num=0, val_loss_epoch=591.0, train_loss_step=558.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.29it/s]\u001b[A\n","Epoch 42: 100% 20/20 [00:06<00:00,  2.91it/s, loss=558, v_num=0, val_loss_epoch=591.0, train_loss_step=558.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 42: 100% 20/20 [00:07<00:00,  2.84it/s, loss=558, v_num=0, val_loss_epoch=591.0, train_loss_step=390.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 43:  80% 16/20 [00:07<00:01,  2.06it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=564.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.78it/s]\u001b[A\n","Epoch 43:  90% 18/20 [00:08<00:00,  2.19it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=564.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.45it/s]\u001b[A\n","Epoch 43: 100% 20/20 [00:08<00:00,  2.34it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=564.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 43: 100% 20/20 [00:08<00:00,  2.30it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=631.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 44:  80% 16/20 [00:06<00:01,  2.60it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=637.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.75it/s]\u001b[A\n","Epoch 44:  90% 18/20 [00:06<00:00,  2.73it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=637.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.36it/s]\u001b[A\n","Epoch 44: 100% 20/20 [00:06<00:00,  2.89it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=637.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 44: 100% 20/20 [00:07<00:00,  2.82it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=405.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 45:  80% 16/20 [00:07<00:01,  2.05it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.48it/s]\u001b[A\n","Epoch 45:  90% 18/20 [00:08<00:00,  2.18it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.29it/s]\u001b[A\n","Epoch 45: 100% 20/20 [00:08<00:00,  2.33it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=571.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 45: 100% 20/20 [00:08<00:00,  2.28it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=361.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 46:  80% 16/20 [00:06<00:01,  2.41it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=572.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.52it/s]\u001b[A\n","Epoch 46:  90% 18/20 [00:07<00:00,  2.54it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=572.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.24it/s]\u001b[A\n","Epoch 46: 100% 20/20 [00:07<00:00,  2.70it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=572.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 46: 100% 20/20 [00:07<00:00,  2.64it/s, loss=566, v_num=0, val_loss_epoch=591.0, train_loss_step=376.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 47:  80% 16/20 [00:07<00:01,  2.06it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=633.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.49it/s]\u001b[A\n","Epoch 47:  90% 18/20 [00:08<00:00,  2.19it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=633.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.29it/s]\u001b[A\n","Epoch 47: 100% 20/20 [00:08<00:00,  2.34it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=633.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 47: 100% 20/20 [00:08<00:00,  2.30it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=659.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 48:  80% 16/20 [00:06<00:01,  2.59it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=671.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.68it/s]\u001b[A\n","Epoch 48:  90% 18/20 [00:06<00:00,  2.73it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=671.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.49it/s]\u001b[A\n","Epoch 48: 100% 20/20 [00:06<00:00,  2.89it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=671.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 48: 100% 20/20 [00:07<00:00,  2.83it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=291.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 49:  80% 16/20 [00:07<00:01,  2.06it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=566.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.71it/s]\u001b[A\n","Epoch 49:  90% 18/20 [00:08<00:00,  2.20it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=566.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.44it/s]\u001b[A\n","Epoch 49: 100% 20/20 [00:08<00:00,  2.35it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=566.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 49: 100% 20/20 [00:08<00:00,  2.30it/s, loss=579, v_num=0, val_loss_epoch=591.0, train_loss_step=503.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 50:  80% 16/20 [00:06<00:01,  2.59it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=575.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.65it/s]\u001b[A\n","Epoch 50:  90% 18/20 [00:06<00:00,  2.73it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=575.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.30it/s]\u001b[A\n","Epoch 50: 100% 20/20 [00:06<00:00,  2.89it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=575.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 100% 5/5 [00:00<00:00,  6.27it/s]\u001b[AEpoch 00051: reducing learning rate of group 0 to 6.2500e-06.\n","Epoch 50: 100% 20/20 [00:07<00:00,  2.82it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=566.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 51:  80% 16/20 [00:07<00:01,  2.05it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=559.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.68it/s]\u001b[A\n","Epoch 51:  90% 18/20 [00:08<00:00,  2.18it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=559.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.25it/s]\u001b[A\n","Epoch 51: 100% 20/20 [00:08<00:00,  2.33it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=559.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 51: 100% 20/20 [00:08<00:00,  2.29it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=488.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 52:  80% 16/20 [00:06<00:01,  2.63it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=542.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.56it/s]\u001b[A\n","Epoch 52:  90% 18/20 [00:06<00:00,  2.76it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=542.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.31it/s]\u001b[A\n","Epoch 52: 100% 20/20 [00:06<00:00,  2.92it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=542.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 52: 100% 20/20 [00:07<00:00,  2.85it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=702.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 53:  80% 16/20 [00:07<00:01,  2.06it/s, loss=595, v_num=0, val_loss_epoch=591.0, train_loss_step=562.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.29it/s]\u001b[A\n","Epoch 53:  90% 18/20 [00:08<00:00,  2.09it/s, loss=595, v_num=0, val_loss_epoch=591.0, train_loss_step=562.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:01<00:00,  2.32it/s]\u001b[A\n","Epoch 53: 100% 20/20 [00:09<00:00,  2.17it/s, loss=595, v_num=0, val_loss_epoch=591.0, train_loss_step=562.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 53: 100% 20/20 [00:09<00:00,  2.10it/s, loss=595, v_num=0, val_loss_epoch=591.0, train_loss_step=630.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 54:  80% 16/20 [00:10<00:02,  1.55it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.62it/s]\u001b[A\n","Epoch 54:  90% 18/20 [00:10<00:01,  1.64it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.77it/s]\u001b[A\n","Epoch 54: 100% 20/20 [00:11<00:00,  1.75it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 54: 100% 20/20 [00:11<00:00,  1.71it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=412.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 55:  80% 16/20 [00:06<00:01,  2.32it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=599.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.60it/s]\u001b[A\n","Epoch 55:  90% 18/20 [00:07<00:00,  2.46it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=599.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.32it/s]\u001b[A\n","Epoch 55: 100% 20/20 [00:07<00:00,  2.62it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=599.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 55: 100% 20/20 [00:07<00:00,  2.55it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=451.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 56:  80% 16/20 [00:07<00:01,  2.19it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=569.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.40it/s]\u001b[A\n","Epoch 56:  90% 18/20 [00:07<00:00,  2.26it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=569.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.69it/s]\u001b[A\n","Epoch 56: 100% 20/20 [00:08<00:00,  2.37it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=569.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 56: 100% 20/20 [00:08<00:00,  2.30it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=554.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 57:  80% 16/20 [00:06<00:01,  2.54it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=646.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.82it/s]\u001b[A\n","Epoch 57:  90% 18/20 [00:06<00:00,  2.68it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=646.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.31it/s]\u001b[A\n","Epoch 57: 100% 20/20 [00:07<00:00,  2.84it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=646.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 57: 100% 20/20 [00:07<00:00,  2.77it/s, loss=582, v_num=0, val_loss_epoch=591.0, train_loss_step=426.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 58:  80% 16/20 [00:07<00:01,  2.19it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=600.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.47it/s]\u001b[A\n","Epoch 58:  90% 18/20 [00:07<00:00,  2.26it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=600.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.59it/s]\u001b[A\n","Epoch 58: 100% 20/20 [00:08<00:00,  2.37it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=600.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 58: 100% 20/20 [00:08<00:00,  2.30it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=403.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 59:  80% 16/20 [00:06<00:01,  2.59it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=576.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.42it/s]\u001b[A\n","Epoch 59:  90% 18/20 [00:06<00:00,  2.71it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=576.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.18it/s]\u001b[A\n","Epoch 59: 100% 20/20 [00:06<00:00,  2.87it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=576.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 59: 100% 20/20 [00:07<00:00,  2.79it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=246.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 60:  80% 16/20 [00:08<00:02,  1.80it/s, loss=552, v_num=0, val_loss_epoch=591.0, train_loss_step=620.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:02,  1.96it/s]\u001b[A\n","Epoch 60:  90% 18/20 [00:09<00:01,  1.87it/s, loss=552, v_num=0, val_loss_epoch=591.0, train_loss_step=620.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.34it/s]\u001b[A\n","Epoch 60: 100% 20/20 [00:10<00:00,  1.98it/s, loss=552, v_num=0, val_loss_epoch=591.0, train_loss_step=620.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 60: 100% 20/20 [00:10<00:00,  1.93it/s, loss=552, v_num=0, val_loss_epoch=591.0, train_loss_step=262.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 61:  80% 16/20 [00:06<00:01,  2.52it/s, loss=585, v_num=0, val_loss_epoch=591.0, train_loss_step=536.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.65it/s]\u001b[A\n","Epoch 61:  90% 18/20 [00:06<00:00,  2.66it/s, loss=585, v_num=0, val_loss_epoch=591.0, train_loss_step=536.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.32it/s]\u001b[A\n","Epoch 61: 100% 20/20 [00:07<00:00,  2.82it/s, loss=585, v_num=0, val_loss_epoch=591.0, train_loss_step=536.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 100% 5/5 [00:00<00:00,  6.29it/s]\u001b[AEpoch 00062: reducing learning rate of group 0 to 3.1250e-06.\n","Epoch 61: 100% 20/20 [00:07<00:00,  2.75it/s, loss=585, v_num=0, val_loss_epoch=591.0, train_loss_step=880.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 62:  80% 16/20 [00:07<00:01,  2.25it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=538.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.60it/s]\u001b[A\n","Epoch 62:  90% 18/20 [00:07<00:00,  2.33it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=538.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.84it/s]\u001b[A\n","Epoch 62: 100% 20/20 [00:08<00:00,  2.45it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=538.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 62: 100% 20/20 [00:08<00:00,  2.38it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=421.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 63:  80% 16/20 [00:06<00:01,  2.47it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=585.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.62it/s]\u001b[A\n","Epoch 63:  90% 18/20 [00:06<00:00,  2.60it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=585.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.33it/s]\u001b[A\n","Epoch 63: 100% 20/20 [00:07<00:00,  2.77it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=585.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 63: 100% 20/20 [00:07<00:00,  2.70it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=409.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 64:  80% 16/20 [00:07<00:01,  2.25it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=630.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.63it/s]\u001b[A\n","Epoch 64:  90% 18/20 [00:07<00:00,  2.33it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=630.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.79it/s]\u001b[A\n","Epoch 64: 100% 20/20 [00:08<00:00,  2.44it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=630.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 64: 100% 20/20 [00:08<00:00,  2.37it/s, loss=584, v_num=0, val_loss_epoch=591.0, train_loss_step=751.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 65:  80% 16/20 [00:06<00:01,  2.53it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.60it/s]\u001b[A\n","Epoch 65:  90% 18/20 [00:06<00:00,  2.66it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.41it/s]\u001b[A\n","Epoch 65: 100% 20/20 [00:07<00:00,  2.82it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 65: 100% 20/20 [00:07<00:00,  2.75it/s, loss=590, v_num=0, val_loss_epoch=591.0, train_loss_step=356.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 66:  80% 16/20 [00:07<00:01,  2.18it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.50it/s]\u001b[A\n","Epoch 66:  90% 18/20 [00:07<00:00,  2.26it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.62it/s]\u001b[A\n","Epoch 66: 100% 20/20 [00:08<00:00,  2.36it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 66: 100% 20/20 [00:08<00:00,  2.29it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=331.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 67:  80% 16/20 [00:06<00:01,  2.56it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=583.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.68it/s]\u001b[A\n","Epoch 67:  90% 18/20 [00:06<00:00,  2.69it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=583.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.32it/s]\u001b[A\n","Epoch 67: 100% 20/20 [00:07<00:00,  2.86it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=583.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 67: 100% 20/20 [00:07<00:00,  2.79it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 68:  80% 16/20 [00:07<00:01,  2.20it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.59it/s]\u001b[A\n","Epoch 68:  90% 18/20 [00:07<00:00,  2.28it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.74it/s]\u001b[A\n","Epoch 68: 100% 20/20 [00:08<00:00,  2.39it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 68: 100% 20/20 [00:08<00:00,  2.32it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=438.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 69:  80% 16/20 [00:06<00:01,  2.57it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=610.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.49it/s]\u001b[A\n","Epoch 69:  90% 18/20 [00:06<00:00,  2.70it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=610.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.30it/s]\u001b[A\n","Epoch 69: 100% 20/20 [00:07<00:00,  2.85it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=610.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 69: 100% 20/20 [00:07<00:00,  2.78it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=486.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 70:  80% 16/20 [00:07<00:01,  2.21it/s, loss=572, v_num=0, val_loss_epoch=591.0, train_loss_step=597.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.60it/s]\u001b[A\n","Epoch 70:  90% 18/20 [00:07<00:00,  2.29it/s, loss=572, v_num=0, val_loss_epoch=591.0, train_loss_step=597.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.83it/s]\u001b[A\n","Epoch 70: 100% 20/20 [00:08<00:00,  2.41it/s, loss=572, v_num=0, val_loss_epoch=591.0, train_loss_step=597.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 70: 100% 20/20 [00:08<00:00,  2.34it/s, loss=572, v_num=0, val_loss_epoch=591.0, train_loss_step=354.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 71:  80% 16/20 [00:06<00:01,  2.55it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=570.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.76it/s]\u001b[A\n","Epoch 71:  90% 18/20 [00:06<00:00,  2.69it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=570.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.46it/s]\u001b[A\n","Epoch 71: 100% 20/20 [00:07<00:00,  2.86it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=570.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 71: 100% 20/20 [00:07<00:00,  2.79it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=400.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 72:  80% 16/20 [00:07<00:01,  2.18it/s, loss=568, v_num=0, val_loss_epoch=591.0, train_loss_step=577.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.50it/s]\u001b[A\n","Epoch 72:  90% 18/20 [00:07<00:00,  2.25it/s, loss=568, v_num=0, val_loss_epoch=591.0, train_loss_step=577.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.58it/s]\u001b[A\n","Epoch 72: 100% 20/20 [00:08<00:00,  2.36it/s, loss=568, v_num=0, val_loss_epoch=591.0, train_loss_step=577.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 100% 5/5 [00:01<00:00,  4.13it/s]\u001b[AEpoch 00073: reducing learning rate of group 0 to 1.5625e-06.\n","Epoch 72: 100% 20/20 [00:08<00:00,  2.29it/s, loss=568, v_num=0, val_loss_epoch=591.0, train_loss_step=478.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 73:  80% 16/20 [00:06<00:01,  2.57it/s, loss=573, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.56it/s]\u001b[A\n","Epoch 73:  90% 18/20 [00:06<00:00,  2.70it/s, loss=573, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.18it/s]\u001b[A\n","Epoch 73: 100% 20/20 [00:06<00:00,  2.86it/s, loss=573, v_num=0, val_loss_epoch=591.0, train_loss_step=625.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 73: 100% 20/20 [00:07<00:00,  2.79it/s, loss=573, v_num=0, val_loss_epoch=591.0, train_loss_step=431.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 74:  80% 16/20 [00:07<00:01,  2.14it/s, loss=595, v_num=0, val_loss_epoch=591.0, train_loss_step=617.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.40it/s]\u001b[A\n","Epoch 74:  90% 18/20 [00:08<00:00,  2.22it/s, loss=595, v_num=0, val_loss_epoch=591.0, train_loss_step=617.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.59it/s]\u001b[A\n","Epoch 74: 100% 20/20 [00:08<00:00,  2.33it/s, loss=595, v_num=0, val_loss_epoch=591.0, train_loss_step=617.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 74: 100% 20/20 [00:08<00:00,  2.29it/s, loss=595, v_num=0, val_loss_epoch=591.0, train_loss_step=986.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 75:  80% 16/20 [00:06<00:01,  2.61it/s, loss=612, v_num=0, val_loss_epoch=591.0, train_loss_step=620.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.39it/s]\u001b[A\n","Epoch 75:  90% 18/20 [00:06<00:00,  2.73it/s, loss=612, v_num=0, val_loss_epoch=591.0, train_loss_step=620.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.02it/s]\u001b[A\n","Epoch 75: 100% 20/20 [00:06<00:00,  2.88it/s, loss=612, v_num=0, val_loss_epoch=591.0, train_loss_step=620.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 75: 100% 20/20 [00:07<00:00,  2.81it/s, loss=612, v_num=0, val_loss_epoch=591.0, train_loss_step=712.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 76:  80% 16/20 [00:07<00:01,  2.09it/s, loss=613, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.45it/s]\u001b[A\n","Epoch 76:  90% 18/20 [00:08<00:00,  2.17it/s, loss=613, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.73it/s]\u001b[A\n","Epoch 76: 100% 20/20 [00:08<00:00,  2.31it/s, loss=613, v_num=0, val_loss_epoch=591.0, train_loss_step=586.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 76: 100% 20/20 [00:08<00:00,  2.26it/s, loss=613, v_num=0, val_loss_epoch=591.0, train_loss_step=939.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 77:  80% 16/20 [00:06<00:01,  2.59it/s, loss=598, v_num=0, val_loss_epoch=591.0, train_loss_step=551.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.64it/s]\u001b[A\n","Epoch 77:  90% 18/20 [00:06<00:00,  2.73it/s, loss=598, v_num=0, val_loss_epoch=591.0, train_loss_step=551.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.15it/s]\u001b[A\n","Epoch 77: 100% 20/20 [00:06<00:00,  2.88it/s, loss=598, v_num=0, val_loss_epoch=591.0, train_loss_step=551.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 77: 100% 20/20 [00:07<00:00,  2.81it/s, loss=598, v_num=0, val_loss_epoch=591.0, train_loss_step=458.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 78:  80% 16/20 [00:07<00:01,  2.03it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=590.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.89it/s]\u001b[A\n","Epoch 78:  90% 18/20 [00:08<00:00,  2.15it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=590.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  4.89it/s]\u001b[A\n","Epoch 78: 100% 20/20 [00:08<00:00,  2.30it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=590.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 78: 100% 20/20 [00:08<00:00,  2.25it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=427.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 79:  80% 16/20 [00:06<00:01,  2.60it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=554.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.42it/s]\u001b[A\n","Epoch 79:  90% 18/20 [00:06<00:00,  2.72it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=554.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.23it/s]\u001b[A\n","Epoch 79: 100% 20/20 [00:06<00:00,  2.89it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=554.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 79: 100% 20/20 [00:07<00:00,  2.82it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=560.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 80:  80% 16/20 [00:07<00:01,  2.01it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.64it/s]\u001b[A\n","Epoch 80:  90% 18/20 [00:08<00:00,  2.15it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.35it/s]\u001b[A\n","Epoch 80: 100% 20/20 [00:08<00:00,  2.29it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 80: 100% 20/20 [00:08<00:00,  2.25it/s, loss=578, v_num=0, val_loss_epoch=591.0, train_loss_step=501.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 81:  80% 16/20 [00:07<00:01,  2.16it/s, loss=583, v_num=0, val_loss_epoch=591.0, train_loss_step=613.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.42it/s]\u001b[A\n","Epoch 81:  90% 18/20 [00:08<00:00,  2.23it/s, loss=583, v_num=0, val_loss_epoch=591.0, train_loss_step=613.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.54it/s]\u001b[A\n","Epoch 81: 100% 20/20 [00:08<00:00,  2.34it/s, loss=583, v_num=0, val_loss_epoch=591.0, train_loss_step=613.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 81: 100% 20/20 [00:08<00:00,  2.26it/s, loss=583, v_num=0, val_loss_epoch=591.0, train_loss_step=503.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 82:  80% 16/20 [00:07<00:01,  2.01it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=573.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.46it/s]\u001b[A\n","Epoch 82:  90% 18/20 [00:08<00:00,  2.14it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=573.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.16it/s]\u001b[A\n","Epoch 82: 100% 20/20 [00:08<00:00,  2.29it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=573.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 82: 100% 20/20 [00:08<00:00,  2.25it/s, loss=576, v_num=0, val_loss_epoch=591.0, train_loss_step=475.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 83:  80% 16/20 [00:06<00:01,  2.58it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.56it/s]\u001b[A\n","Epoch 83:  90% 18/20 [00:06<00:00,  2.71it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.33it/s]\u001b[A\n","Epoch 83: 100% 20/20 [00:06<00:00,  2.87it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=593.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 100% 5/5 [00:00<00:00,  5.98it/s]\u001b[AEpoch 00084: reducing learning rate of group 0 to 7.8125e-07.\n","Epoch 83: 100% 20/20 [00:07<00:00,  2.79it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=407.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 84:  80% 16/20 [00:07<00:01,  2.05it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=629.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.69it/s]\u001b[A\n","Epoch 84:  90% 18/20 [00:08<00:00,  2.18it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=629.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.34it/s]\u001b[A\n","Epoch 84: 100% 20/20 [00:08<00:00,  2.34it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=629.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 84: 100% 20/20 [00:08<00:00,  2.29it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=425.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 85:  80% 16/20 [00:06<00:01,  2.54it/s, loss=572, v_num=0, val_loss_epoch=591.0, train_loss_step=545.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.54it/s]\u001b[A\n","Epoch 85:  90% 18/20 [00:06<00:00,  2.67it/s, loss=572, v_num=0, val_loss_epoch=591.0, train_loss_step=545.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.11it/s]\u001b[A\n","Epoch 85: 100% 20/20 [00:07<00:00,  2.81it/s, loss=572, v_num=0, val_loss_epoch=591.0, train_loss_step=545.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 85: 100% 20/20 [00:07<00:00,  2.70it/s, loss=572, v_num=0, val_loss_epoch=591.0, train_loss_step=378.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 86:  80% 16/20 [00:07<00:01,  2.09it/s, loss=574, v_num=0, val_loss_epoch=591.0, train_loss_step=671.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.47it/s]\u001b[A\n","Epoch 86:  90% 18/20 [00:08<00:00,  2.22it/s, loss=574, v_num=0, val_loss_epoch=591.0, train_loss_step=671.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.26it/s]\u001b[A\n","Epoch 86: 100% 20/20 [00:08<00:00,  2.37it/s, loss=574, v_num=0, val_loss_epoch=591.0, train_loss_step=671.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 86: 100% 20/20 [00:08<00:00,  2.32it/s, loss=574, v_num=0, val_loss_epoch=591.0, train_loss_step=506.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 87:  80% 16/20 [00:06<00:01,  2.57it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=565.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.63it/s]\u001b[A\n","Epoch 87:  90% 18/20 [00:06<00:00,  2.63it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=565.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.84it/s]\u001b[A\n","Epoch 87: 100% 20/20 [00:07<00:00,  2.74it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=565.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 87: 100% 20/20 [00:07<00:00,  2.65it/s, loss=580, v_num=0, val_loss_epoch=591.0, train_loss_step=491.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 88:  80% 16/20 [00:07<00:01,  2.14it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=603.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.49it/s]\u001b[A\n","Epoch 88:  90% 18/20 [00:07<00:00,  2.27it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=603.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.18it/s]\u001b[A\n","Epoch 88: 100% 20/20 [00:08<00:00,  2.42it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=603.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 88: 100% 20/20 [00:08<00:00,  2.37it/s, loss=564, v_num=0, val_loss_epoch=591.0, train_loss_step=323.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 89:  80% 16/20 [00:06<00:01,  2.48it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=563.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.51it/s]\u001b[A\n","Epoch 89:  90% 18/20 [00:07<00:00,  2.54it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=563.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.73it/s]\u001b[A\n","Epoch 89: 100% 20/20 [00:07<00:00,  2.66it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=563.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 89: 100% 20/20 [00:07<00:00,  2.56it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=385.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 90:  80% 16/20 [00:07<00:01,  2.22it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=609.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.52it/s]\u001b[A\n","Epoch 90:  90% 18/20 [00:07<00:00,  2.35it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=609.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.29it/s]\u001b[A\n","Epoch 90: 100% 20/20 [00:07<00:00,  2.51it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=609.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 90: 100% 20/20 [00:08<00:00,  2.45it/s, loss=569, v_num=0, val_loss_epoch=591.0, train_loss_step=468.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 91:  80% 16/20 [00:06<00:01,  2.46it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=651.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.52it/s]\u001b[A\n","Epoch 91:  90% 18/20 [00:07<00:00,  2.53it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=651.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.76it/s]\u001b[A\n","Epoch 91: 100% 20/20 [00:07<00:00,  2.64it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=651.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 91: 100% 20/20 [00:07<00:00,  2.57it/s, loss=575, v_num=0, val_loss_epoch=591.0, train_loss_step=356.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 92:  80% 16/20 [00:07<00:01,  2.27it/s, loss=568, v_num=0, val_loss_epoch=591.0, train_loss_step=617.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.67it/s]\u001b[A\n","Epoch 92:  90% 18/20 [00:07<00:00,  2.40it/s, loss=568, v_num=0, val_loss_epoch=591.0, train_loss_step=617.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.34it/s]\u001b[A\n","Epoch 92: 100% 20/20 [00:07<00:00,  2.56it/s, loss=568, v_num=0, val_loss_epoch=591.0, train_loss_step=617.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 92: 100% 20/20 [00:07<00:00,  2.50it/s, loss=568, v_num=0, val_loss_epoch=591.0, train_loss_step=399.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 93:  80% 16/20 [00:06<00:01,  2.41it/s, loss=563, v_num=0, val_loss_epoch=591.0, train_loss_step=603.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.57it/s]\u001b[A\n","Epoch 93:  90% 18/20 [00:07<00:00,  2.47it/s, loss=563, v_num=0, val_loss_epoch=591.0, train_loss_step=603.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.64it/s]\u001b[A\n","Epoch 93: 100% 20/20 [00:07<00:00,  2.59it/s, loss=563, v_num=0, val_loss_epoch=591.0, train_loss_step=603.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 93: 100% 20/20 [00:08<00:00,  2.50it/s, loss=563, v_num=0, val_loss_epoch=591.0, train_loss_step=248.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 94:  80% 16/20 [00:06<00:01,  2.31it/s, loss=560, v_num=0, val_loss_epoch=591.0, train_loss_step=520.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.44it/s]\u001b[A\n","Epoch 94:  90% 18/20 [00:07<00:00,  2.44it/s, loss=560, v_num=0, val_loss_epoch=591.0, train_loss_step=520.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.03it/s]\u001b[A\n","Epoch 94: 100% 20/20 [00:07<00:00,  2.59it/s, loss=560, v_num=0, val_loss_epoch=591.0, train_loss_step=520.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 100% 5/5 [00:00<00:00,  6.08it/s]\u001b[AEpoch 00095: reducing learning rate of group 0 to 3.9063e-07.\n","Epoch 94: 100% 20/20 [00:07<00:00,  2.54it/s, loss=560, v_num=0, val_loss_epoch=591.0, train_loss_step=344.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 95:  80% 16/20 [00:06<00:01,  2.31it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=630.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.69it/s]\u001b[A\n","Epoch 95:  90% 18/20 [00:07<00:00,  2.38it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=630.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.84it/s]\u001b[A\n","Epoch 95: 100% 20/20 [00:07<00:00,  2.50it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=630.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 95: 100% 20/20 [00:08<00:00,  2.42it/s, loss=567, v_num=0, val_loss_epoch=591.0, train_loss_step=403.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 96:  80% 16/20 [00:06<00:01,  2.33it/s, loss=581, v_num=0, val_loss_epoch=591.0, train_loss_step=627.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.58it/s]\u001b[A\n","Epoch 96:  90% 18/20 [00:07<00:00,  2.47it/s, loss=581, v_num=0, val_loss_epoch=591.0, train_loss_step=627.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.23it/s]\u001b[A\n","Epoch 96: 100% 20/20 [00:07<00:00,  2.62it/s, loss=581, v_num=0, val_loss_epoch=591.0, train_loss_step=627.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 96: 100% 20/20 [00:07<00:00,  2.56it/s, loss=581, v_num=0, val_loss_epoch=591.0, train_loss_step=498.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 97:  80% 16/20 [00:06<00:01,  2.32it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=607.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.71it/s]\u001b[A\n","Epoch 97:  90% 18/20 [00:07<00:00,  2.40it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=607.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.61it/s]\u001b[A\n","Epoch 97: 100% 20/20 [00:07<00:00,  2.50it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=607.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 97: 100% 20/20 [00:08<00:00,  2.42it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=579.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 98:  80% 16/20 [00:06<00:01,  2.38it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=591.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.37it/s]\u001b[A\n","Epoch 98:  90% 18/20 [00:07<00:00,  2.51it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=591.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  5.22it/s]\u001b[A\n","Epoch 98: 100% 20/20 [00:07<00:00,  2.67it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=591.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 98: 100% 20/20 [00:07<00:00,  2.61it/s, loss=586, v_num=0, val_loss_epoch=591.0, train_loss_step=513.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 99:  80% 16/20 [00:07<00:01,  2.26it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=624.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.55it/s]\u001b[A\n","Epoch 99:  90% 18/20 [00:07<00:00,  2.32it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=624.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Validating:  60% 3/5 [00:00<00:00,  3.59it/s]\u001b[A\n","Epoch 99: 100% 20/20 [00:08<00:00,  2.44it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=624.0, train_loss_epoch=588.0, val_loss_step=619.0]\n","Epoch 99: 100% 20/20 [00:08<00:00,  2.36it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=685.0, train_loss_epoch=588.0, val_loss_step=618.0]\n","Epoch 99: 100% 20/20 [00:08<00:00,  2.36it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=685.0, train_loss_epoch=588.0, val_loss_step=618.0]\n","Path to best model found during training: \n","/content/outputs/2023-06-11/19-48-11/lightning_logs/version_0/checkpoints/6-104.ckpt\n"]}]},{"cell_type":"code","source":["# copy path file dari baris terakhir output cell di atas.\n","\n","best_ssl_model_ckpt = \"/content/outputs/2023-06-11/19-48-11/lightning_logs/version_0/checkpoints/6-104.ckpt\""],"metadata":{"id":"nAskwksGLXVF","executionInfo":{"status":"ok","timestamp":1686513746634,"user_tz":-420,"elapsed":382,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["**WARNING : Edit dulu**\n","\n","```1. /content/saint/utils/utils.py, line 42```\n","```\n","def auroc(self):\n","  return AUROC(num_classes=self.num_classes, task='multiclass')\n","```\n","```2. /content/saint/src/trainer.py, line 81```\n","```\n","def on_training_epoch_end(self, training_step_outputs):\n","```\n","```4. /content/saint/src/trainer.py, line 107```\n","```\n","def on_test_epoch_end(self):\n","```\n","```5. /content/saint/src/trainer.py, line 16```\n","```\n","super().__init__()\n","self.validation_step_outputs = []\n","self.transformer = transformer\n","```\n","```6. /content/saint/src/trainer.py, line 88```\n","```\n","def validation_step(self, batch, batch_idx):\n","    val_loss = self._shared_step(batch, self.valid_metric)\n","\n","    # log the outputs!\n","    self.log(f'val_loss', val_loss, on_step=False, \n","              on_epoch=True, prog_bar=True, logger=True)\n","    \n","    self.log(f'val_{self.metric}_epoch', self.valid_metric.compute(), prog_bar=True,)\n","    \n","    self.validation_step_outputs.append(val_loss)\n","    return val_loss\n","```\n","```7. /content/saint/src/trainer.py, line 98```\n","```\n","def on_validation_epoch_end(self):\n","    self.log(f'val_{self.metric}_epoch', self.valid_metric.compute(), prog_bar=True,)\n","\n","    # reset after each epoch\n","    self.valid_metric.reset()\n","    \n","    epoch_average = torch.stack(self.validation_step_outputs).mean()\n","    self.log(\"validation_epoch_average\", epoch_average)\n","    self.validation_step_outputs.clear()  # free memory\n","```\n","```Reference : https://github.com/Lightning-AI/lightning/discussions/17182```\n","\n","```8. /content/saint/src/train.py, line 42```\n","```\n","trainer.fit(model, dataloaders['train_loader'], dataloaders['validation_loader'])\n","```"],"metadata":{"id":"pl-QqhdQBdvr"}},{"cell_type":"code","source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"I9ogLvZUJHr6","executionInfo":{"status":"ok","timestamp":1686513783582,"user_tz":-420,"elapsed":402,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# SUP from SSL\n","\n","!python /content/saint/main.py experiment=supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_sup \\\n","  experiment.pretrained_checkpoint={best_ssl_model_ckpt}"],"metadata":{"id":"kyF5SYkKLY2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686515980676,"user_tz":-420,"elapsed":278031,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"2b64d7de-a36a-44de-ccdc-b4eea5a887eb"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 20:35:06.465433: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-11 20:35:08.182616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/main.py:11: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Global seed set to 1234\n","Initializing supervised task using pretrained model:\n","/content/outputs/2023-06-11/19-48-11/lightning_logs/version_0/checkpoints/6-104.ckpt\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","\n","  | Name         | Type             | Params\n","--------------------------------------------------\n","0 | transformer  | Encoder          | 65.0 K\n","1 | embedding    | Embedding        | 3.2 K \n","2 | fc           | Linear           | 165   \n","3 | criterion    | CrossEntropyLoss | 0     \n","4 | train_metric | Accuracy         | 0     \n","5 | valid_metric | Accuracy         | 0     \n","6 | test_metric  | Accuracy         | 0     \n","--------------------------------------------------\n","68.3 K    Trainable params\n","0         Non-trainable params\n","68.3 K    Total params\n","0.273     Total estimated model params size (MB)\n","Global seed set to 1234\n","Epoch 0:  58% 7/12 [00:02<00:02,  2.46it/s, loss=1.68, v_num=0, val_loss=1.760, val_acc_epoch=0.250, train_loss_step=1.520]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  75% 9/12 [00:03<00:01,  2.84it/s, loss=1.68, v_num=0, val_loss=1.760, val_acc_epoch=0.250, train_loss_step=1.520]\n","Validating:  40% 2/5 [00:00<00:00,  4.71it/s]\u001b[A\n","Epoch 0:  92% 11/12 [00:03<00:00,  3.18it/s, loss=1.68, v_num=0, val_loss=1.760, val_acc_epoch=0.250, train_loss_step=1.520]\n","Validating:  80% 4/5 [00:00<00:00,  6.23it/s]\u001b[A\n","Epoch 0: 100% 12/12 [00:03<00:00,  3.16it/s, loss=1.68, v_num=0, val_loss=1.680, val_acc_epoch=0.253, train_loss_step=1.850, train_loss_epoch=1.660, train_acc_epoch=0.235]\n","Epoch 1:  67% 8/12 [00:02<00:01,  3.17it/s, loss=1.63, v_num=0, val_loss=1.680, val_acc_epoch=0.253, train_loss_step=1.430, train_loss_epoch=1.660, train_acc_epoch=0.235]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.89it/s]\u001b[A\n","Epoch 1:  83% 10/12 [00:02<00:00,  3.54it/s, loss=1.63, v_num=0, val_loss=1.680, val_acc_epoch=0.253, train_loss_step=1.430, train_loss_epoch=1.660, train_acc_epoch=0.235]\n","Epoch 1: 100% 12/12 [00:03<00:00,  3.84it/s, loss=1.63, v_num=0, val_loss=1.660, val_acc_epoch=0.193, train_loss_step=1.350, train_loss_epoch=1.610, train_acc_epoch=0.230]\n","Epoch 2:  67% 8/12 [00:01<00:00,  4.78it/s, loss=1.59, v_num=0, val_loss=1.660, val_acc_epoch=0.193, train_loss_step=1.490, train_loss_epoch=1.610, train_acc_epoch=0.230]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.73it/s]\u001b[A\n","Epoch 2:  83% 10/12 [00:01<00:00,  5.02it/s, loss=1.59, v_num=0, val_loss=1.660, val_acc_epoch=0.193, train_loss_step=1.490, train_loss_epoch=1.610, train_acc_epoch=0.230]\n","Epoch 2: 100% 12/12 [00:02<00:00,  5.23it/s, loss=1.59, v_num=0, val_loss=1.640, val_acc_epoch=0.227, train_loss_step=1.310, train_loss_epoch=1.560, train_acc_epoch=0.255]\n","Epoch 3:  67% 8/12 [00:01<00:00,  4.97it/s, loss=1.55, v_num=0, val_loss=1.640, val_acc_epoch=0.227, train_loss_step=1.410, train_loss_epoch=1.560, train_acc_epoch=0.255]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 3:  83% 10/12 [00:01<00:00,  5.30it/s, loss=1.55, v_num=0, val_loss=1.640, val_acc_epoch=0.227, train_loss_step=1.410, train_loss_epoch=1.560, train_acc_epoch=0.255]\n","Epoch 3: 100% 12/12 [00:02<00:00,  5.86it/s, loss=1.55, v_num=0, val_loss=1.640, val_acc_epoch=0.227, train_loss_step=1.410, train_loss_epoch=1.560, train_acc_epoch=0.255]\n","Epoch 3: 100% 12/12 [00:02<00:00,  5.61it/s, loss=1.55, v_num=0, val_loss=1.630, val_acc_epoch=0.260, train_loss_step=1.570, train_loss_epoch=1.550, train_acc_epoch=0.270]\n","Epoch 4:  67% 8/12 [00:01<00:00,  4.53it/s, loss=1.53, v_num=0, val_loss=1.630, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.550, train_acc_epoch=0.270]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 4:  83% 10/12 [00:02<00:00,  4.94it/s, loss=1.53, v_num=0, val_loss=1.630, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.550, train_acc_epoch=0.270]\n","Epoch 4: 100% 12/12 [00:02<00:00,  5.49it/s, loss=1.53, v_num=0, val_loss=1.630, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.550, train_acc_epoch=0.270]\n","Epoch 4: 100% 12/12 [00:02<00:00,  5.19it/s, loss=1.53, v_num=0, val_loss=1.630, val_acc_epoch=0.253, train_loss_step=1.370, train_loss_epoch=1.550, train_acc_epoch=0.275]\n","Epoch 5:  67% 8/12 [00:01<00:00,  4.65it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.253, train_loss_step=1.430, train_loss_epoch=1.550, train_acc_epoch=0.275]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.94it/s]\u001b[A\n","Epoch 5:  83% 10/12 [00:02<00:00,  4.73it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.253, train_loss_step=1.430, train_loss_epoch=1.550, train_acc_epoch=0.275]\n","Validating:  60% 3/5 [00:00<00:00,  6.33it/s]\u001b[A\n","Epoch 5: 100% 12/12 [00:02<00:00,  4.70it/s, loss=1.52, v_num=0, val_loss=1.620, val_acc_epoch=0.247, train_loss_step=1.280, train_loss_epoch=1.530, train_acc_epoch=0.330]\n","Epoch 6:  67% 8/12 [00:02<00:01,  3.34it/s, loss=1.5, v_num=0, val_loss=1.620, val_acc_epoch=0.247, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.330]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.39it/s]\u001b[A\n","Epoch 6:  83% 10/12 [00:02<00:00,  3.50it/s, loss=1.5, v_num=0, val_loss=1.620, val_acc_epoch=0.247, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.330]\n","Validating:  60% 3/5 [00:00<00:00,  5.80it/s]\u001b[A\n","Epoch 6: 100% 12/12 [00:03<00:00,  3.88it/s, loss=1.5, v_num=0, val_loss=1.620, val_acc_epoch=0.247, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.330]\n","Epoch 6: 100% 12/12 [00:03<00:00,  3.67it/s, loss=1.5, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.250, train_loss_epoch=1.530, train_acc_epoch=0.285]\n","Epoch 7:  67% 8/12 [00:02<00:01,  3.50it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.285]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 7:  83% 10/12 [00:02<00:00,  3.86it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.285]\n","Epoch 7: 100% 12/12 [00:02<00:00,  4.30it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.285]\n","Epoch 7: 100% 12/12 [00:02<00:00,  4.12it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.430, train_loss_epoch=1.570, train_acc_epoch=0.275]\n","Epoch 8:  67% 8/12 [00:01<00:00,  4.20it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.440, train_loss_epoch=1.570, train_acc_epoch=0.275]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.86it/s]\u001b[A\n","Epoch 8:  83% 10/12 [00:02<00:00,  4.50it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.440, train_loss_epoch=1.570, train_acc_epoch=0.275]\n","Epoch 8: 100% 12/12 [00:02<00:00,  4.75it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.260, train_loss_epoch=1.550, train_acc_epoch=0.300]\n","Epoch 9:  67% 8/12 [00:01<00:00,  4.78it/s, loss=1.51, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.330, train_loss_epoch=1.550, train_acc_epoch=0.300]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.32it/s]\u001b[A\n","Epoch 9:  83% 10/12 [00:01<00:00,  5.05it/s, loss=1.51, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.330, train_loss_epoch=1.550, train_acc_epoch=0.300]\n","Epoch 9: 100% 12/12 [00:02<00:00,  5.29it/s, loss=1.51, v_num=0, val_loss=1.630, val_acc_epoch=0.233, train_loss_step=1.290, train_loss_epoch=1.510, train_acc_epoch=0.335]\n","Epoch 10:  67% 8/12 [00:01<00:00,  4.25it/s, loss=1.5, v_num=0, val_loss=1.630, val_acc_epoch=0.233, train_loss_step=1.400, train_loss_epoch=1.510, train_acc_epoch=0.335]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.97it/s]\u001b[A\n","Epoch 10:  83% 10/12 [00:02<00:00,  4.56it/s, loss=1.5, v_num=0, val_loss=1.630, val_acc_epoch=0.233, train_loss_step=1.400, train_loss_epoch=1.510, train_acc_epoch=0.335]\n","Epoch 10: 100% 12/12 [00:02<00:00,  4.83it/s, loss=1.5, v_num=0, val_loss=1.620, val_acc_epoch=0.233, train_loss_step=1.460, train_loss_epoch=1.510, train_acc_epoch=0.295]\n","Epoch 11:  67% 8/12 [00:02<00:01,  3.91it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.233, train_loss_step=1.360, train_loss_epoch=1.510, train_acc_epoch=0.295]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.44it/s]\u001b[A\n","Epoch 11:  83% 10/12 [00:02<00:00,  4.05it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.233, train_loss_step=1.360, train_loss_epoch=1.510, train_acc_epoch=0.295]\n","Validating:  60% 3/5 [00:00<00:00,  6.13it/s]\u001b[A\n","Epoch 11: 100% 12/12 [00:02<00:00,  4.42it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.233, train_loss_step=1.360, train_loss_epoch=1.510, train_acc_epoch=0.295]\n","Epoch 11: 100% 12/12 [00:02<00:00,  4.12it/s, loss=1.48, v_num=0, val_loss=1.630, val_acc_epoch=0.247, train_loss_step=1.220, train_loss_epoch=1.490, train_acc_epoch=0.320]\n","Epoch 12:  67% 8/12 [00:02<00:01,  3.33it/s, loss=1.48, v_num=0, val_loss=1.630, val_acc_epoch=0.247, train_loss_step=1.410, train_loss_epoch=1.490, train_acc_epoch=0.320]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.88it/s]\u001b[A\n","Epoch 12:  83% 10/12 [00:02<00:00,  3.59it/s, loss=1.48, v_num=0, val_loss=1.630, val_acc_epoch=0.247, train_loss_step=1.410, train_loss_epoch=1.490, train_acc_epoch=0.320]\n","Validating:  60% 3/5 [00:00<00:00,  6.68it/s]\u001b[A\n","Epoch 12: 100% 12/12 [00:03<00:00,  3.78it/s, loss=1.48, v_num=0, val_loss=1.630, val_acc_epoch=0.227, train_loss_step=1.360, train_loss_epoch=1.520, train_acc_epoch=0.310]\n","Epoch 13:  67% 8/12 [00:02<00:01,  4.00it/s, loss=1.49, v_num=0, val_loss=1.630, val_acc_epoch=0.227, train_loss_step=1.430, train_loss_epoch=1.520, train_acc_epoch=0.310]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 13:  83% 10/12 [00:02<00:00,  4.41it/s, loss=1.49, v_num=0, val_loss=1.630, val_acc_epoch=0.227, train_loss_step=1.430, train_loss_epoch=1.520, train_acc_epoch=0.310]\n","Epoch 13: 100% 12/12 [00:02<00:00,  4.96it/s, loss=1.49, v_num=0, val_loss=1.630, val_acc_epoch=0.227, train_loss_step=1.430, train_loss_epoch=1.520, train_acc_epoch=0.310]\n","Epoch 13: 100% 12/12 [00:02<00:00,  4.77it/s, loss=1.49, v_num=0, val_loss=1.620, val_acc_epoch=0.227, train_loss_step=1.480, train_loss_epoch=1.510, train_acc_epoch=0.320]\n","Epoch 14:  67% 8/12 [00:01<00:00,  4.15it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.227, train_loss_step=1.440, train_loss_epoch=1.510, train_acc_epoch=0.320]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.36it/s]\u001b[A\n","Epoch 14:  83% 10/12 [00:02<00:00,  4.24it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.227, train_loss_step=1.440, train_loss_epoch=1.510, train_acc_epoch=0.320]\n","Validating:  60% 3/5 [00:00<00:00,  6.03it/s]\u001b[A\n","Epoch 14: 100% 12/12 [00:02<00:00,  4.59it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.227, train_loss_step=1.440, train_loss_epoch=1.510, train_acc_epoch=0.320]\n","Epoch 14: 100% 12/12 [00:02<00:00,  4.32it/s, loss=1.48, v_num=0, val_loss=1.610, val_acc_epoch=0.253, train_loss_step=1.270, train_loss_epoch=1.490, train_acc_epoch=0.280]\n","Epoch 15:  67% 8/12 [00:02<00:01,  3.12it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.253, train_loss_step=1.300, train_loss_epoch=1.490, train_acc_epoch=0.280]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.47it/s]\u001b[A\n","Epoch 15:  83% 10/12 [00:02<00:00,  3.36it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.253, train_loss_step=1.300, train_loss_epoch=1.490, train_acc_epoch=0.280]\n","Validating:  60% 3/5 [00:00<00:00,  6.40it/s]\u001b[A\n","Epoch 15: 100% 12/12 [00:03<00:00,  3.57it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.267, train_loss_step=1.400, train_loss_epoch=1.490, train_acc_epoch=0.305]\n","Epoch 16:  67% 8/12 [00:01<00:00,  4.31it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.267, train_loss_step=1.460, train_loss_epoch=1.490, train_acc_epoch=0.305]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.39it/s]\u001b[A\n","Epoch 16:  83% 10/12 [00:02<00:00,  4.38it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.267, train_loss_step=1.460, train_loss_epoch=1.490, train_acc_epoch=0.305]\n","Validating:  60% 3/5 [00:00<00:00,  6.14it/s]\u001b[A\n","Epoch 16: 100% 12/12 [00:02<00:00,  4.71it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.267, train_loss_step=1.460, train_loss_epoch=1.490, train_acc_epoch=0.305]\n","Epoch 16: 100% 12/12 [00:02<00:00,  4.39it/s, loss=1.47, v_num=0, val_loss=1.600, val_acc_epoch=0.273, train_loss_step=1.290, train_loss_epoch=1.510, train_acc_epoch=0.300]\n","Epoch 17:  67% 8/12 [00:02<00:01,  3.41it/s, loss=1.46, v_num=0, val_loss=1.600, val_acc_epoch=0.273, train_loss_step=1.420, train_loss_epoch=1.510, train_acc_epoch=0.300]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.99it/s]\u001b[A\n","Epoch 17:  83% 10/12 [00:02<00:00,  3.69it/s, loss=1.46, v_num=0, val_loss=1.600, val_acc_epoch=0.273, train_loss_step=1.420, train_loss_epoch=1.510, train_acc_epoch=0.300]\n","Validating:  60% 3/5 [00:00<00:00,  7.10it/s]\u001b[A\n","Epoch 17: 100% 12/12 [00:03<00:00,  3.89it/s, loss=1.46, v_num=0, val_loss=1.600, val_acc_epoch=0.260, train_loss_step=1.100, train_loss_epoch=1.470, train_acc_epoch=0.340]\n","Epoch 18:  67% 8/12 [00:02<00:01,  3.87it/s, loss=1.44, v_num=0, val_loss=1.600, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.470, train_acc_epoch=0.340]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 18:  83% 10/12 [00:02<00:00,  4.31it/s, loss=1.44, v_num=0, val_loss=1.600, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.470, train_acc_epoch=0.340]\n","Epoch 18: 100% 12/12 [00:02<00:00,  4.86it/s, loss=1.44, v_num=0, val_loss=1.600, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.470, train_acc_epoch=0.340]\n","Epoch 18: 100% 12/12 [00:02<00:00,  4.68it/s, loss=1.44, v_num=0, val_loss=1.590, val_acc_epoch=0.253, train_loss_step=1.210, train_loss_epoch=1.470, train_acc_epoch=0.305]\n","Epoch 19:  67% 8/12 [00:01<00:00,  4.57it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.253, train_loss_step=1.410, train_loss_epoch=1.470, train_acc_epoch=0.305]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 19:  83% 10/12 [00:02<00:00,  4.98it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.253, train_loss_step=1.410, train_loss_epoch=1.470, train_acc_epoch=0.305]\n","Epoch 19: 100% 12/12 [00:02<00:00,  5.49it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.253, train_loss_step=1.410, train_loss_epoch=1.470, train_acc_epoch=0.305]\n","Epoch 19: 100% 12/12 [00:02<00:00,  5.18it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.310, train_loss_epoch=1.460, train_acc_epoch=0.305]\n","Epoch 20:  67% 8/12 [00:01<00:00,  4.94it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.360, train_loss_epoch=1.460, train_acc_epoch=0.305]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.29it/s]\u001b[A\n","Epoch 20:  83% 10/12 [00:01<00:00,  5.17it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.360, train_loss_epoch=1.460, train_acc_epoch=0.305]\n","Epoch 20: 100% 12/12 [00:02<00:00,  5.38it/s, loss=1.43, v_num=0, val_loss=1.600, val_acc_epoch=0.267, train_loss_step=1.230, train_loss_epoch=1.460, train_acc_epoch=0.375]\n","Epoch 21:  67% 8/12 [00:01<00:00,  4.32it/s, loss=1.43, v_num=0, val_loss=1.600, val_acc_epoch=0.267, train_loss_step=1.340, train_loss_epoch=1.460, train_acc_epoch=0.375]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 21:  83% 10/12 [00:02<00:00,  4.63it/s, loss=1.43, v_num=0, val_loss=1.600, val_acc_epoch=0.267, train_loss_step=1.340, train_loss_epoch=1.460, train_acc_epoch=0.375]\n","Epoch 21: 100% 12/12 [00:02<00:00,  5.10it/s, loss=1.43, v_num=0, val_loss=1.600, val_acc_epoch=0.267, train_loss_step=1.340, train_loss_epoch=1.460, train_acc_epoch=0.375]\n","Epoch 21: 100% 12/12 [00:02<00:00,  4.86it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.330, train_loss_epoch=1.430, train_acc_epoch=0.420]\n","Epoch 22:  67% 8/12 [00:01<00:00,  4.32it/s, loss=1.42, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.420]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.75it/s]\u001b[A\n","Epoch 22:  83% 10/12 [00:02<00:00,  4.41it/s, loss=1.42, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.420]\n","Validating:  60% 3/5 [00:00<00:00,  5.85it/s]\u001b[A\n","Epoch 22: 100% 12/12 [00:02<00:00,  4.75it/s, loss=1.42, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.420]\n","Epoch 22: 100% 12/12 [00:02<00:00,  4.41it/s, loss=1.42, v_num=0, val_loss=1.580, val_acc_epoch=0.313, train_loss_step=1.450, train_loss_epoch=1.440, train_acc_epoch=0.375]\n","Epoch 23:  67% 8/12 [00:02<00:01,  3.41it/s, loss=1.42, v_num=0, val_loss=1.580, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.440, train_acc_epoch=0.375]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.06it/s]\u001b[A\n","Epoch 23:  83% 10/12 [00:02<00:00,  3.67it/s, loss=1.42, v_num=0, val_loss=1.580, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.440, train_acc_epoch=0.375]\n","Validating:  60% 3/5 [00:00<00:00,  6.87it/s]\u001b[A\n","Epoch 23: 100% 12/12 [00:02<00:00,  4.07it/s, loss=1.42, v_num=0, val_loss=1.580, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.440, train_acc_epoch=0.375]\n","Epoch 23: 100% 12/12 [00:03<00:00,  3.87it/s, loss=1.42, v_num=0, val_loss=1.570, val_acc_epoch=0.293, train_loss_step=1.460, train_loss_epoch=1.400, train_acc_epoch=0.450]\n","Epoch 24:  67% 8/12 [00:01<00:00,  4.04it/s, loss=1.41, v_num=0, val_loss=1.570, val_acc_epoch=0.293, train_loss_step=1.260, train_loss_epoch=1.400, train_acc_epoch=0.450]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  6.11it/s]\u001b[A\n","Epoch 24:  83% 10/12 [00:02<00:00,  4.45it/s, loss=1.41, v_num=0, val_loss=1.570, val_acc_epoch=0.293, train_loss_step=1.260, train_loss_epoch=1.400, train_acc_epoch=0.450]\n","Epoch 24: 100% 12/12 [00:02<00:00,  4.74it/s, loss=1.41, v_num=0, val_loss=1.570, val_acc_epoch=0.307, train_loss_step=1.330, train_loss_epoch=1.430, train_acc_epoch=0.395]\n","Epoch 25:  67% 8/12 [00:01<00:00,  4.46it/s, loss=1.4, v_num=0, val_loss=1.570, val_acc_epoch=0.307, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.395]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  6.11it/s]\u001b[A\n","Epoch 25:  83% 10/12 [00:02<00:00,  4.85it/s, loss=1.4, v_num=0, val_loss=1.570, val_acc_epoch=0.307, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.395]\n","Epoch 25: 100% 12/12 [00:02<00:00,  5.20it/s, loss=1.4, v_num=0, val_loss=1.550, val_acc_epoch=0.300, train_loss_step=1.240, train_loss_epoch=1.420, train_acc_epoch=0.355]\n","Epoch 26:  67% 8/12 [00:01<00:00,  4.97it/s, loss=1.41, v_num=0, val_loss=1.550, val_acc_epoch=0.300, train_loss_step=1.320, train_loss_epoch=1.420, train_acc_epoch=0.355]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.57it/s]\u001b[A\n","Epoch 26:  83% 10/12 [00:01<00:00,  5.18it/s, loss=1.41, v_num=0, val_loss=1.550, val_acc_epoch=0.300, train_loss_step=1.320, train_loss_epoch=1.420, train_acc_epoch=0.355]\n","Epoch 26: 100% 12/12 [00:02<00:00,  5.34it/s, loss=1.41, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.350, train_loss_epoch=1.410, train_acc_epoch=0.385]\n","Epoch 27:  67% 8/12 [00:01<00:00,  4.39it/s, loss=1.38, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.410, train_acc_epoch=0.385]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 27:  83% 10/12 [00:02<00:00,  4.71it/s, loss=1.38, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.410, train_acc_epoch=0.385]\n","Epoch 27: 100% 12/12 [00:02<00:00,  5.28it/s, loss=1.38, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.410, train_acc_epoch=0.385]\n","Epoch 27: 100% 12/12 [00:02<00:00,  5.06it/s, loss=1.38, v_num=0, val_loss=1.520, val_acc_epoch=0.300, train_loss_step=1.320, train_loss_epoch=1.390, train_acc_epoch=0.425]\n","Epoch 28:  67% 8/12 [00:01<00:00,  4.75it/s, loss=1.38, v_num=0, val_loss=1.520, val_acc_epoch=0.300, train_loss_step=1.310, train_loss_epoch=1.390, train_acc_epoch=0.425]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.52it/s]\u001b[A\n","Epoch 28:  83% 10/12 [00:02<00:00,  4.75it/s, loss=1.38, v_num=0, val_loss=1.520, val_acc_epoch=0.300, train_loss_step=1.310, train_loss_epoch=1.390, train_acc_epoch=0.425]\n","Validating:  60% 3/5 [00:00<00:00,  5.96it/s]\u001b[A\n","Epoch 28: 100% 12/12 [00:02<00:00,  5.08it/s, loss=1.38, v_num=0, val_loss=1.520, val_acc_epoch=0.300, train_loss_step=1.310, train_loss_epoch=1.390, train_acc_epoch=0.425]\n","Epoch 28: 100% 12/12 [00:02<00:00,  4.72it/s, loss=1.38, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.370, train_acc_epoch=0.415]\n","Epoch 29:  67% 8/12 [00:02<00:01,  3.53it/s, loss=1.37, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.240, train_loss_epoch=1.370, train_acc_epoch=0.415]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.87it/s]\u001b[A\n","Epoch 29:  83% 10/12 [00:02<00:00,  3.77it/s, loss=1.37, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.240, train_loss_epoch=1.370, train_acc_epoch=0.415]\n","Validating:  60% 3/5 [00:00<00:00,  6.72it/s]\u001b[A\n","Epoch 29: 100% 12/12 [00:03<00:00,  3.98it/s, loss=1.37, v_num=0, val_loss=1.550, val_acc_epoch=0.313, train_loss_step=1.320, train_loss_epoch=1.370, train_acc_epoch=0.410]\n","Epoch 30:  67% 8/12 [00:02<00:01,  3.50it/s, loss=1.36, v_num=0, val_loss=1.550, val_acc_epoch=0.313, train_loss_step=1.200, train_loss_epoch=1.370, train_acc_epoch=0.410]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.59it/s]\u001b[A\n","Epoch 30:  83% 10/12 [00:02<00:00,  3.89it/s, loss=1.36, v_num=0, val_loss=1.550, val_acc_epoch=0.313, train_loss_step=1.200, train_loss_epoch=1.370, train_acc_epoch=0.410]\n","Epoch 30: 100% 12/12 [00:02<00:00,  4.25it/s, loss=1.36, v_num=0, val_loss=1.550, val_acc_epoch=0.307, train_loss_step=1.230, train_loss_epoch=1.360, train_acc_epoch=0.410]\n","Epoch 31:  67% 8/12 [00:01<00:00,  4.56it/s, loss=1.33, v_num=0, val_loss=1.550, val_acc_epoch=0.307, train_loss_step=1.240, train_loss_epoch=1.360, train_acc_epoch=0.410]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.56it/s]\u001b[A\n","Epoch 31:  83% 10/12 [00:02<00:00,  4.80it/s, loss=1.33, v_num=0, val_loss=1.550, val_acc_epoch=0.307, train_loss_step=1.240, train_loss_epoch=1.360, train_acc_epoch=0.410]\n","Epoch 31: 100% 12/12 [00:02<00:00,  5.04it/s, loss=1.33, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.040, train_loss_epoch=1.340, train_acc_epoch=0.435]\n","Epoch 32:  67% 8/12 [00:01<00:00,  4.58it/s, loss=1.31, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.210, train_loss_epoch=1.340, train_acc_epoch=0.435]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 32:  83% 10/12 [00:02<00:00,  4.96it/s, loss=1.31, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.210, train_loss_epoch=1.340, train_acc_epoch=0.435]\n","Epoch 32: 100% 12/12 [00:02<00:00,  5.51it/s, loss=1.31, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.210, train_loss_epoch=1.340, train_acc_epoch=0.435]\n","Epoch 32: 100% 12/12 [00:02<00:00,  5.23it/s, loss=1.31, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.240, train_loss_epoch=1.320, train_acc_epoch=0.425]\n","Epoch 33:  67% 8/12 [00:01<00:00,  4.66it/s, loss=1.31, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.220, train_loss_epoch=1.320, train_acc_epoch=0.425]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 33:  83% 10/12 [00:01<00:00,  5.01it/s, loss=1.31, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.220, train_loss_epoch=1.320, train_acc_epoch=0.425]\n","Epoch 33: 100% 12/12 [00:02<00:00,  5.53it/s, loss=1.31, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.220, train_loss_epoch=1.320, train_acc_epoch=0.425]\n","Epoch 33: 100% 12/12 [00:02<00:00,  5.25it/s, loss=1.31, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.130, train_loss_epoch=1.330, train_acc_epoch=0.395]\n","Epoch 34:  67% 8/12 [00:01<00:00,  4.13it/s, loss=1.29, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.330, train_acc_epoch=0.395]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.92it/s]\u001b[A\n","Epoch 34:  83% 10/12 [00:02<00:00,  4.31it/s, loss=1.29, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.330, train_acc_epoch=0.395]\n","Epoch 34: 100% 12/12 [00:02<00:00,  4.49it/s, loss=1.29, v_num=0, val_loss=1.550, val_acc_epoch=0.293, train_loss_step=0.974, train_loss_epoch=1.310, train_acc_epoch=0.420]\n","Epoch 35:  67% 8/12 [00:02<00:01,  3.77it/s, loss=1.29, v_num=0, val_loss=1.550, val_acc_epoch=0.293, train_loss_step=1.120, train_loss_epoch=1.310, train_acc_epoch=0.420]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.40it/s]\u001b[A\n","Epoch 35:  83% 10/12 [00:02<00:00,  3.88it/s, loss=1.29, v_num=0, val_loss=1.550, val_acc_epoch=0.293, train_loss_step=1.120, train_loss_epoch=1.310, train_acc_epoch=0.420]\n","Validating:  60% 3/5 [00:00<00:00,  5.82it/s]\u001b[A\n","Epoch 35: 100% 12/12 [00:03<00:00,  3.99it/s, loss=1.29, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.260, train_loss_epoch=1.290, train_acc_epoch=0.470]\n","Epoch 36:  67% 8/12 [00:01<00:00,  4.08it/s, loss=1.28, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.130, train_loss_epoch=1.290, train_acc_epoch=0.470]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.26it/s]\u001b[A\n","Epoch 36:  83% 10/12 [00:02<00:00,  4.43it/s, loss=1.28, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.130, train_loss_epoch=1.290, train_acc_epoch=0.470]\n","Epoch 36: 100% 12/12 [00:02<00:00,  4.73it/s, loss=1.28, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=0.988, train_loss_epoch=1.300, train_acc_epoch=0.475]\n","Epoch 37:  67% 8/12 [00:01<00:00,  4.82it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.400, train_loss_epoch=1.300, train_acc_epoch=0.475]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.96it/s]\u001b[A\n","Epoch 37:  83% 10/12 [00:01<00:00,  5.06it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.400, train_loss_epoch=1.300, train_acc_epoch=0.475]\n","Epoch 37: 100% 12/12 [00:02<00:00,  5.26it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.040, train_loss_epoch=1.320, train_acc_epoch=0.415]\n","Epoch 38:  67% 8/12 [00:01<00:00,  4.84it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.320, train_loss_epoch=1.320, train_acc_epoch=0.415]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.93it/s]\u001b[A\n","Epoch 38:  83% 10/12 [00:01<00:00,  5.09it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.320, train_loss_epoch=1.320, train_acc_epoch=0.415]\n","Epoch 38: 100% 12/12 [00:02<00:00,  5.34it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.220, train_loss_epoch=1.270, train_acc_epoch=0.480]\n","Epoch 39:  67% 8/12 [00:01<00:00,  4.32it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.270, train_acc_epoch=0.480]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 39:  83% 10/12 [00:02<00:00,  4.65it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.270, train_acc_epoch=0.480]\n","Epoch 39: 100% 12/12 [00:02<00:00,  5.14it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.270, train_acc_epoch=0.480]\n","Epoch 39: 100% 12/12 [00:02<00:00,  4.91it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.120, train_loss_epoch=1.290, train_acc_epoch=0.430]\n","Epoch 40:  67% 8/12 [00:01<00:00,  4.86it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.210, train_loss_epoch=1.290, train_acc_epoch=0.430]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.40it/s]\u001b[A\n","Epoch 40:  83% 10/12 [00:02<00:00,  4.98it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.210, train_loss_epoch=1.290, train_acc_epoch=0.430]\n","Validating:  60% 3/5 [00:00<00:00,  6.62it/s]\u001b[A\n","Epoch 40: 100% 12/12 [00:02<00:00,  5.30it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.210, train_loss_epoch=1.290, train_acc_epoch=0.430]\n","Epoch 40: 100% 12/12 [00:02<00:00,  4.92it/s, loss=1.25, v_num=0, val_loss=1.540, val_acc_epoch=0.333, train_loss_step=1.030, train_loss_epoch=1.260, train_acc_epoch=0.405]\n","Epoch 41:  67% 8/12 [00:02<00:01,  3.09it/s, loss=1.25, v_num=0, val_loss=1.540, val_acc_epoch=0.333, train_loss_step=1.190, train_loss_epoch=1.260, train_acc_epoch=0.405]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.46it/s]\u001b[A\n","Epoch 41:  83% 10/12 [00:03<00:00,  3.32it/s, loss=1.25, v_num=0, val_loss=1.540, val_acc_epoch=0.333, train_loss_step=1.190, train_loss_epoch=1.260, train_acc_epoch=0.405]\n","Validating:  60% 3/5 [00:00<00:00,  5.71it/s]\u001b[A\n","Epoch 41: 100% 12/12 [00:03<00:00,  3.65it/s, loss=1.25, v_num=0, val_loss=1.540, val_acc_epoch=0.333, train_loss_step=1.190, train_loss_epoch=1.260, train_acc_epoch=0.405]\n","Epoch 41: 100% 12/12 [00:03<00:00,  3.47it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.230, train_loss_epoch=1.280, train_acc_epoch=0.470]\n","Epoch 42:  67% 8/12 [00:02<00:01,  3.53it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.120, train_loss_epoch=1.280, train_acc_epoch=0.470]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 42:  83% 10/12 [00:02<00:00,  3.89it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.120, train_loss_epoch=1.280, train_acc_epoch=0.470]\n","Validating:  60% 3/5 [00:00<00:00,  8.19it/s]\u001b[A\n","Epoch 42: 100% 12/12 [00:02<00:00,  4.16it/s, loss=1.25, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.410, train_loss_epoch=1.230, train_acc_epoch=0.475]\n","Epoch 43:  67% 8/12 [00:01<00:00,  5.19it/s, loss=1.26, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.220, train_loss_epoch=1.230, train_acc_epoch=0.475]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 43:  83% 10/12 [00:01<00:00,  5.53it/s, loss=1.26, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.220, train_loss_epoch=1.230, train_acc_epoch=0.475]\n","Epoch 43: 100% 12/12 [00:01<00:00,  6.06it/s, loss=1.26, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.220, train_loss_epoch=1.230, train_acc_epoch=0.475]\n","Epoch 43: 100% 12/12 [00:02<00:00,  5.69it/s, loss=1.26, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.230, train_loss_epoch=1.250, train_acc_epoch=0.505]\n","Epoch 44:  67% 8/12 [00:01<00:00,  4.67it/s, loss=1.24, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.040, train_loss_epoch=1.250, train_acc_epoch=0.505]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 44:  83% 10/12 [00:01<00:00,  5.02it/s, loss=1.24, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.040, train_loss_epoch=1.250, train_acc_epoch=0.505]\n","Epoch 44: 100% 12/12 [00:02<00:00,  5.62it/s, loss=1.24, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.040, train_loss_epoch=1.250, train_acc_epoch=0.505]\n","Epoch 44: 100% 12/12 [00:02<00:00,  5.36it/s, loss=1.24, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.310, train_loss_epoch=1.230, train_acc_epoch=0.490]\n","Epoch 45:  67% 8/12 [00:01<00:00,  4.61it/s, loss=1.22, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.150, train_loss_epoch=1.230, train_acc_epoch=0.490]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 45:  83% 10/12 [00:01<00:00,  5.02it/s, loss=1.22, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.150, train_loss_epoch=1.230, train_acc_epoch=0.490]\n","Epoch 45: 100% 12/12 [00:02<00:00,  5.60it/s, loss=1.22, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.150, train_loss_epoch=1.230, train_acc_epoch=0.490]\n","Epoch 45: 100% 12/12 [00:02<00:00,  5.35it/s, loss=1.22, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.110, train_loss_epoch=1.210, train_acc_epoch=0.525]\n","Epoch 46:  67% 8/12 [00:01<00:00,  4.37it/s, loss=1.23, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.140, train_loss_epoch=1.210, train_acc_epoch=0.525]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.03it/s]\u001b[A\n","Epoch 46:  83% 10/12 [00:02<00:00,  4.53it/s, loss=1.23, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.140, train_loss_epoch=1.210, train_acc_epoch=0.525]\n","Validating:  60% 3/5 [00:00<00:00,  6.58it/s]\u001b[A\n","Epoch 46: 100% 12/12 [00:02<00:00,  4.65it/s, loss=1.23, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.410, train_loss_epoch=1.260, train_acc_epoch=0.445]\n","Epoch 47:  67% 8/12 [00:02<00:01,  3.27it/s, loss=1.21, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.240, train_loss_epoch=1.260, train_acc_epoch=0.445]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.49it/s]\u001b[A\n","Epoch 47:  83% 10/12 [00:02<00:00,  3.49it/s, loss=1.21, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.240, train_loss_epoch=1.260, train_acc_epoch=0.445]\n","Validating:  60% 3/5 [00:00<00:00,  5.76it/s]\u001b[A\n","Epoch 47: 100% 12/12 [00:03<00:00,  3.62it/s, loss=1.21, v_num=0, val_loss=1.500, val_acc_epoch=0.320, train_loss_step=1.040, train_loss_epoch=1.200, train_acc_epoch=0.540]\n","Epoch 48:  67% 8/12 [00:02<00:01,  3.37it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.320, train_loss_step=1.070, train_loss_epoch=1.200, train_acc_epoch=0.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 48:  83% 10/12 [00:02<00:00,  3.75it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.320, train_loss_step=1.070, train_loss_epoch=1.200, train_acc_epoch=0.540]\n","Epoch 48: 100% 12/12 [00:02<00:00,  4.24it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.320, train_loss_step=1.070, train_loss_epoch=1.200, train_acc_epoch=0.540]\n","Epoch 48: 100% 12/12 [00:02<00:00,  4.10it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=0.966, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Epoch 49:  67% 8/12 [00:01<00:00,  4.57it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.75it/s]\u001b[A\n","Epoch 49:  83% 10/12 [00:02<00:00,  4.84it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Epoch 49: 100% 12/12 [00:02<00:00,  5.09it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.210, train_loss_epoch=1.160, train_acc_epoch=0.560]\n","Epoch 50:  67% 8/12 [00:01<00:00,  4.24it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.160, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.97it/s]\u001b[A\n","Epoch 50:  83% 10/12 [00:02<00:00,  4.55it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.160, train_acc_epoch=0.560]\n","Epoch 50: 100% 12/12 [00:02<00:00,  4.84it/s, loss=1.16, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.340, train_loss_epoch=1.130, train_acc_epoch=0.535]\n","Epoch 51:  67% 8/12 [00:01<00:00,  4.54it/s, loss=1.17, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.150, train_loss_epoch=1.130, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.86it/s]\u001b[A\n","Epoch 51:  83% 10/12 [00:02<00:00,  4.81it/s, loss=1.17, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.150, train_loss_epoch=1.130, train_acc_epoch=0.535]\n","Epoch 51: 100% 12/12 [00:02<00:00,  5.15it/s, loss=1.17, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.290, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Epoch 52:  67% 8/12 [00:01<00:00,  4.54it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.27it/s]\u001b[A\n","Epoch 52:  83% 10/12 [00:02<00:00,  4.52it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Validating:  60% 3/5 [00:00<00:00,  6.08it/s]\u001b[A\n","Epoch 52: 100% 12/12 [00:02<00:00,  4.92it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Epoch 52: 100% 12/12 [00:02<00:00,  4.57it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.340, train_loss_step=1.440, train_loss_epoch=1.180, train_acc_epoch=0.515]\n","Epoch 53:  67% 8/12 [00:02<00:01,  3.10it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.340, train_loss_step=1.130, train_loss_epoch=1.180, train_acc_epoch=0.515]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.28it/s]\u001b[A\n","Epoch 53:  83% 10/12 [00:03<00:00,  3.31it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.340, train_loss_step=1.130, train_loss_epoch=1.180, train_acc_epoch=0.515]\n","Validating:  60% 3/5 [00:00<00:00,  6.06it/s]\u001b[A\n","Epoch 53: 100% 12/12 [00:03<00:00,  3.67it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.340, train_loss_step=1.130, train_loss_epoch=1.180, train_acc_epoch=0.515]\n","Epoch 53: 100% 12/12 [00:03<00:00,  3.46it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.160, train_loss_epoch=1.190, train_acc_epoch=0.500]\n","Epoch 54:  67% 8/12 [00:02<00:01,  3.85it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.110, train_loss_epoch=1.190, train_acc_epoch=0.500]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.55it/s]\u001b[A\n","Epoch 54:  83% 10/12 [00:02<00:00,  4.14it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.110, train_loss_epoch=1.190, train_acc_epoch=0.500]\n","Epoch 54: 100% 12/12 [00:02<00:00,  4.42it/s, loss=1.19, v_num=0, val_loss=1.550, val_acc_epoch=0.327, train_loss_step=1.160, train_loss_epoch=1.180, train_acc_epoch=0.535]\n","Epoch 55:  67% 8/12 [00:01<00:00,  4.90it/s, loss=1.17, v_num=0, val_loss=1.550, val_acc_epoch=0.327, train_loss_step=1.230, train_loss_epoch=1.180, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 55:  83% 10/12 [00:01<00:00,  5.25it/s, loss=1.17, v_num=0, val_loss=1.550, val_acc_epoch=0.327, train_loss_step=1.230, train_loss_epoch=1.180, train_acc_epoch=0.535]\n","Epoch 55: 100% 12/12 [00:02<00:00,  5.85it/s, loss=1.17, v_num=0, val_loss=1.550, val_acc_epoch=0.327, train_loss_step=1.230, train_loss_epoch=1.180, train_acc_epoch=0.535]\n","Validating: 100% 5/5 [00:00<00:00, 11.28it/s]\u001b[AEpoch 00056: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 55: 100% 12/12 [00:02<00:00,  5.58it/s, loss=1.17, v_num=0, val_loss=1.510, val_acc_epoch=0.327, train_loss_step=1.190, train_loss_epoch=1.150, train_acc_epoch=0.545]\n","Epoch 56:  67% 8/12 [00:01<00:00,  4.48it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.327, train_loss_step=1.040, train_loss_epoch=1.150, train_acc_epoch=0.545]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.86it/s]\u001b[A\n","Epoch 56:  83% 10/12 [00:02<00:00,  4.76it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.327, train_loss_step=1.040, train_loss_epoch=1.150, train_acc_epoch=0.545]\n","Epoch 56: 100% 12/12 [00:02<00:00,  4.94it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.170, train_loss_epoch=1.150, train_acc_epoch=0.555]\n","Epoch 57:  67% 8/12 [00:01<00:00,  4.86it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.150, train_acc_epoch=0.555]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  6.04it/s]\u001b[A\n","Epoch 57:  83% 10/12 [00:01<00:00,  5.22it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.150, train_acc_epoch=0.555]\n","Epoch 57: 100% 12/12 [00:02<00:00,  5.57it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.100, train_loss_epoch=1.150, train_acc_epoch=0.560]\n","Epoch 58:  67% 8/12 [00:01<00:00,  4.57it/s, loss=1.13, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.110, train_loss_epoch=1.150, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.47it/s]\u001b[A\n","Epoch 58:  83% 10/12 [00:02<00:00,  4.58it/s, loss=1.13, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.110, train_loss_epoch=1.150, train_acc_epoch=0.560]\n","Validating:  60% 3/5 [00:00<00:00,  6.04it/s]\u001b[A\n","Epoch 58: 100% 12/12 [00:02<00:00,  4.93it/s, loss=1.13, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.110, train_loss_epoch=1.150, train_acc_epoch=0.560]\n","Epoch 58: 100% 12/12 [00:02<00:00,  4.59it/s, loss=1.13, v_num=0, val_loss=1.520, val_acc_epoch=0.307, train_loss_step=1.110, train_loss_epoch=1.120, train_acc_epoch=0.565]\n","Epoch 59:  67% 8/12 [00:02<00:01,  3.10it/s, loss=1.13, v_num=0, val_loss=1.520, val_acc_epoch=0.307, train_loss_step=0.883, train_loss_epoch=1.120, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.45it/s]\u001b[A\n","Epoch 59:  83% 10/12 [00:03<00:00,  3.32it/s, loss=1.13, v_num=0, val_loss=1.520, val_acc_epoch=0.307, train_loss_step=0.883, train_loss_epoch=1.120, train_acc_epoch=0.565]\n","Validating:  60% 3/5 [00:00<00:00,  5.93it/s]\u001b[A\n","Epoch 59: 100% 12/12 [00:03<00:00,  3.68it/s, loss=1.13, v_num=0, val_loss=1.520, val_acc_epoch=0.307, train_loss_step=0.883, train_loss_epoch=1.120, train_acc_epoch=0.565]\n","Epoch 59: 100% 12/12 [00:03<00:00,  3.48it/s, loss=1.13, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.230, train_loss_epoch=1.140, train_acc_epoch=0.570]\n","Epoch 60:  67% 8/12 [00:02<00:01,  3.74it/s, loss=1.14, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.220, train_loss_epoch=1.140, train_acc_epoch=0.570]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 60:  83% 10/12 [00:02<00:00,  4.15it/s, loss=1.14, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.220, train_loss_epoch=1.140, train_acc_epoch=0.570]\n","Epoch 60: 100% 12/12 [00:02<00:00,  4.68it/s, loss=1.14, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.220, train_loss_epoch=1.140, train_acc_epoch=0.570]\n","Epoch 60: 100% 12/12 [00:02<00:00,  4.47it/s, loss=1.14, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=0.977, train_loss_epoch=1.160, train_acc_epoch=0.545]\n","Epoch 61:  67% 8/12 [00:01<00:00,  4.32it/s, loss=1.14, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.160, train_acc_epoch=0.545]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.80it/s]\u001b[A\n","Epoch 61:  83% 10/12 [00:02<00:00,  4.59it/s, loss=1.14, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.160, train_acc_epoch=0.545]\n","Epoch 61: 100% 12/12 [00:02<00:00,  4.84it/s, loss=1.14, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.120, train_loss_epoch=1.130, train_acc_epoch=0.545]\n","Epoch 62:  67% 8/12 [00:01<00:00,  4.91it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.010, train_loss_epoch=1.130, train_acc_epoch=0.545]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 62:  83% 10/12 [00:01<00:00,  5.27it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.010, train_loss_epoch=1.130, train_acc_epoch=0.545]\n","Epoch 62: 100% 12/12 [00:02<00:00,  5.84it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.010, train_loss_epoch=1.130, train_acc_epoch=0.545]\n","Epoch 62: 100% 12/12 [00:02<00:00,  5.53it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.020, train_loss_epoch=1.110, train_acc_epoch=0.560]\n","Epoch 63:  67% 8/12 [00:01<00:00,  5.00it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=0.978, train_loss_epoch=1.110, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.98it/s]\u001b[A\n","Epoch 63:  83% 10/12 [00:01<00:00,  5.18it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=0.978, train_loss_epoch=1.110, train_acc_epoch=0.560]\n","Epoch 63: 100% 12/12 [00:02<00:00,  5.32it/s, loss=1.12, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.210, train_loss_epoch=1.130, train_acc_epoch=0.525]\n","Epoch 64:  67% 8/12 [00:01<00:00,  5.02it/s, loss=1.1, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.100, train_loss_epoch=1.130, train_acc_epoch=0.525]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.35it/s]\u001b[A\n","Epoch 64:  83% 10/12 [00:01<00:00,  5.21it/s, loss=1.1, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.100, train_loss_epoch=1.130, train_acc_epoch=0.525]\n","Validating:  60% 3/5 [00:00<00:00,  7.49it/s]\u001b[A\n","Epoch 64: 100% 12/12 [00:02<00:00,  5.63it/s, loss=1.1, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.100, train_loss_epoch=1.130, train_acc_epoch=0.525]\n","Epoch 64: 100% 12/12 [00:02<00:00,  5.19it/s, loss=1.1, v_num=0, val_loss=1.520, val_acc_epoch=0.327, train_loss_step=0.700, train_loss_epoch=1.100, train_acc_epoch=0.565]\n","Epoch 65:  67% 8/12 [00:02<00:01,  3.17it/s, loss=1.11, v_num=0, val_loss=1.520, val_acc_epoch=0.327, train_loss_step=1.060, train_loss_epoch=1.100, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.52it/s]\u001b[A\n","Epoch 65:  83% 10/12 [00:02<00:00,  3.40it/s, loss=1.11, v_num=0, val_loss=1.520, val_acc_epoch=0.327, train_loss_step=1.060, train_loss_epoch=1.100, train_acc_epoch=0.565]\n","Validating:  60% 3/5 [00:00<00:00,  6.59it/s]\u001b[A\n","Epoch 65: 100% 12/12 [00:03<00:00,  3.63it/s, loss=1.11, v_num=0, val_loss=1.520, val_acc_epoch=0.333, train_loss_step=1.010, train_loss_epoch=1.150, train_acc_epoch=0.540]\n","Epoch 66:  67% 8/12 [00:02<00:01,  3.69it/s, loss=1.09, v_num=0, val_loss=1.520, val_acc_epoch=0.333, train_loss_step=1.070, train_loss_epoch=1.150, train_acc_epoch=0.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 66:  83% 10/12 [00:02<00:00,  4.11it/s, loss=1.09, v_num=0, val_loss=1.520, val_acc_epoch=0.333, train_loss_step=1.070, train_loss_epoch=1.150, train_acc_epoch=0.540]\n","Epoch 66: 100% 12/12 [00:02<00:00,  4.63it/s, loss=1.09, v_num=0, val_loss=1.520, val_acc_epoch=0.333, train_loss_step=1.070, train_loss_epoch=1.150, train_acc_epoch=0.540]\n","Epoch 66: 100% 12/12 [00:02<00:00,  4.43it/s, loss=1.09, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.050, train_loss_epoch=1.100, train_acc_epoch=0.535]\n","Epoch 67:  67% 8/12 [00:01<00:00,  4.27it/s, loss=1.13, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.160, train_loss_epoch=1.100, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.76it/s]\u001b[A\n","Epoch 67:  83% 10/12 [00:02<00:00,  4.58it/s, loss=1.13, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.160, train_loss_epoch=1.100, train_acc_epoch=0.535]\n","Epoch 67: 100% 12/12 [00:02<00:00,  5.09it/s, loss=1.13, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.160, train_loss_epoch=1.100, train_acc_epoch=0.535]Epoch 00068: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 67: 100% 12/12 [00:02<00:00,  4.84it/s, loss=1.13, v_num=0, val_loss=1.540, val_acc_epoch=0.327, train_loss_step=1.590, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Epoch 68:  67% 8/12 [00:01<00:00,  4.14it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.327, train_loss_step=0.902, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.95it/s]\u001b[A\n","Epoch 68:  83% 10/12 [00:02<00:00,  4.46it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.327, train_loss_step=0.902, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Epoch 68: 100% 12/12 [00:02<00:00,  4.67it/s, loss=1.11, v_num=0, val_loss=1.530, val_acc_epoch=0.333, train_loss_step=0.978, train_loss_epoch=1.070, train_acc_epoch=0.570]\n","Epoch 69:  67% 8/12 [00:01<00:00,  4.33it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.333, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.570]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.66it/s]\u001b[A\n","Epoch 69:  83% 10/12 [00:02<00:00,  4.59it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.333, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.570]\n","Epoch 69: 100% 12/12 [00:02<00:00,  4.86it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.320, train_loss_step=0.919, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Epoch 70:  67% 8/12 [00:01<00:00,  4.17it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.320, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.11it/s]\u001b[A\n","Epoch 70:  83% 10/12 [00:02<00:00,  4.40it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.320, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Validating:  60% 3/5 [00:00<00:00,  7.18it/s]\u001b[A\n","Epoch 70: 100% 12/12 [00:02<00:00,  4.54it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.310, train_loss_epoch=1.140, train_acc_epoch=0.530]\n","Epoch 71:  67% 8/12 [00:02<00:01,  3.19it/s, loss=1.11, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.140, train_acc_epoch=0.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.31it/s]\u001b[A\n","Epoch 71:  83% 10/12 [00:02<00:00,  3.38it/s, loss=1.11, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.140, train_acc_epoch=0.530]\n","Validating:  60% 3/5 [00:00<00:00,  5.90it/s]\u001b[A\n","Epoch 71: 100% 12/12 [00:03<00:00,  3.58it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.858, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Epoch 72:  67% 8/12 [00:02<00:01,  3.87it/s, loss=1.12, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.030, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.89it/s]\u001b[A\n","Epoch 72:  83% 10/12 [00:02<00:00,  4.21it/s, loss=1.12, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.030, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Epoch 72: 100% 12/12 [00:02<00:00,  4.51it/s, loss=1.12, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.970, train_loss_epoch=1.140, train_acc_epoch=0.520]\n","Epoch 73:  67% 8/12 [00:01<00:00,  4.28it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.140, train_loss_epoch=1.140, train_acc_epoch=0.520]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.97it/s]\u001b[A\n","Epoch 73:  83% 10/12 [00:02<00:00,  4.59it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.140, train_loss_epoch=1.140, train_acc_epoch=0.520]\n","Epoch 73: 100% 12/12 [00:02<00:00,  4.94it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.661, train_loss_epoch=1.090, train_acc_epoch=0.540]\n","Epoch 74:  67% 8/12 [00:01<00:00,  5.00it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.160, train_loss_epoch=1.090, train_acc_epoch=0.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 74:  83% 10/12 [00:01<00:00,  5.36it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.160, train_loss_epoch=1.090, train_acc_epoch=0.540]\n","Epoch 74: 100% 12/12 [00:02<00:00,  5.95it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.160, train_loss_epoch=1.090, train_acc_epoch=0.540]\n","Epoch 74: 100% 12/12 [00:02<00:00,  5.66it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.110, train_loss_epoch=1.110, train_acc_epoch=0.540]\n","Epoch 75:  67% 8/12 [00:01<00:00,  5.02it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.110, train_acc_epoch=0.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 75:  83% 10/12 [00:01<00:00,  5.38it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.110, train_acc_epoch=0.540]\n","Epoch 75: 100% 12/12 [00:02<00:00,  5.97it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.110, train_acc_epoch=0.540]\n","Epoch 75: 100% 12/12 [00:02<00:00,  5.70it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.882, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Epoch 76:  67% 8/12 [00:01<00:00,  5.23it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.080, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.40it/s]\u001b[A\n","Epoch 76:  83% 10/12 [00:01<00:00,  5.43it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.080, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Validating:  60% 3/5 [00:00<00:00,  7.78it/s]\u001b[A\n","Epoch 76: 100% 12/12 [00:02<00:00,  5.48it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.815, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Epoch 77:  67% 8/12 [00:02<00:01,  3.15it/s, loss=1.05, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.985, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.94it/s]\u001b[A\n","Epoch 77:  83% 10/12 [00:02<00:00,  3.34it/s, loss=1.05, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.985, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Validating:  60% 3/5 [00:00<00:00,  6.10it/s]\u001b[A\n","Epoch 77: 100% 12/12 [00:03<00:00,  3.58it/s, loss=1.05, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.985, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 78:  67% 8/12 [00:02<00:01,  3.18it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.92it/s]\u001b[A\n","Epoch 78:  83% 10/12 [00:02<00:00,  3.34it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating:  60% 3/5 [00:00<00:00,  5.49it/s]\u001b[A\n","Epoch 78: 100% 12/12 [00:03<00:00,  3.68it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating: 100% 5/5 [00:00<00:00,  7.04it/s]\u001b[AEpoch 00079: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 78: 100% 12/12 [00:03<00:00,  3.49it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.969, train_loss_epoch=1.080, train_acc_epoch=0.550]\n","Epoch 79:  67% 8/12 [00:02<00:01,  2.83it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.992, train_loss_epoch=1.080, train_acc_epoch=0.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.90it/s]\u001b[A\n","Epoch 79:  83% 10/12 [00:03<00:00,  3.00it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.992, train_loss_epoch=1.080, train_acc_epoch=0.550]\n","Validating:  60% 3/5 [00:00<00:00,  5.30it/s]\u001b[A\n","Epoch 79: 100% 12/12 [00:03<00:00,  3.34it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.992, train_loss_epoch=1.080, train_acc_epoch=0.550]\n","Epoch 79: 100% 12/12 [00:03<00:00,  3.16it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.883, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 80:  67% 8/12 [00:01<00:00,  4.72it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.905, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 80:  83% 10/12 [00:02<00:00,  4.94it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.905, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 80: 100% 12/12 [00:02<00:00,  5.49it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.905, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 80: 100% 12/12 [00:02<00:00,  5.19it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.140, train_loss_epoch=1.120, train_acc_epoch=0.575]\n","Epoch 81:  67% 8/12 [00:01<00:00,  4.74it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.250, train_loss_epoch=1.120, train_acc_epoch=0.575]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 81:  83% 10/12 [00:01<00:00,  5.07it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.250, train_loss_epoch=1.120, train_acc_epoch=0.575]\n","Epoch 81: 100% 12/12 [00:02<00:00,  5.58it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.250, train_loss_epoch=1.120, train_acc_epoch=0.575]\n","Epoch 81: 100% 12/12 [00:02<00:00,  5.29it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.130, train_acc_epoch=0.520]\n","Epoch 82:  67% 8/12 [00:01<00:00,  4.46it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.130, train_acc_epoch=0.520]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 82:  83% 10/12 [00:02<00:00,  4.84it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.130, train_acc_epoch=0.520]\n","Epoch 82: 100% 12/12 [00:02<00:00,  5.42it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.130, train_acc_epoch=0.520]\n","Epoch 82: 100% 12/12 [00:02<00:00,  5.15it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.080, train_loss_epoch=1.080, train_acc_epoch=0.565]\n","Epoch 83:  67% 8/12 [00:01<00:00,  5.01it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.190, train_loss_epoch=1.080, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.66it/s]\u001b[A\n","Epoch 83:  83% 10/12 [00:01<00:00,  5.22it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.190, train_loss_epoch=1.080, train_acc_epoch=0.565]\n","Epoch 83: 100% 12/12 [00:02<00:00,  5.66it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.190, train_loss_epoch=1.080, train_acc_epoch=0.565]\n","Epoch 83: 100% 12/12 [00:02<00:00,  5.35it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.989, train_loss_epoch=1.090, train_acc_epoch=0.595]\n","Epoch 84:  67% 8/12 [00:02<00:01,  3.15it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.120, train_loss_epoch=1.090, train_acc_epoch=0.595]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.22it/s]\u001b[A\n","Epoch 84:  83% 10/12 [00:02<00:00,  3.46it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.120, train_loss_epoch=1.090, train_acc_epoch=0.595]\n","Validating:  60% 3/5 [00:00<00:00,  6.93it/s]\u001b[A\n","Epoch 84: 100% 12/12 [00:03<00:00,  3.69it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.888, train_loss_epoch=1.070, train_acc_epoch=0.550]\n","Epoch 85:  67% 8/12 [00:02<00:01,  3.28it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 85:  83% 10/12 [00:02<00:00,  3.58it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.550]\n","Epoch 85: 100% 12/12 [00:02<00:00,  4.02it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.550]\n","Epoch 85: 100% 12/12 [00:03<00:00,  3.88it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.170, train_loss_epoch=1.090, train_acc_epoch=0.570]\n","Epoch 86:  67% 8/12 [00:01<00:00,  4.34it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.090, train_acc_epoch=0.570]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 86:  83% 10/12 [00:02<00:00,  4.68it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.090, train_acc_epoch=0.570]\n","Epoch 86: 100% 12/12 [00:02<00:00,  5.23it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.090, train_acc_epoch=0.570]\n","Epoch 86: 100% 12/12 [00:02<00:00,  5.02it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.030, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Epoch 87:  67% 8/12 [00:01<00:00,  4.36it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.030, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.01it/s]\u001b[A\n","Epoch 87:  83% 10/12 [00:02<00:00,  4.68it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.030, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Epoch 87: 100% 12/12 [00:02<00:00,  5.17it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.030, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Epoch 87: 100% 12/12 [00:02<00:00,  4.90it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.951, train_loss_epoch=1.090, train_acc_epoch=0.565]\n","Epoch 88:  67% 8/12 [00:01<00:00,  4.79it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.090, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 88:  83% 10/12 [00:01<00:00,  5.03it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.090, train_acc_epoch=0.565]\n","Epoch 88: 100% 12/12 [00:02<00:00,  5.49it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.090, train_acc_epoch=0.565]\n","Epoch 88: 100% 12/12 [00:02<00:00,  5.20it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.000, train_loss_epoch=1.100, train_acc_epoch=0.585]\n","Epoch 89:  67% 8/12 [00:01<00:00,  4.19it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.080, train_loss_epoch=1.100, train_acc_epoch=0.585]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.57it/s]\u001b[A\n","Epoch 89:  83% 10/12 [00:02<00:00,  4.48it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.080, train_loss_epoch=1.100, train_acc_epoch=0.585]\n","Epoch 89: 100% 12/12 [00:02<00:00,  4.96it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.080, train_loss_epoch=1.100, train_acc_epoch=0.585]\n","Validating: 100% 5/5 [00:00<00:00,  8.82it/s]\u001b[AEpoch 00090: reducing learning rate of group 0 to 6.2500e-06.\n","Epoch 89: 100% 12/12 [00:02<00:00,  4.62it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.100, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 90:  67% 8/12 [00:02<00:01,  3.08it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.100, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.08it/s]\u001b[A\n","Epoch 90:  83% 10/12 [00:03<00:00,  3.26it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.100, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating:  60% 3/5 [00:00<00:00,  5.55it/s]\u001b[A\n","Epoch 90: 100% 12/12 [00:03<00:00,  3.60it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.100, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 90: 100% 12/12 [00:03<00:00,  3.40it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.170, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 91:  67% 8/12 [00:02<00:01,  3.19it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.040, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.95it/s]\u001b[A\n","Epoch 91:  83% 10/12 [00:02<00:00,  3.54it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.040, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 91: 100% 12/12 [00:03<00:00,  3.88it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.010, train_loss_epoch=1.070, train_acc_epoch=0.565]\n","Epoch 92:  67% 8/12 [00:01<00:00,  4.62it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.070, train_loss_epoch=1.070, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 92:  83% 10/12 [00:02<00:00,  4.93it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.070, train_loss_epoch=1.070, train_acc_epoch=0.565]\n","Epoch 92: 100% 12/12 [00:02<00:00,  5.48it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.070, train_loss_epoch=1.070, train_acc_epoch=0.565]\n","Epoch 92: 100% 12/12 [00:02<00:00,  5.25it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.969, train_loss_epoch=1.130, train_acc_epoch=0.500]\n","Epoch 93:  67% 8/12 [00:01<00:00,  5.20it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.090, train_loss_epoch=1.130, train_acc_epoch=0.500]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  6.10it/s]\u001b[A\n","Epoch 93:  83% 10/12 [00:01<00:00,  5.52it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.090, train_loss_epoch=1.130, train_acc_epoch=0.500]\n","Epoch 93: 100% 12/12 [00:02<00:00,  5.62it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.946, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Epoch 94:  67% 8/12 [00:01<00:00,  4.78it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.938, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.44it/s]\u001b[A\n","Epoch 94:  83% 10/12 [00:02<00:00,  4.97it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.938, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Epoch 94: 100% 12/12 [00:02<00:00,  5.16it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Epoch 95:  67% 8/12 [00:01<00:00,  4.17it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.942, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.89it/s]\u001b[A\n","Epoch 95:  83% 10/12 [00:02<00:00,  4.49it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.942, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Validating:  60% 3/5 [00:00<00:00,  7.45it/s]\u001b[A\n","Epoch 95: 100% 12/12 [00:02<00:00,  4.86it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.942, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Epoch 95: 100% 12/12 [00:02<00:00,  4.60it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.956, train_loss_epoch=1.030, train_acc_epoch=0.625]\n","Epoch 96:  67% 8/12 [00:02<00:01,  3.25it/s, loss=1.03, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.902, train_loss_epoch=1.030, train_acc_epoch=0.625]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.50it/s]\u001b[A\n","Epoch 96:  83% 10/12 [00:02<00:00,  3.47it/s, loss=1.03, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.902, train_loss_epoch=1.030, train_acc_epoch=0.625]\n","Validating:  60% 3/5 [00:00<00:00,  6.44it/s]\u001b[A\n","Epoch 96: 100% 12/12 [00:03<00:00,  3.67it/s, loss=1.03, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.140, train_loss_epoch=1.030, train_acc_epoch=0.610]\n","Epoch 97:  67% 8/12 [00:02<00:01,  3.38it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.030, train_acc_epoch=0.610]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.88it/s]\u001b[A\n","Epoch 97:  83% 10/12 [00:02<00:00,  3.73it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.030, train_acc_epoch=0.610]\n","Epoch 97: 100% 12/12 [00:03<00:00,  3.99it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.230, train_loss_epoch=1.070, train_acc_epoch=0.610]\n","Epoch 98:  67% 8/12 [00:01<00:00,  4.84it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.060, train_loss_epoch=1.070, train_acc_epoch=0.610]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.86it/s]\u001b[A\n","Epoch 98:  83% 10/12 [00:01<00:00,  5.08it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.060, train_loss_epoch=1.070, train_acc_epoch=0.610]\n","Epoch 98: 100% 12/12 [00:02<00:00,  5.29it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.858, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 99:  67% 8/12 [00:01<00:00,  4.88it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.923, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 99:  83% 10/12 [00:01<00:00,  5.27it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.923, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 99: 100% 12/12 [00:02<00:00,  5.84it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.923, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 99: 100% 12/12 [00:02<00:00,  5.56it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.796, train_loss_epoch=1.050, train_acc_epoch=0.620]\n","Epoch 99: 100% 12/12 [00:02<00:00,  5.55it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.796, train_loss_epoch=1.050, train_acc_epoch=0.620]\n","Testing: 100% 7/7 [00:00<00:00, 11.12it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test_acc_best_epoch': 0.42500001192092896, 'test_loss': 1.3911406993865967}\n","--------------------------------------------------------------------------------\n","Path to best model found during training: \n","/content/outputs/2023-06-11/20-35-11/lightning_logs/version_0/checkpoints/56-398.ckpt\n"]}]},{"cell_type":"markdown","source":["## Supervised learning only"],"metadata":{"id":"NyPOPTmDOePz"}},{"cell_type":"code","source":["# bare SUP\n","\n","!python /content/saint/main.py experiment=supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_sup"],"metadata":{"id":"dEoRCf56GlGD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686513770596,"user_tz":-420,"elapsed":2508,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"0e6d4763-99ab-4da9-e88d-25b9f4f15ece"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/saint/main.py\", line 4, in <module>\n","    from pytorch_lightning import seed_everything\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/__init__.py\", line 20, in <module>\n","    from pytorch_lightning import metrics  # noqa: E402\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/metrics/__init__.py\", line 15, in <module>\n","    from pytorch_lightning.metrics.classification import (  # noqa: F401\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/metrics/classification/__init__.py\", line 14, in <module>\n","    from pytorch_lightning.metrics.classification.accuracy import Accuracy  # noqa: F401\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/metrics/classification/accuracy.py\", line 16, in <module>\n","    from torchmetrics import Accuracy as _Accuracy\n","  File \"/usr/local/lib/python3.10/dist-packages/torchmetrics/__init__.py\", line 14, in <module>\n","    from torchmetrics.average import AverageMeter  # noqa: F401 E402\n","  File \"/usr/local/lib/python3.10/dist-packages/torchmetrics/average.py\", line 16, in <module>\n","    import torch\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1465, in <module>\n","    from . import _meta_registrations\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 7, in <module>\n","    from torch._decomp import _add_op_to_registry, global_decomposition_table, meta_table\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 169, in <module>\n","    import torch._decomp.decompositions\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/decompositions.py\", line 10, in <module>\n","    import torch._prims as prims\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\", line 33, in <module>\n","    from torch._subclasses.fake_tensor import FakeTensor, FakeTensorMode\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/__init__.py\", line 3, in <module>\n","    from torch._subclasses.fake_tensor import (\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 13, in <module>\n","    from torch._guards import Source\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_guards.py\", line 14, in <module>\n","    import sympy  # type: ignore[import]\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\", line 196, in <module>\n","    from .geometry import (Point, Point2D, Point3D, Line, Ray, Segment, Line2D,\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/geometry/__init__.py\", line 22, in <module>\n","    from sympy.geometry.curve import Curve\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 937, in _find_spec\n","  File \"<frozen importlib._bootstrap>\", line 893, in __enter__\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"gvrjFwH2JpA-"}},{"cell_type":"code","source":["pretrained_checkpoint = \"/content/outputs/2023-06-11/20-03-11/lightning_logs/version_0/checkpoints/56-398.ckpt\""],"metadata":{"id":"pbNGb90_qzMt","executionInfo":{"status":"ok","timestamp":1686514137885,"user_tz":-420,"elapsed":378,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["!python /content/saint/predict.py experiment=predict \\\n","  experiment.model=saint \\\n","  data=bank_sup \\\n","  data.data_folder=/content/saint/data \\\n","  experiment.pretrained_checkpoint={pretrained_checkpoint} \\\n","  experiment.pred_sav_path=/content/predict.csv"],"metadata":{"id":"2BkcD_bOHJlX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686514897120,"user_tz":-420,"elapsed":10723,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"82f1be31-e3c8-4d4d-93ae-063477d8e21e"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 20:21:32.260458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-11 20:21:33.540769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/predict.py:15: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","{'seed': 1234, 'transformer': {'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'dropout_ff': 0.1, 'embed_dim': 32, 'd_ff': 32, 'cls_token_idx': 0}, 'augmentation': {'prob_cutmix': 0.3, 'alpha': 0.2, 'lambda_pt': 10}, 'optimizer': {'temperature': 0.7, 'proj_head_dim': 128, 'beta_1': 0.9, 'beta_2': 0.99, 'lr': 0.0001, 'weight_decay': 0.01, 'optim': 'adamw', 'metric': 'auroc'}, 'preproc': {'data_folder': None, 'train_split': 0.65, 'validation_split': 0.15, 'test_split': 0.2, 'num_supervised_train_data': None}, 'callback': {'monitor': 'val_loss', 'mode': 'min', 'auto_insert_metric_name': False}, 'trainer': {'max_epochs': 100, 'deterministic': True, 'default_root_dir': None}, 'dataloader': {'shuffle_val': False, 'train_bs': 32, 'val_bs': 32, 'test_bs': 16, 'num_workers': 2, 'pin_memory': False}, 'metric': '${optimizer.metric}', 'print_config': False, 'experiment': {'model': 'saint', 'task': 'classification', 'pretrained_checkpoint': '/content/outputs/2023-06-11/20-03-11/lightning_logs/version_0/checkpoints/56-398.ckpt', 'num_output': 1, 'pred_sav_path': '/content/predict.csv', 'save_prediction': True, 'id_col': 'index', 'target_col': 'target'}, 'data': {'data_folder': '/content/saint/data', 'data_paths': {'train_csv_path': '${data.data_folder}/train.csv', 'train_y_csv_path': '${data.data_folder}/train_y.csv', 'val_csv_path': '${data.data_folder}/val.csv', 'val_y_csv_path': '${data.data_folder}/val_y.csv', 'test_csv_path': '${data.data_folder}/test.csv', 'test_y_csv_path': '${data.data_folder}/test_y.csv'}, 'data_stats': {'no_cat': 1, 'no_num': 49, 'cats': [1]}}}\n","/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)\n","Prediction finished,  csv saved at /content/predict.csv\n"]}]},{"cell_type":"code","source":["pred = pd.read_csv(\"/content/predict.csv\")\n","pred['target'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkms-sdgJFYA","executionInfo":{"status":"ok","timestamp":1686515329523,"user_tz":-420,"elapsed":1064,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"e5726bc0-423d-45da-d0f0-3dd93a68dac3"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    160\n","1     40\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":[],"metadata":{"id":"V9pdLifpJ0ug"},"execution_count":null,"outputs":[]}]}