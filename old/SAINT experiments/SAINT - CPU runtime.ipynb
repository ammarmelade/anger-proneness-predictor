{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPwGAofqaocmINdOf3C6gU3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Preliminaries"],"metadata":{"id":"6ZiHKXJ0JQ2y"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"],"metadata":{"id":"irMRi0gI9vH_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686511634676,"user_tz":-420,"elapsed":20237,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"ee91fec2-931d-4b6f-fd21-fad85808c732"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wF_EZuxx8-UB","executionInfo":{"status":"ok","timestamp":1686511638636,"user_tz":-420,"elapsed":3979,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"82e292da-426b-42b4-c974-9a117c93ba61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'saint'...\n","remote: Enumerating objects: 664, done.\u001b[K\n","remote: Total 664 (delta 0), reused 0 (delta 0), pack-reused 664\u001b[K\n","Receiving objects: 100% (664/664), 17.00 MiB | 13.06 MiB/s, done.\n","Resolving deltas: 100% (364/364), done.\n"]}],"source":["!git clone https://github.com/ogunlao/saint.git"]},{"cell_type":"code","source":["!pip install pytorch-lightning==1.3.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZ4VpFXm7GDp","executionInfo":{"status":"ok","timestamp":1686511933129,"user_tz":-420,"elapsed":29249,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"8daaf278-a83e-40e9-9087-6f10a1f52078"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-lightning==1.3.2\n","  Downloading pytorch_lightning-1.3.2-py3-none-any.whl (805 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.7/805.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (1.22.4)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2.0.1+cu118)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (0.18.3)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (4.65.0)\n","Collecting PyYAML<=5.4.1,>=5.1 (from pytorch-lightning==1.3.2)\n","  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2023.4.0)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2.12.2)\n","Collecting torchmetrics>=0.2.0 (from pytorch-lightning==1.3.2)\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyDeprecate==0.3.0 (from pytorch-lightning==1.3.2)\n","  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2.27.1)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.54.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.4.3)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->pytorch-lightning==1.3.2) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->pytorch-lightning==1.3.2) (16.0.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->pytorch-lightning==1.3.2) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.2.2)\n","Building wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=529feeaabf25b7c7f6d7a46356df4ab5afde9325370bb1e8ec24e63ba58fc473\n","  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n","Successfully built PyYAML\n","Installing collected packages: PyYAML, pyDeprecate, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","Successfully installed PyYAML-5.4.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 pyDeprecate-0.3.0 pytorch-lightning-1.3.2 torchmetrics-0.11.4 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["# !pip3 install -r \"/content/saint/requirements.txt\""],"metadata":{"id":"0ECBnzW59KNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scikit-learn==0.24.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czKgW7eW7lTi","executionInfo":{"status":"ok","timestamp":1686512351596,"user_tz":-420,"elapsed":418490,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"f9d27d57-20b0-408f-e639-6a7b34639f87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-learn==0.24.2\n","  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.22.4)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.10.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (3.1.0)\n","Building wheels for collected packages: scikit-learn\n","  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install scipy==1.5.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdZC8Xhx7rEc","executionInfo":{"status":"ok","timestamp":1686512524014,"user_tz":-420,"elapsed":172463,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"3ab6a22c-681e-4647-f5c6-474371bcbcbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scipy==1.5.4\n","  Downloading scipy-1.5.4.tar.gz (25.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/25.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"code","source":["!pip install tensorboard==2.4.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"QzrvzYGU7q_C","executionInfo":{"status":"ok","timestamp":1686512539785,"user_tz":-420,"elapsed":15828,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"9a150161-c891-40ab-e533-ea35fe95c054"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboard==2.4.1\n","  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.54.0)\n","Collecting google-auth<2,>=1.6.3 (from tensorboard==2.4.1)\n","  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.4.1)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (3.4.3)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.22.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (2.27.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (67.7.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.16.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (0.40.0)\n","Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard==2.4.1)\n","  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.1) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.1) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.1) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard==2.4.1) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.4.1) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.1) (3.2.2)\n","Installing collected packages: cachetools, google-auth, google-auth-oauthlib, tensorboard\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.3.0\n","    Uninstalling cachetools-5.3.0:\n","      Successfully uninstalled cachetools-5.3.0\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 2.17.3\n","    Uninstalling google-auth-2.17.3:\n","      Successfully uninstalled google-auth-2.17.3\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.2\n","    Uninstalling tensorboard-2.12.2:\n","      Successfully uninstalled tensorboard-2.12.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n","google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 1.35.0 which is incompatible.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cachetools-4.2.4 google-auth-1.35.0 google-auth-oauthlib-0.4.6 tensorboard-2.4.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install tensorboard-plugin-wit==1.8.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wNz-mi0s7q0l","executionInfo":{"status":"ok","timestamp":1686512545473,"user_tz":-420,"elapsed":5728,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"fd5cfa31-74d0-4c94-c7fd-0bb711cec1e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboard-plugin-wit==1.8.0\n","  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.2/781.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard-plugin-wit\n","  Attempting uninstall: tensorboard-plugin-wit\n","    Found existing installation: tensorboard-plugin-wit 1.8.1\n","    Uninstalling tensorboard-plugin-wit-1.8.1:\n","      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tensorboard-plugin-wit-1.8.0\n"]}]},{"cell_type":"code","source":["!pip install torch==1.8.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sk55Z5-d75xp","executionInfo":{"status":"ok","timestamp":1686512547264,"user_tz":-420,"elapsed":1819,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"53639bc9-8e75-4a29-d463-6f5df05bc954"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.1\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install torchmetrics==0.3.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"832RK6OB78_Y","executionInfo":{"status":"ok","timestamp":1686512554994,"user_tz":-420,"elapsed":7752,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"1f3b9e64-e084-41f9-ec03-849fff94a978"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics==0.3.2\n","  Downloading torchmetrics-0.3.2-py3-none-any.whl (274 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/274.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/274.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.3.2) (2.0.1+cu118)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.3.2) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.1->torchmetrics==0.3.2) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.1->torchmetrics==0.3.2) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.1->torchmetrics==0.3.2) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.1->torchmetrics==0.3.2) (1.3.0)\n","Installing collected packages: torchmetrics\n","  Attempting uninstall: torchmetrics\n","    Found existing installation: torchmetrics 0.11.4\n","    Uninstalling torchmetrics-0.11.4:\n","      Successfully uninstalled torchmetrics-0.11.4\n","Successfully installed torchmetrics-0.3.2\n"]}]},{"cell_type":"code","source":["!pip install torchtext==0.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cBUbpbK7_ji","executionInfo":{"status":"ok","timestamp":1686512562513,"user_tz":-420,"elapsed":7543,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"64664873-04c0-4c19-f101-7c7b74c77142"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.6.0\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.6.0)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.15.2\n","    Uninstalling torchtext-0.15.2:\n","      Successfully uninstalled torchtext-0.15.2\n","Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"]}]},{"cell_type":"code","source":["!pip install torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIYK5adF7_et","executionInfo":{"status":"ok","timestamp":1686512568601,"user_tz":-420,"elapsed":6104,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"31bc02a2-3d66-41d6-98e9-06d02c73662b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install hydra-core"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"S6QGqYdU8Frx","executionInfo":{"status":"ok","timestamp":1686512580186,"user_tz":-420,"elapsed":11605,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"1976a2da-85d4-40d8-9d98-650dc098e33a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hydra-core\n","  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting omegaconf<2.4,>=2.2 (from hydra-core)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (23.1)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (5.4.1)\n","Building wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=d6fd23553251f35bea6e5a04251513a7012aa75c5b20fdaa42c2b8627f9a6704\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n","Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install ruamel_yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZpR1cNo8IRD","executionInfo":{"status":"ok","timestamp":1686512591463,"user_tz":-420,"elapsed":11316,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"ecde7ac3-1875-408d-ec41-87fea57d3754"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ruamel_yaml\n","  Downloading ruamel.yaml-0.17.31-py3-none-any.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel_yaml)\n","  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel_yaml\n","Successfully installed ruamel.yaml.clib-0.2.7 ruamel_yaml-0.17.31\n"]}]},{"cell_type":"code","source":["# !pip install torch --upgrade torch\n","# !pip install torch --upgrade pytorch-lightning"],"metadata":{"id":"RMjvVdfYITxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !nvcc --version"],"metadata":{"id":"WEdpJXk_I_lg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","\n","print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9VEnYWP1KH_Z","executionInfo":{"status":"ok","timestamp":1686512597082,"user_tz":-420,"elapsed":5680,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"44197c26-bc2f-4c00-f1e2-ccafdd88781d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["\n","# Setting up parameters"],"metadata":{"id":"Q5gQs6SaILhx"}},{"cell_type":"code","source":["from ruamel.yaml import YAML "],"metadata":{"id":"VnOHG2rCImIq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**IMPORTANT : EDIT FILE-FILE INI DULU SEBELUM RUN**\n","\n","Cara editnya : double-click file yg mau di edit di \"Files\" (tab kiri colab) ato klik link-link dibawah, terus ctrl-s buat save.\n","\n","```1. /content/saint/configs/config.yaml, line 46```\n","\n","```\n","trainer:\n","  max_epochs: 100 # default is 100\n","  # gpus: 0\n","  accelerator: auto\n","  deterministic: true\n","  default_root_dir: null\n","  # resume_from_checkpoint: null\n","```\n","\n","```2. /content/saint/configs/data/bank_ssl.yaml, line 10```\n","```\n","data_stats:\n","  no_cat: 1\n","  no_num: 49\n","  cats: [1]\n","```\n","\n","```3. /content/saint/configs/data/bank_sup.yaml, line 10```\n","\n","```\n","data_stats:\n","  no_cat: 1\n","  no_num: 49\n","  cats: [1]\n","```\n","\n","```4. /content/saint/configs/experiment/supervised.yaml```\n","\n","```\n","experiment: supervised\n","task: classification # {classification, regression}\n","model: saint\n","num_output: 5 # no of output neurons: 1 for binary classification num of classes in target for multiclass}\n","freeze_encoder: false # freeze transformer layer\n","pretrained_checkpoint: null #checkpoints/lightning_logs/version_7/checkpoints/epoch=0-step=1.ckpt\n","```"],"metadata":{"id":"fdYj59RZzVWT"}},{"cell_type":"code","source":["config_path = 'saint/configs/config.yaml'\n","\n","yaml = YAML(typ='safe')\n","with open(config_path) as f:\n","  args = yaml.load(f)\n","\n","print(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3yBXKWlIOjL","executionInfo":{"status":"ok","timestamp":1686512597646,"user_tz":-420,"elapsed":26,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"fe402d77-a90c-464b-d9cf-5b5b5d8600bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'defaults': ['_self_', {'experiment': 'supervised'}, {'data': 'bank_sup'}], 'seed': 1234, 'transformer': {'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'dropout_ff': 0.1, 'embed_dim': 32, 'd_ff': 32, 'cls_token_idx': 0}, 'augmentation': {'prob_cutmix': 0.3, 'alpha': 0.2, 'lambda_pt': 10}, 'optimizer': {'temperature': 0.7, 'proj_head_dim': 128, 'beta_1': 0.9, 'beta_2': 0.99, 'lr': 0.0001, 'weight_decay': 0.01, 'optim': 'adamw', 'metric': 'auroc'}, 'preproc': {'data_folder': None, 'train_split': 0.65, 'validation_split': 0.15, 'test_split': 0.2, 'num_supervised_train_data': None}, 'callback': {'monitor': 'val_loss', 'mode': 'min', 'auto_insert_metric_name': False}, 'trainer': {'max_epochs': 100, 'gpus': 0, 'deterministic': True, 'default_root_dir': None, 'resume_from_checkpoint': None}, 'dataloader': {'shuffle_val': False, 'train_bs': 32, 'val_bs': 32, 'test_bs': 16, 'num_workers': 2, 'pin_memory': False}, 'metric': '${optimizer.metric}', 'print_config': False}\n"]}]},{"cell_type":"markdown","source":["# Data preprocessing"],"metadata":{"id":"mIFvOAGlIueV"}},{"cell_type":"code","source":["data_folder = \"/content/saint/data\"\n","\n","os.mkdir(\"/content/saint/data\")"],"metadata":{"id":"b2yZLsEkI1Ex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_csv(\"/content/drive/MyDrive/SEM4/Research Method/RM Kel 19 Experiment/data-final.csv\", sep='\\t')\n","\n","print(dataset.shape)\n","dataset.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"0NCa-gD39lta","executionInfo":{"status":"ok","timestamp":1686512623813,"user_tz":-420,"elapsed":26184,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"b2e722d1-db91-4745-9793-8ea0aed7013f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1015341, 110)\n"]},{"output_type":"execute_result","data":{"text/plain":["   EXT1  EXT2  EXT3  EXT4  EXT5  EXT6  EXT7  EXT8  EXT9  EXT10  ...  \\\n","0   4.0   1.0   5.0   2.0   5.0   1.0   5.0   2.0   4.0    1.0  ...   \n","1   3.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0   1.0    5.0  ...   \n","2   2.0   3.0   4.0   4.0   3.0   2.0   1.0   3.0   2.0    5.0  ...   \n","3   2.0   2.0   2.0   3.0   4.0   2.0   2.0   4.0   1.0    4.0  ...   \n","4   3.0   3.0   3.0   3.0   5.0   3.0   3.0   5.0   3.0    4.0  ...   \n","\n","              dateload  screenw  screenh  introelapse  testelapse  endelapse  \\\n","0  2016-03-03 02:01:01    768.0   1024.0          9.0       234.0          6   \n","1  2016-03-03 02:01:20   1360.0    768.0         12.0       179.0         11   \n","2  2016-03-03 02:01:56   1366.0    768.0          3.0       186.0          7   \n","3  2016-03-03 02:02:02   1920.0   1200.0        186.0       219.0          7   \n","4  2016-03-03 02:02:57   1366.0    768.0          8.0       315.0         17   \n","\n","   IPC  country  lat_appx_lots_of_err  long_appx_lots_of_err  \n","0    1       GB               51.5448                 0.1991  \n","1    1       MY                3.1698                101.706  \n","2    1       GB               54.9119                -1.3833  \n","3    1       GB                 51.75                  -1.25  \n","4    2       KE                   1.0                   38.0  \n","\n","[5 rows x 110 columns]"],"text/html":["\n","  <div id=\"df-58fa9dc2-9dd9-40ae-b152-a79b07a00913\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EXT1</th>\n","      <th>EXT2</th>\n","      <th>EXT3</th>\n","      <th>EXT4</th>\n","      <th>EXT5</th>\n","      <th>EXT6</th>\n","      <th>EXT7</th>\n","      <th>EXT8</th>\n","      <th>EXT9</th>\n","      <th>EXT10</th>\n","      <th>...</th>\n","      <th>dateload</th>\n","      <th>screenw</th>\n","      <th>screenh</th>\n","      <th>introelapse</th>\n","      <th>testelapse</th>\n","      <th>endelapse</th>\n","      <th>IPC</th>\n","      <th>country</th>\n","      <th>lat_appx_lots_of_err</th>\n","      <th>long_appx_lots_of_err</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:01</td>\n","      <td>768.0</td>\n","      <td>1024.0</td>\n","      <td>9.0</td>\n","      <td>234.0</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>51.5448</td>\n","      <td>0.1991</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:20</td>\n","      <td>1360.0</td>\n","      <td>768.0</td>\n","      <td>12.0</td>\n","      <td>179.0</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>MY</td>\n","      <td>3.1698</td>\n","      <td>101.706</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:56</td>\n","      <td>1366.0</td>\n","      <td>768.0</td>\n","      <td>3.0</td>\n","      <td>186.0</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>54.9119</td>\n","      <td>-1.3833</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:02:02</td>\n","      <td>1920.0</td>\n","      <td>1200.0</td>\n","      <td>186.0</td>\n","      <td>219.0</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>51.75</td>\n","      <td>-1.25</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:02:57</td>\n","      <td>1366.0</td>\n","      <td>768.0</td>\n","      <td>8.0</td>\n","      <td>315.0</td>\n","      <td>17</td>\n","      <td>2</td>\n","      <td>KE</td>\n","      <td>1.0</td>\n","      <td>38.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 110 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58fa9dc2-9dd9-40ae-b152-a79b07a00913')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-58fa9dc2-9dd9-40ae-b152-a79b07a00913 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-58fa9dc2-9dd9-40ae-b152-a79b07a00913');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["data = dataset.drop(list(dataset)[50:], axis=1)\n","\n","print(data.shape)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"kKU71FbL-CH1","executionInfo":{"status":"ok","timestamp":1686516681484,"user_tz":-420,"elapsed":614,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"00cae174-1e24-45c7-cd2a-92d110dd9181"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["(1015341, 50)\n"]},{"output_type":"execute_result","data":{"text/plain":["   EXT1  EXT2  EXT3  EXT4  EXT5  EXT6  EXT7  EXT8  EXT9  EXT10  ...  OPN1  \\\n","0   4.0   1.0   5.0   2.0   5.0   1.0   5.0   2.0   4.0    1.0  ...   5.0   \n","1   3.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0   1.0    5.0  ...   1.0   \n","2   2.0   3.0   4.0   4.0   3.0   2.0   1.0   3.0   2.0    5.0  ...   5.0   \n","3   2.0   2.0   2.0   3.0   4.0   2.0   2.0   4.0   1.0    4.0  ...   4.0   \n","4   3.0   3.0   3.0   3.0   5.0   3.0   3.0   5.0   3.0    4.0  ...   5.0   \n","\n","   OPN2  OPN3  OPN4  OPN5  OPN6  OPN7  OPN8  OPN9  OPN10  \n","0   1.0   4.0   1.0   4.0   1.0   5.0   3.0   4.0    5.0  \n","1   2.0   4.0   2.0   3.0   1.0   4.0   2.0   5.0    3.0  \n","2   1.0   2.0   1.0   4.0   2.0   5.0   3.0   4.0    4.0  \n","3   2.0   5.0   2.0   3.0   1.0   4.0   4.0   3.0    3.0  \n","4   1.0   5.0   1.0   5.0   1.0   5.0   3.0   5.0    5.0  \n","\n","[5 rows x 50 columns]"],"text/html":["\n","  <div id=\"df-84bde034-0868-4381-9f72-d886c0bfbf3a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EXT1</th>\n","      <th>EXT2</th>\n","      <th>EXT3</th>\n","      <th>EXT4</th>\n","      <th>EXT5</th>\n","      <th>EXT6</th>\n","      <th>EXT7</th>\n","      <th>EXT8</th>\n","      <th>EXT9</th>\n","      <th>EXT10</th>\n","      <th>...</th>\n","      <th>OPN1</th>\n","      <th>OPN2</th>\n","      <th>OPN3</th>\n","      <th>OPN4</th>\n","      <th>OPN5</th>\n","      <th>OPN6</th>\n","      <th>OPN7</th>\n","      <th>OPN8</th>\n","      <th>OPN9</th>\n","      <th>OPN10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 50 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84bde034-0868-4381-9f72-d886c0bfbf3a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-84bde034-0868-4381-9f72-d886c0bfbf3a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-84bde034-0868-4381-9f72-d886c0bfbf3a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["for i in data.columns:\n","  data = data[(data[i].notna()) & (data[i] != 0)]\n","\n","print(data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z75-zCjK-IiC","executionInfo":{"status":"ok","timestamp":1686516699634,"user_tz":-420,"elapsed":16434,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"1656f61a-cc55-4d43-c5c7-d915a14b2380"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["(874434, 50)\n"]}]},{"cell_type":"code","source":["data = data.astype(int)"],"metadata":{"id":"v6-PfEfr9vjd","executionInfo":{"status":"ok","timestamp":1686516699636,"user_tz":-420,"elapsed":29,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["data['EST9'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fK-gKGep-GKA","executionInfo":{"status":"ok","timestamp":1686516699638,"user_tz":-420,"elapsed":29,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"995be5dc-35ee-4062-ae5c-a38cb6000237"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4    247851\n","2    199050\n","3    182001\n","5    133152\n","1    112380\n","Name: EST9, dtype: int64"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["data = data[:10000]"],"metadata":{"id":"rOAEEVuG1oC5","executionInfo":{"status":"ok","timestamp":1686516699639,"user_tz":-420,"elapsed":27,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["x = data.drop(columns=['EST9'])\n","y = data['EST9']"],"metadata":{"id":"f9Arorto-KIy","executionInfo":{"status":"ok","timestamp":1686516699639,"user_tz":-420,"elapsed":26,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["y = y - 1"],"metadata":{"id":"QN7RPmy9S8X9","executionInfo":{"status":"ok","timestamp":1686516699641,"user_tz":-420,"elapsed":26,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["from saint.src.dataset import generate_splits, preprocess"],"metadata":{"id":"XYfjIiOeJe91","executionInfo":{"status":"ok","timestamp":1686516699642,"user_tz":-420,"elapsed":26,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["num_supervised_train_data = 2000\n","\n","sup_train_indices, val_indices, test_indices, ssl_train_indices = generate_splits(len(x), num_supervised_train_data, args['preproc']['validation_split'], args['preproc']['test_split'], args['seed'],)"],"metadata":{"id":"sBdu-PJLJjt_","executionInfo":{"status":"ok","timestamp":1686516699642,"user_tz":-420,"elapsed":26,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["x_proc, y_proc, no_num, no_cat, cats  = preprocess(x, y, args['transformer']['cls_token_idx'])"],"metadata":{"id":"1ldx0xi8J9mT","executionInfo":{"status":"ok","timestamp":1686516699643,"user_tz":-420,"elapsed":26,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["print('no of numerical columns: ', no_num)\n","print('no of categorical columns: ', no_cat)\n","\n","print('list of categories in each categorical column: ', cats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99TtxxqOKEMK","executionInfo":{"status":"ok","timestamp":1686516699643,"user_tz":-420,"elapsed":24,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"67cb6c04-47ec-4a6e-9fc3-edf993813548"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["no of numerical columns:  49\n","no of categorical columns:  1\n","list of categories in each categorical column:  [1]\n"]}]},{"cell_type":"code","source":["train_df, train_y   = x_proc.iloc[sup_train_indices], y_proc.iloc[sup_train_indices]\n","val_df, val_y       = x_proc.iloc[val_indices], y_proc.iloc[val_indices]\n","test_df, test_y     = x_proc.iloc[test_indices], y_proc.iloc[test_indices]"],"metadata":{"id":"zO_CGazRKIlS","executionInfo":{"status":"ok","timestamp":1686516699644,"user_tz":-420,"elapsed":21,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["train_ssl, train_ssl_y = None, None\n","\n","if num_supervised_train_data != 'all':\n","    train_ssl, train_ssl_y = x_proc.iloc[ssl_train_indices], y_proc.iloc[ssl_train_indices]"],"metadata":{"id":"xdM8JYynKciy","executionInfo":{"status":"ok","timestamp":1686516699644,"user_tz":-420,"elapsed":20,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["train_df.to_csv('/content/saint/data/train.csv' , index=False)\n","train_y.to_csv('/content/saint/data/train_y.csv' , index=False)\n","val_df.to_csv('/content/saint/data/val.csv' , index=False)\n","val_y.to_csv('/content/saint/data/val_y.csv' , index=False)\n","test_df.to_csv('/content/saint/data/test.csv' , index=False)\n","test_y.to_csv('/content/saint/data/test_y.csv' , index=False)\n","\n","if train_ssl is not None:\n","   train_ssl.to_csv('/content/saint/data/train_ssl.csv' , index=False)\n","\n","if train_ssl_y is not None:\n","  train_ssl_y.to_csv('/content/saint/data/train_ssl_y.csv' , index=False)"],"metadata":{"id":"xn-T7swhKpmA","executionInfo":{"status":"ok","timestamp":1686516700200,"user_tz":-420,"elapsed":576,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":["# SAINT training"],"metadata":{"id":"XK3XHgVCGfOm"}},{"cell_type":"code","source":["num_gpus = 1"],"metadata":{"id":"mqJKcN-ZNr7i","executionInfo":{"status":"ok","timestamp":1686516700753,"user_tz":-420,"elapsed":11,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["os.environ[\"HYDRA_FULL_ERROR\"] = \"1\""],"metadata":{"id":"uMwBMOpT51E3","executionInfo":{"status":"ok","timestamp":1686516700756,"user_tz":-420,"elapsed":13,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}}},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":["## Self-supervised learning + supervised learning"],"metadata":{"id":"rZyNJ8mpOm6A"}},{"cell_type":"code","source":["# Self-Supervised Learning\n","\n","!python /content/saint/main.py experiment=self-supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_ssl"],"metadata":{"id":"tVpqTt4RLUV7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c535178e-b160-43ab-f480-d17e50beaac9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 20:51:50.479126: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-11 20:51:53.383817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/main.py:11: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Global seed set to 1234\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","\n","  | Name                | Type            | Params\n","--------------------------------------------------------\n","0 | transformer         | Encoder         | 65.0 K\n","1 | embedding           | Embedding       | 3.2 K \n","2 | contrastive_loss_fn | ContrastiveLoss | 409 K \n","3 | denoising_loss_fn   | DenoisingLoss   | 1.6 K \n","--------------------------------------------------------\n","479 K     Trainable params\n","0         Non-trainable params\n","479 K     Total params\n","1.918     Total estimated model params size (MB)\n","Global seed set to 1234\n","Epoch 0:  75% 141/188 [01:14<00:24,  1.89it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  76% 143/188 [01:15<00:23,  1.90it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:   4% 2/47 [00:00<00:16,  2.76it/s]\u001b[A\n","Epoch 0:  77% 145/188 [01:15<00:22,  1.91it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:   9% 4/47 [00:01<00:12,  3.48it/s]\u001b[A\n","Epoch 0:  78% 147/188 [01:16<00:21,  1.93it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  13% 6/47 [00:01<00:11,  3.67it/s]\u001b[A\n","Epoch 0:  79% 149/188 [01:16<00:20,  1.94it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  17% 8/47 [00:02<00:10,  3.74it/s]\u001b[A\n","Epoch 0:  80% 151/188 [01:17<00:18,  1.95it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  21% 10/47 [00:02<00:09,  3.78it/s]\u001b[A\n","Epoch 0:  81% 153/188 [01:17<00:17,  1.97it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  26% 12/47 [00:03<00:09,  3.75it/s]\u001b[A\n","Epoch 0:  82% 155/188 [01:18<00:16,  1.98it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  30% 14/47 [00:03<00:09,  3.66it/s]\u001b[A\n","Epoch 0:  84% 157/188 [01:18<00:15,  1.99it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  34% 16/47 [00:04<00:08,  3.81it/s]\u001b[A\n","Epoch 0:  85% 159/188 [01:19<00:14,  2.00it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  38% 18/47 [00:04<00:06,  4.43it/s]\u001b[A\n","Epoch 0:  86% 161/188 [01:19<00:13,  2.02it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  43% 20/47 [00:05<00:05,  4.80it/s]\u001b[A\n","Epoch 0:  87% 163/188 [01:20<00:12,  2.03it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  47% 22/47 [00:05<00:04,  5.09it/s]\u001b[A\n","Epoch 0:  88% 165/188 [01:20<00:11,  2.05it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  51% 24/47 [00:05<00:04,  5.30it/s]\u001b[A\n","Epoch 0:  89% 167/188 [01:20<00:10,  2.07it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  55% 26/47 [00:06<00:03,  5.42it/s]\u001b[A\n","Epoch 0:  90% 169/188 [01:21<00:09,  2.08it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  60% 28/47 [00:06<00:03,  5.29it/s]\u001b[A\n","Epoch 0:  91% 171/188 [01:21<00:08,  2.10it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  64% 30/47 [00:07<00:03,  5.42it/s]\u001b[A\n","Epoch 0:  92% 173/188 [01:21<00:07,  2.11it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  68% 32/47 [00:07<00:02,  5.44it/s]\u001b[A\n","Epoch 0:  93% 175/188 [01:22<00:06,  2.13it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  72% 34/47 [00:07<00:02,  5.42it/s]\u001b[A\n","Epoch 0:  94% 177/188 [01:22<00:05,  2.14it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  77% 36/47 [00:08<00:01,  5.67it/s]\u001b[A\n","Epoch 0:  95% 179/188 [01:22<00:04,  2.16it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  81% 38/47 [00:08<00:01,  6.10it/s]\u001b[A\n","Epoch 0:  96% 181/188 [01:23<00:03,  2.17it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  85% 40/47 [00:08<00:01,  6.15it/s]\u001b[A\n","Epoch 0:  97% 183/188 [01:23<00:02,  2.19it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  89% 42/47 [00:09<00:00,  6.12it/s]\u001b[A\n","Epoch 0:  98% 185/188 [01:23<00:01,  2.20it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  94% 44/47 [00:09<00:00,  5.84it/s]\u001b[A\n","Epoch 0:  99% 187/188 [01:24<00:00,  2.22it/s, loss=593, v_num=0, val_loss_epoch=684.0, train_loss_step=589.0]\n","Validating:  98% 46/47 [00:09<00:00,  5.67it/s]\u001b[A\n","Epoch 0: 100% 188/188 [01:24<00:00,  2.22it/s, loss=593, v_num=0, val_loss_epoch=598.0, train_loss_step=552.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Epoch 1:  76% 142/188 [01:12<00:23,  1.95it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:22,  2.03it/s]\u001b[A\n","Epoch 1:  77% 144/188 [01:13<00:22,  1.95it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:01<00:14,  3.05it/s]\u001b[A\n","Epoch 1:  78% 146/188 [01:14<00:21,  1.97it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:10,  3.87it/s]\u001b[A\n","Epoch 1:  79% 148/188 [01:14<00:20,  1.98it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:08,  4.80it/s]\u001b[A\n","Epoch 1:  80% 150/188 [01:14<00:18,  2.00it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:02<00:06,  5.48it/s]\u001b[A\n","Epoch 1:  81% 152/188 [01:15<00:17,  2.02it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.86it/s]\u001b[A\n","Epoch 1:  82% 154/188 [01:15<00:16,  2.04it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.07it/s]\u001b[A\n","Epoch 1:  83% 156/188 [01:15<00:15,  2.06it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:05,  6.27it/s]\u001b[A\n","Epoch 1:  84% 158/188 [01:16<00:14,  2.07it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:04,  6.23it/s]\u001b[A\n","Epoch 1:  85% 160/188 [01:16<00:13,  2.09it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.27it/s]\u001b[A\n","Epoch 1:  86% 162/188 [01:16<00:12,  2.11it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.85it/s]\u001b[A\n","Epoch 1:  87% 164/188 [01:17<00:11,  2.12it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.52it/s]\u001b[A\n","Epoch 1:  88% 166/188 [01:17<00:10,  2.14it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.39it/s]\u001b[A\n","Epoch 1:  89% 168/188 [01:17<00:09,  2.15it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.46it/s]\u001b[A\n","Epoch 1:  90% 170/188 [01:18<00:08,  2.17it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.34it/s]\u001b[A\n","Epoch 1:  91% 172/188 [01:18<00:07,  2.18it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.55it/s]\u001b[A\n","Epoch 1:  93% 174/188 [01:19<00:06,  2.20it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.94it/s]\u001b[A\n","Epoch 1:  94% 176/188 [01:19<00:05,  2.22it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:06<00:01,  6.10it/s]\u001b[A\n","Epoch 1:  95% 178/188 [01:19<00:04,  2.23it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:06<00:01,  6.24it/s]\u001b[A\n","Epoch 1:  96% 180/188 [01:20<00:03,  2.25it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:07<00:01,  6.40it/s]\u001b[A\n","Epoch 1:  97% 182/188 [01:20<00:02,  2.27it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:07<00:00,  6.23it/s]\u001b[A\n","Epoch 1:  98% 184/188 [01:20<00:01,  2.28it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:07<00:00,  6.28it/s]\u001b[A\n","Epoch 1:  99% 186/188 [01:20<00:00,  2.30it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:08<00:00,  6.46it/s]\u001b[A\n","Epoch 1: 100% 188/188 [01:21<00:00,  2.31it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=590.0, train_loss_epoch=606.0, val_loss_step=602.0]\n","Epoch 1: 100% 188/188 [01:21<00:00,  2.31it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=576.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 2:  76% 142/188 [01:13<00:23,  1.94it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:22,  2.02it/s]\u001b[A\n","Epoch 2:  77% 144/188 [01:14<00:22,  1.94it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:01<00:13,  3.23it/s]\u001b[A\n","Epoch 2:  78% 146/188 [01:14<00:21,  1.96it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:11,  3.71it/s]\u001b[A\n","Epoch 2:  79% 148/188 [01:15<00:20,  1.97it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:10,  3.94it/s]\u001b[A\n","Epoch 2:  80% 150/188 [01:15<00:19,  1.98it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:02<00:09,  4.12it/s]\u001b[A\n","Epoch 2:  81% 152/188 [01:16<00:18,  2.00it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:09,  3.92it/s]\u001b[A\n","Epoch 2:  82% 154/188 [01:16<00:16,  2.01it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:03<00:08,  3.80it/s]\u001b[A\n","Epoch 2:  83% 156/188 [01:17<00:15,  2.02it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.46it/s]\u001b[A\n","Epoch 2:  84% 158/188 [01:17<00:14,  2.04it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:04<00:06,  4.93it/s]\u001b[A\n","Epoch 2:  85% 160/188 [01:17<00:13,  2.06it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:04<00:05,  5.22it/s]\u001b[A\n","Epoch 2:  86% 162/188 [01:18<00:12,  2.07it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:05<00:05,  5.08it/s]\u001b[A\n","Epoch 2:  87% 164/188 [01:18<00:11,  2.09it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:05<00:04,  5.19it/s]\u001b[A\n","Epoch 2:  88% 166/188 [01:18<00:10,  2.10it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:05<00:04,  5.26it/s]\u001b[A\n","Epoch 2:  89% 168/188 [01:19<00:09,  2.12it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:06<00:03,  5.47it/s]\u001b[A\n","Epoch 2:  90% 170/188 [01:19<00:08,  2.13it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:06<00:03,  5.81it/s]\u001b[A\n","Epoch 2:  91% 172/188 [01:19<00:07,  2.15it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.94it/s]\u001b[A\n","Epoch 2:  93% 174/188 [01:20<00:06,  2.17it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:07<00:02,  5.62it/s]\u001b[A\n","Epoch 2:  94% 176/188 [01:20<00:05,  2.18it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.52it/s]\u001b[A\n","Epoch 2:  95% 178/188 [01:21<00:04,  2.20it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.36it/s]\u001b[A\n","Epoch 2:  96% 180/188 [01:21<00:03,  2.21it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.28it/s]\u001b[A\n","Epoch 2:  97% 182/188 [01:21<00:02,  2.22it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.22it/s]\u001b[A\n","Epoch 2:  98% 184/188 [01:22<00:01,  2.24it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:09<00:00,  5.18it/s]\u001b[A\n","Epoch 2:  99% 186/188 [01:22<00:00,  2.25it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:09<00:00,  5.41it/s]\u001b[A\n","Epoch 2: 100% 188/188 [01:22<00:00,  2.27it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=595.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 2: 100% 188/188 [01:23<00:00,  2.26it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=523.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 3:  76% 142/188 [01:12<00:23,  1.96it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:19,  2.41it/s]\u001b[A\n","Epoch 3:  77% 144/188 [01:13<00:22,  1.97it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:12,  3.42it/s]\u001b[A\n","Epoch 3:  78% 146/188 [01:13<00:21,  1.99it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:09,  4.32it/s]\u001b[A\n","Epoch 3:  79% 148/188 [01:13<00:19,  2.00it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:08,  4.86it/s]\u001b[A\n","Epoch 3:  80% 150/188 [01:14<00:18,  2.02it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:02<00:07,  5.18it/s]\u001b[A\n","Epoch 3:  81% 152/188 [01:14<00:17,  2.04it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.34it/s]\u001b[A\n","Epoch 3:  82% 154/188 [01:14<00:16,  2.05it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.78it/s]\u001b[A\n","Epoch 3:  83% 156/188 [01:15<00:15,  2.07it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:05,  6.19it/s]\u001b[A\n","Epoch 3:  84% 158/188 [01:15<00:14,  2.09it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:04,  6.15it/s]\u001b[A\n","Epoch 3:  85% 160/188 [01:15<00:13,  2.11it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.94it/s]\u001b[A\n","Epoch 3:  86% 162/188 [01:16<00:12,  2.12it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.80it/s]\u001b[A\n","Epoch 3:  87% 164/188 [01:16<00:11,  2.14it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.57it/s]\u001b[A\n","Epoch 3:  88% 166/188 [01:17<00:10,  2.16it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.37it/s]\u001b[A\n","Epoch 3:  89% 168/188 [01:17<00:09,  2.17it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.39it/s]\u001b[A\n","Epoch 3:  90% 170/188 [01:17<00:08,  2.19it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.46it/s]\u001b[A\n","Epoch 3:  91% 172/188 [01:18<00:07,  2.20it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.43it/s]\u001b[A\n","Epoch 3:  93% 174/188 [01:18<00:06,  2.22it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.27it/s]\u001b[A\n","Epoch 3:  94% 176/188 [01:18<00:05,  2.23it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.30it/s]\u001b[A\n","Epoch 3:  95% 178/188 [01:19<00:04,  2.25it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.53it/s]\u001b[A\n","Epoch 3:  96% 180/188 [01:19<00:03,  2.26it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.51it/s]\u001b[A\n","Epoch 3:  97% 182/188 [01:19<00:02,  2.28it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.69it/s]\u001b[A\n","Epoch 3:  98% 184/188 [01:20<00:01,  2.29it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:08<00:00,  6.01it/s]\u001b[A\n","Epoch 3:  99% 186/188 [01:20<00:00,  2.31it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:08<00:00,  6.09it/s]\u001b[A\n","Epoch 3: 100% 188/188 [01:20<00:00,  2.32it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=632.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 3: 100% 188/188 [01:21<00:00,  2.32it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=662.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 4:  76% 142/188 [01:14<00:24,  1.92it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:21,  2.17it/s]\u001b[A\n","Epoch 4:  77% 144/188 [01:14<00:22,  1.92it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:13,  3.29it/s]\u001b[A\n","Epoch 4:  78% 146/188 [01:15<00:21,  1.94it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:11,  3.69it/s]\u001b[A\n","Epoch 4:  79% 148/188 [01:15<00:20,  1.95it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:02<00:10,  3.71it/s]\u001b[A\n","Epoch 4:  80% 150/188 [01:16<00:19,  1.96it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:02<00:10,  3.71it/s]\u001b[A\n","Epoch 4:  81% 152/188 [01:16<00:18,  1.98it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:03<00:09,  3.71it/s]\u001b[A\n","Epoch 4:  82% 154/188 [01:17<00:17,  1.99it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:03<00:08,  4.19it/s]\u001b[A\n","Epoch 4:  83% 156/188 [01:17<00:15,  2.00it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:06,  4.77it/s]\u001b[A\n","Epoch 4:  84% 158/188 [01:18<00:14,  2.02it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:04<00:05,  5.11it/s]\u001b[A\n","Epoch 4:  85% 160/188 [01:18<00:13,  2.04it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:04<00:04,  5.61it/s]\u001b[A\n","Epoch 4:  86% 162/188 [01:18<00:12,  2.05it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.90it/s]\u001b[A\n","Epoch 4:  87% 164/188 [01:19<00:11,  2.07it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:05<00:03,  6.12it/s]\u001b[A\n","Epoch 4:  88% 166/188 [01:19<00:10,  2.09it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:05<00:03,  6.17it/s]\u001b[A\n","Epoch 4:  89% 168/188 [01:19<00:09,  2.10it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:05<00:03,  6.22it/s]\u001b[A\n","Epoch 4:  90% 170/188 [01:20<00:08,  2.12it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:06<00:03,  5.98it/s]\u001b[A\n","Epoch 4:  91% 172/188 [01:20<00:07,  2.14it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.97it/s]\u001b[A\n","Epoch 4:  93% 174/188 [01:20<00:06,  2.15it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.69it/s]\u001b[A\n","Epoch 4:  94% 176/188 [01:21<00:05,  2.17it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.60it/s]\u001b[A\n","Epoch 4:  95% 178/188 [01:21<00:04,  2.18it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.42it/s]\u001b[A\n","Epoch 4:  96% 180/188 [01:21<00:03,  2.20it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.44it/s]\u001b[A\n","Epoch 4:  97% 182/188 [01:22<00:02,  2.21it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.51it/s]\u001b[A\n","Epoch 4:  98% 184/188 [01:22<00:01,  2.23it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:08<00:00,  5.43it/s]\u001b[A\n","Epoch 4:  99% 186/188 [01:23<00:00,  2.24it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:09<00:00,  5.44it/s]\u001b[A\n","Epoch 4: 100% 188/188 [01:23<00:00,  2.25it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=540.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 4: 100% 188/188 [01:23<00:00,  2.25it/s, loss=618, v_num=0, val_loss_epoch=598.0, train_loss_step=593.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 5:  76% 142/188 [01:13<00:23,  1.93it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:18,  2.52it/s]\u001b[A\n","Epoch 5:  77% 144/188 [01:14<00:22,  1.94it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.26it/s]\u001b[A\n","Epoch 5:  78% 146/188 [01:14<00:21,  1.96it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.99it/s]\u001b[A\n","Epoch 5:  79% 148/188 [01:14<00:20,  1.98it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.70it/s]\u001b[A\n","Epoch 5:  80% 150/188 [01:15<00:19,  2.00it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:01<00:06,  6.01it/s]\u001b[A\n","Epoch 5:  81% 152/188 [01:15<00:17,  2.02it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:05,  6.18it/s]\u001b[A\n","Epoch 5:  82% 154/188 [01:15<00:16,  2.03it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.75it/s]\u001b[A\n","Epoch 5:  83% 156/188 [01:16<00:15,  2.05it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.62it/s]\u001b[A\n","Epoch 5:  84% 158/188 [01:16<00:14,  2.07it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.63it/s]\u001b[A\n","Epoch 5:  85% 160/188 [01:16<00:13,  2.08it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.55it/s]\u001b[A\n","Epoch 5:  86% 162/188 [01:17<00:12,  2.10it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.43it/s]\u001b[A\n","Epoch 5:  87% 164/188 [01:17<00:11,  2.11it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.32it/s]\u001b[A\n","Epoch 5:  88% 166/188 [01:17<00:10,  2.13it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.45it/s]\u001b[A\n","Epoch 5:  89% 168/188 [01:18<00:09,  2.15it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.40it/s]\u001b[A\n","Epoch 5:  90% 170/188 [01:18<00:08,  2.16it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.48it/s]\u001b[A\n","Epoch 5:  91% 172/188 [01:18<00:07,  2.18it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.67it/s]\u001b[A\n","Epoch 5:  93% 174/188 [01:19<00:06,  2.19it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.91it/s]\u001b[A\n","Epoch 5:  94% 176/188 [01:19<00:05,  2.21it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.84it/s]\u001b[A\n","Epoch 5:  95% 178/188 [01:20<00:04,  2.22it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.55it/s]\u001b[A\n","Epoch 5:  96% 180/188 [01:20<00:03,  2.24it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.50it/s]\u001b[A\n","Epoch 5:  97% 182/188 [01:20<00:02,  2.25it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.48it/s]\u001b[A\n","Epoch 5:  98% 184/188 [01:21<00:01,  2.27it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:07<00:00,  5.25it/s]\u001b[A\n","Epoch 5:  99% 186/188 [01:21<00:00,  2.28it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.30it/s]\u001b[A\n","Epoch 5: 100% 188/188 [01:21<00:00,  2.30it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=589.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 5: 100% 188/188 [01:22<00:00,  2.29it/s, loss=599, v_num=0, val_loss_epoch=598.0, train_loss_step=582.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 6:  76% 142/188 [01:15<00:24,  1.87it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:15,  2.98it/s]\u001b[A\n","Epoch 6:  77% 144/188 [01:16<00:23,  1.89it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:09,  4.80it/s]\u001b[A\n","Epoch 6:  78% 146/188 [01:16<00:22,  1.90it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.67it/s]\u001b[A\n","Epoch 6:  79% 148/188 [01:17<00:20,  1.92it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:06,  6.03it/s]\u001b[A\n","Epoch 6:  80% 150/188 [01:17<00:19,  1.94it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:01<00:06,  6.08it/s]\u001b[A\n","Epoch 6:  81% 152/188 [01:17<00:18,  1.96it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:01<00:06,  5.70it/s]\u001b[A\n","Epoch 6:  82% 154/188 [01:18<00:17,  1.97it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.54it/s]\u001b[A\n","Epoch 6:  83% 156/188 [01:18<00:16,  1.99it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.50it/s]\u001b[A\n","Epoch 6:  84% 158/188 [01:18<00:14,  2.01it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.43it/s]\u001b[A\n","Epoch 6:  85% 160/188 [01:19<00:13,  2.02it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.45it/s]\u001b[A\n","Epoch 6:  86% 162/188 [01:19<00:12,  2.04it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.72it/s]\u001b[A\n","Epoch 6:  87% 164/188 [01:19<00:11,  2.05it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:04<00:03,  6.04it/s]\u001b[A\n","Epoch 6:  88% 166/188 [01:20<00:10,  2.07it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.11it/s]\u001b[A\n","Epoch 6:  89% 168/188 [01:20<00:09,  2.09it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.96it/s]\u001b[A\n","Epoch 6:  90% 170/188 [01:20<00:08,  2.10it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.67it/s]\u001b[A\n","Epoch 6:  91% 172/188 [01:21<00:07,  2.12it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.61it/s]\u001b[A\n","Epoch 6:  93% 174/188 [01:21<00:06,  2.13it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.60it/s]\u001b[A\n","Epoch 6:  94% 176/188 [01:21<00:05,  2.15it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.67it/s]\u001b[A\n","Epoch 6:  95% 178/188 [01:22<00:04,  2.16it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.44it/s]\u001b[A\n","Epoch 6:  96% 180/188 [01:22<00:03,  2.18it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:06<00:01,  5.42it/s]\u001b[A\n","Epoch 6:  97% 182/188 [01:23<00:02,  2.19it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.22it/s]\u001b[A\n","Epoch 6:  98% 184/188 [01:23<00:01,  2.20it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:07<00:00,  4.38it/s]\u001b[A\n","Epoch 6:  99% 186/188 [01:24<00:00,  2.21it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.18it/s]\u001b[A\n","Epoch 6: 100% 188/188 [01:24<00:00,  2.22it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 6: 100% 188/188 [01:24<00:00,  2.22it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=616.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 7:  76% 142/188 [01:13<00:23,  1.92it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:12,  3.68it/s]\u001b[A\n","Epoch 7:  77% 144/188 [01:14<00:22,  1.94it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.26it/s]\u001b[A\n","Epoch 7:  78% 146/188 [01:14<00:21,  1.95it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.38it/s]\u001b[A\n","Epoch 7:  79% 148/188 [01:15<00:20,  1.97it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.39it/s]\u001b[A\n","Epoch 7:  80% 150/188 [01:15<00:19,  1.99it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.29it/s]\u001b[A\n","Epoch 7:  81% 152/188 [01:15<00:17,  2.00it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.24it/s]\u001b[A\n","Epoch 7:  82% 154/188 [01:16<00:16,  2.02it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.25it/s]\u001b[A\n","Epoch 7:  83% 156/188 [01:16<00:15,  2.04it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:02<00:06,  5.31it/s]\u001b[A\n","Epoch 7:  84% 158/188 [01:16<00:14,  2.05it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.33it/s]\u001b[A\n","Epoch 7:  85% 160/188 [01:17<00:13,  2.07it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.40it/s]\u001b[A\n","Epoch 7:  86% 162/188 [01:17<00:12,  2.09it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.38it/s]\u001b[A\n","Epoch 7:  87% 164/188 [01:18<00:11,  2.10it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:04<00:05,  4.65it/s]\u001b[A\n","Epoch 7:  88% 166/188 [01:18<00:10,  2.11it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:04<00:04,  4.45it/s]\u001b[A\n","Epoch 7:  89% 168/188 [01:19<00:09,  2.13it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:05<00:04,  4.56it/s]\u001b[A\n","Epoch 7:  90% 170/188 [01:19<00:08,  2.14it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:05<00:04,  4.34it/s]\u001b[A\n","Epoch 7:  91% 172/188 [01:20<00:07,  2.15it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.04it/s]\u001b[A\n","Epoch 7:  93% 174/188 [01:20<00:06,  2.16it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:06<00:03,  3.98it/s]\u001b[A\n","Epoch 7:  94% 176/188 [01:21<00:05,  2.17it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:07<00:03,  3.95it/s]\u001b[A\n","Epoch 7:  95% 178/188 [01:21<00:04,  2.18it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:07<00:02,  3.90it/s]\u001b[A\n","Epoch 7:  96% 180/188 [01:22<00:03,  2.19it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:08<00:02,  3.98it/s]\u001b[A\n","Epoch 7:  97% 182/188 [01:22<00:02,  2.21it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:08<00:01,  3.84it/s]\u001b[A\n","Epoch 7:  98% 184/188 [01:23<00:01,  2.21it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:09<00:01,  3.85it/s]\u001b[A\n","Epoch 7:  99% 186/188 [01:23<00:00,  2.22it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:09<00:00,  3.70it/s]\u001b[A\n","Epoch 7: 100% 188/188 [01:24<00:00,  2.23it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=584.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 7: 100% 188/188 [01:24<00:00,  2.23it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=570.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 8:  76% 142/188 [01:15<00:24,  1.89it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:15,  2.93it/s]\u001b[A\n","Epoch 8:  77% 144/188 [01:15<00:23,  1.90it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:12,  3.62it/s]\u001b[A\n","Epoch 8:  78% 146/188 [01:16<00:21,  1.91it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:11,  3.72it/s]\u001b[A\n","Epoch 8:  79% 148/188 [01:16<00:20,  1.93it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:10,  3.83it/s]\u001b[A\n","Epoch 8:  80% 150/188 [01:17<00:19,  1.94it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:02<00:10,  3.78it/s]\u001b[A\n","Epoch 8:  81% 152/188 [01:17<00:18,  1.95it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:09,  3.87it/s]\u001b[A\n","Epoch 8:  82% 154/188 [01:18<00:17,  1.96it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:03<00:08,  3.82it/s]\u001b[A\n","Epoch 8:  83% 156/188 [01:18<00:16,  1.98it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:08,  3.86it/s]\u001b[A\n","Epoch 8:  84% 158/188 [01:19<00:15,  1.99it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:04<00:07,  3.84it/s]\u001b[A\n","Epoch 8:  85% 160/188 [01:19<00:13,  2.00it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:05<00:07,  3.76it/s]\u001b[A\n","Epoch 8:  86% 162/188 [01:20<00:12,  2.01it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:05<00:07,  3.68it/s]\u001b[A\n","Epoch 8:  87% 164/188 [01:21<00:11,  2.02it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:06<00:06,  3.61it/s]\u001b[A\n","Epoch 8:  88% 166/188 [01:21<00:10,  2.03it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:06<00:05,  3.68it/s]\u001b[A\n","Epoch 8:  89% 168/188 [01:22<00:09,  2.05it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:07<00:04,  4.31it/s]\u001b[A\n","Epoch 8:  90% 170/188 [01:22<00:08,  2.06it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:07<00:03,  4.74it/s]\u001b[A\n","Epoch 8:  91% 172/188 [01:22<00:07,  2.08it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:07<00:03,  5.05it/s]\u001b[A\n","Epoch 8:  93% 174/188 [01:23<00:06,  2.09it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:08<00:02,  5.24it/s]\u001b[A\n","Epoch 8:  94% 176/188 [01:23<00:05,  2.11it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:08<00:02,  5.34it/s]\u001b[A\n","Epoch 8:  95% 178/188 [01:23<00:04,  2.12it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:08<00:01,  5.51it/s]\u001b[A\n","Epoch 8:  96% 180/188 [01:24<00:03,  2.14it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:09<00:01,  5.50it/s]\u001b[A\n","Epoch 8:  97% 182/188 [01:24<00:02,  2.15it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:09<00:01,  5.39it/s]\u001b[A\n","Epoch 8:  98% 184/188 [01:25<00:01,  2.16it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:10<00:00,  5.37it/s]\u001b[A\n","Epoch 8:  99% 186/188 [01:25<00:00,  2.18it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:10<00:00,  5.38it/s]\u001b[A\n","Epoch 8: 100% 188/188 [01:25<00:00,  2.19it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=579.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 8: 100% 188/188 [01:26<00:00,  2.18it/s, loss=591, v_num=0, val_loss_epoch=598.0, train_loss_step=488.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 9:  76% 142/188 [01:14<00:23,  1.92it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:22,  2.03it/s]\u001b[A\n","Epoch 9:  77% 144/188 [01:14<00:22,  1.93it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:01<00:14,  3.11it/s]\u001b[A\n","Epoch 9:  78% 146/188 [01:15<00:21,  1.94it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:12,  3.35it/s]\u001b[A\n","Epoch 9:  79% 148/188 [01:15<00:20,  1.95it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:02<00:09,  4.07it/s]\u001b[A\n","Epoch 9:  80% 150/188 [01:16<00:19,  1.97it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:02<00:07,  4.94it/s]\u001b[A\n","Epoch 9:  81% 152/188 [01:16<00:18,  1.99it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.40it/s]\u001b[A\n","Epoch 9:  82% 154/188 [01:16<00:16,  2.00it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:03<00:05,  5.75it/s]\u001b[A\n","Epoch 9:  83% 156/188 [01:17<00:15,  2.02it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:05,  5.59it/s]\u001b[A\n","Epoch 9:  84% 158/188 [01:17<00:14,  2.04it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.52it/s]\u001b[A\n","Epoch 9:  85% 160/188 [01:17<00:13,  2.05it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:04<00:05,  5.50it/s]\u001b[A\n","Epoch 9:  86% 162/188 [01:18<00:12,  2.07it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.43it/s]\u001b[A\n","Epoch 9:  87% 164/188 [01:18<00:11,  2.08it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.30it/s]\u001b[A\n","Epoch 9:  88% 166/188 [01:19<00:10,  2.10it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:05<00:04,  5.39it/s]\u001b[A\n","Epoch 9:  89% 168/188 [01:19<00:09,  2.12it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.43it/s]\u001b[A\n","Epoch 9:  90% 170/188 [01:19<00:08,  2.13it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.34it/s]\u001b[A\n","Epoch 9:  91% 172/188 [01:20<00:07,  2.15it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.62it/s]\u001b[A\n","Epoch 9:  93% 174/188 [01:20<00:06,  2.16it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.63it/s]\u001b[A\n","Epoch 9:  94% 176/188 [01:20<00:05,  2.18it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.84it/s]\u001b[A\n","Epoch 9:  95% 178/188 [01:21<00:04,  2.19it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.74it/s]\u001b[A\n","Epoch 9:  96% 180/188 [01:21<00:03,  2.21it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.57it/s]\u001b[A\n","Epoch 9:  97% 182/188 [01:21<00:02,  2.22it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.57it/s]\u001b[A\n","Epoch 9:  98% 184/188 [01:22<00:01,  2.24it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:08<00:00,  5.81it/s]\u001b[A\n","Epoch 9:  99% 186/188 [01:22<00:00,  2.25it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:08<00:00,  6.10it/s]\u001b[A\n","Epoch 9: 100% 188/188 [01:22<00:00,  2.27it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=599.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 9: 100% 188/188 [01:23<00:00,  2.26it/s, loss=588, v_num=0, val_loss_epoch=598.0, train_loss_step=466.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 10:  76% 142/188 [01:15<00:24,  1.87it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:22,  2.04it/s]\u001b[A\n","Epoch 10:  77% 144/188 [01:16<00:23,  1.88it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:01<00:13,  3.19it/s]\u001b[A\n","Epoch 10:  78% 146/188 [01:17<00:22,  1.89it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:12,  3.50it/s]\u001b[A\n","Epoch 10:  79% 148/188 [01:17<00:20,  1.91it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:02<00:10,  3.98it/s]\u001b[A\n","Epoch 10:  80% 150/188 [01:17<00:19,  1.92it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:02<00:08,  4.56it/s]\u001b[A\n","Epoch 10:  81% 152/188 [01:18<00:18,  1.94it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:07,  5.01it/s]\u001b[A\n","Epoch 10:  82% 154/188 [01:18<00:17,  1.96it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:03<00:06,  5.24it/s]\u001b[A\n","Epoch 10:  83% 156/188 [01:19<00:16,  1.97it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:06,  5.13it/s]\u001b[A\n","Epoch 10:  84% 158/188 [01:19<00:15,  1.99it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.11it/s]\u001b[A\n","Epoch 10:  85% 160/188 [01:19<00:13,  2.00it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:04<00:05,  5.15it/s]\u001b[A\n","Epoch 10:  86% 162/188 [01:20<00:12,  2.02it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.53it/s]\u001b[A\n","Epoch 10:  87% 164/188 [01:20<00:11,  2.03it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.89it/s]\u001b[A\n","Epoch 10:  88% 166/188 [01:20<00:10,  2.05it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:05<00:03,  5.80it/s]\u001b[A\n","Epoch 10:  89% 168/188 [01:21<00:09,  2.07it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.62it/s]\u001b[A\n","Epoch 10:  90% 170/188 [01:21<00:08,  2.08it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:06<00:03,  5.44it/s]\u001b[A\n","Epoch 10:  91% 172/188 [01:22<00:07,  2.10it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.53it/s]\u001b[A\n","Epoch 10:  93% 174/188 [01:22<00:06,  2.11it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.77it/s]\u001b[A\n","Epoch 10:  94% 176/188 [01:22<00:05,  2.13it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.82it/s]\u001b[A\n","Epoch 10:  95% 178/188 [01:23<00:04,  2.14it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.66it/s]\u001b[A\n","Epoch 10:  96% 180/188 [01:23<00:03,  2.16it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.57it/s]\u001b[A\n","Epoch 10:  97% 182/188 [01:23<00:02,  2.17it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.38it/s]\u001b[A\n","Epoch 10:  98% 184/188 [01:24<00:01,  2.19it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:08<00:00,  5.33it/s]\u001b[A\n","Epoch 10:  99% 186/188 [01:24<00:00,  2.20it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.32it/s]\u001b[A\n","Epoch 10: 100% 188/188 [01:24<00:00,  2.21it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=644.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 10: 100% 188/188 [01:25<00:00,  2.21it/s, loss=598, v_num=0, val_loss_epoch=598.0, train_loss_step=496.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 11:  76% 142/188 [01:14<00:24,  1.91it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:14,  3.24it/s]\u001b[A\n","Epoch 11:  77% 144/188 [01:14<00:22,  1.93it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.10it/s]\u001b[A\n","Epoch 11:  78% 146/188 [01:15<00:21,  1.95it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.80it/s]\u001b[A\n","Epoch 11:  79% 148/188 [01:15<00:20,  1.97it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:06,  6.06it/s]\u001b[A\n","Epoch 11:  80% 150/188 [01:15<00:19,  1.98it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:01<00:06,  6.19it/s]\u001b[A\n","Epoch 11:  81% 152/188 [01:15<00:17,  2.00it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.25it/s]\u001b[A\n","Epoch 11:  82% 154/188 [01:16<00:16,  2.02it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.34it/s]\u001b[A\n","Epoch 11:  83% 156/188 [01:16<00:15,  2.04it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.25it/s]\u001b[A\n","Epoch 11:  84% 158/188 [01:16<00:14,  2.05it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:02<00:05,  5.61it/s]\u001b[A\n","Epoch 11:  85% 160/188 [01:17<00:13,  2.07it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.29it/s]\u001b[A\n","Epoch 11:  86% 162/188 [01:17<00:12,  2.08it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.38it/s]\u001b[A\n","Epoch 11:  87% 164/188 [01:18<00:11,  2.10it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.78it/s]\u001b[A\n","Epoch 11:  88% 166/188 [01:18<00:10,  2.12it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.13it/s]\u001b[A\n","Epoch 11:  89% 168/188 [01:18<00:09,  2.14it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.96it/s]\u001b[A\n","Epoch 11:  90% 170/188 [01:19<00:08,  2.15it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.66it/s]\u001b[A\n","Epoch 11:  91% 172/188 [01:19<00:07,  2.17it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.67it/s]\u001b[A\n","Epoch 11:  93% 174/188 [01:19<00:06,  2.18it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.68it/s]\u001b[A\n","Epoch 11:  94% 176/188 [01:20<00:05,  2.20it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.95it/s]\u001b[A\n","Epoch 11:  95% 178/188 [01:20<00:04,  2.21it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:06<00:01,  6.22it/s]\u001b[A\n","Epoch 11:  96% 180/188 [01:20<00:03,  2.23it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:06<00:01,  6.15it/s]\u001b[A\n","Epoch 11:  97% 182/188 [01:21<00:02,  2.24it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.71it/s]\u001b[A\n","Epoch 11:  98% 184/188 [01:21<00:01,  2.26it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:07<00:00,  5.58it/s]\u001b[A\n","Epoch 11:  99% 186/188 [01:21<00:00,  2.27it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:07<00:00,  5.49it/s]\u001b[A\n","Epoch 11: 100% 188/188 [01:22<00:00,  2.29it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=610.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 100% 47/47 [00:08<00:00,  5.41it/s]\u001b[AEpoch 00012: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 11: 100% 188/188 [01:22<00:00,  2.28it/s, loss=601, v_num=0, val_loss_epoch=597.0, train_loss_step=531.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Epoch 12:  76% 142/188 [01:16<00:24,  1.86it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:15,  2.91it/s]\u001b[A\n","Epoch 12:  77% 144/188 [01:16<00:23,  1.88it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:   6% 3/47 [00:00<00:09,  4.44it/s]\u001b[A\n","Epoch 12:  78% 146/188 [01:17<00:22,  1.89it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  11% 5/47 [00:01<00:08,  5.07it/s]\u001b[A\n","Epoch 12:  79% 148/188 [01:17<00:20,  1.91it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.60it/s]\u001b[A\n","Epoch 12:  80% 150/188 [01:17<00:19,  1.93it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.84it/s]\u001b[A\n","Epoch 12:  81% 152/188 [01:18<00:18,  1.95it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  23% 11/47 [00:02<00:05,  6.13it/s]\u001b[A\n","Epoch 12:  82% 154/188 [01:18<00:17,  1.96it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.16it/s]\u001b[A\n","Epoch 12:  83% 156/188 [01:18<00:16,  1.98it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.03it/s]\u001b[A\n","Epoch 12:  84% 158/188 [01:19<00:15,  2.00it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  36% 17/47 [00:03<00:04,  6.16it/s]\u001b[A\n","Epoch 12:  85% 160/188 [01:19<00:13,  2.01it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.68it/s]\u001b[A\n","Epoch 12:  86% 162/188 [01:19<00:12,  2.03it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.29it/s]\u001b[A\n","Epoch 12:  87% 164/188 [01:20<00:11,  2.04it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.29it/s]\u001b[A\n","Epoch 12:  88% 166/188 [01:20<00:10,  2.06it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.31it/s]\u001b[A\n","Epoch 12:  89% 168/188 [01:20<00:09,  2.07it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.28it/s]\u001b[A\n","Epoch 12:  90% 170/188 [01:21<00:08,  2.09it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.65it/s]\u001b[A\n","Epoch 12:  91% 172/188 [01:21<00:07,  2.11it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.92it/s]\u001b[A\n","Epoch 12:  93% 174/188 [01:22<00:06,  2.12it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  70% 33/47 [00:05<00:02,  6.03it/s]\u001b[A\n","Epoch 12:  94% 176/188 [01:22<00:05,  2.14it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.92it/s]\u001b[A\n","Epoch 12:  95% 178/188 [01:22<00:04,  2.15it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.69it/s]\u001b[A\n","Epoch 12:  96% 180/188 [01:23<00:03,  2.17it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.50it/s]\u001b[A\n","Epoch 12:  97% 182/188 [01:23<00:02,  2.18it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.32it/s]\u001b[A\n","Epoch 12:  98% 184/188 [01:23<00:01,  2.19it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  91% 43/47 [00:07<00:00,  4.56it/s]\u001b[A\n","Epoch 12:  99% 186/188 [01:24<00:00,  2.20it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.20it/s]\u001b[A\n","Epoch 12: 100% 188/188 [01:24<00:00,  2.21it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=700.0, train_loss_epoch=600.0, val_loss_step=603.0]\n","Epoch 12: 100% 188/188 [01:25<00:00,  2.20it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=509.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Epoch 13:  76% 142/188 [01:11<00:23,  2.00it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.51it/s]\u001b[A\n","Epoch 13:  77% 144/188 [01:11<00:21,  2.01it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.07it/s]\u001b[A\n","Epoch 13:  78% 146/188 [01:11<00:20,  2.03it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.39it/s]\u001b[A\n","Epoch 13:  79% 148/188 [01:12<00:19,  2.05it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.46it/s]\u001b[A\n","Epoch 13:  80% 150/188 [01:12<00:18,  2.06it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.64it/s]\u001b[A\n","Epoch 13:  81% 152/188 [01:13<00:17,  2.08it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.52it/s]\u001b[A\n","Epoch 13:  82% 154/188 [01:13<00:16,  2.10it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.53it/s]\u001b[A\n","Epoch 13:  83% 156/188 [01:13<00:15,  2.12it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.50it/s]\u001b[A\n","Epoch 13:  84% 158/188 [01:14<00:14,  2.13it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.60it/s]\u001b[A\n","Epoch 13:  85% 160/188 [01:14<00:13,  2.15it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.58it/s]\u001b[A\n","Epoch 13:  86% 162/188 [01:14<00:12,  2.17it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.63it/s]\u001b[A\n","Epoch 13:  87% 164/188 [01:15<00:10,  2.18it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.65it/s]\u001b[A\n","Epoch 13:  88% 166/188 [01:15<00:10,  2.20it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.68it/s]\u001b[A\n","Epoch 13:  89% 168/188 [01:15<00:09,  2.21it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.29it/s]\u001b[A\n","Epoch 13:  90% 170/188 [01:16<00:08,  2.23it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.48it/s]\u001b[A\n","Epoch 13:  91% 172/188 [01:16<00:07,  2.25it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.68it/s]\u001b[A\n","Epoch 13:  93% 174/188 [01:16<00:06,  2.26it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.85it/s]\u001b[A\n","Epoch 13:  94% 176/188 [01:17<00:05,  2.28it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.75it/s]\u001b[A\n","Epoch 13:  95% 178/188 [01:17<00:04,  2.29it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.58it/s]\u001b[A\n","Epoch 13:  96% 180/188 [01:17<00:03,  2.31it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.60it/s]\u001b[A\n","Epoch 13:  97% 182/188 [01:18<00:02,  2.32it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  87% 41/47 [00:07<00:01,  4.65it/s]\u001b[A\n","Epoch 13:  98% 184/188 [01:18<00:01,  2.33it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.43it/s]\u001b[A\n","Epoch 13:  99% 186/188 [01:19<00:00,  2.34it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.52it/s]\u001b[A\n","Epoch 13: 100% 188/188 [01:19<00:00,  2.36it/s, loss=598, v_num=0, val_loss_epoch=597.0, train_loss_step=605.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Epoch 13: 100% 188/188 [01:20<00:00,  2.35it/s, loss=598, v_num=0, val_loss_epoch=596.0, train_loss_step=477.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Epoch 14:  76% 142/188 [01:12<00:23,  1.96it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.79it/s]\u001b[A\n","Epoch 14:  77% 144/188 [01:13<00:22,  1.97it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.36it/s]\u001b[A\n","Epoch 14:  78% 146/188 [01:13<00:21,  1.99it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.74it/s]\u001b[A\n","Epoch 14:  79% 148/188 [01:13<00:19,  2.01it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.10it/s]\u001b[A\n","Epoch 14:  80% 150/188 [01:14<00:18,  2.02it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.11it/s]\u001b[A\n","Epoch 14:  81% 152/188 [01:14<00:17,  2.04it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.16it/s]\u001b[A\n","Epoch 14:  82% 154/188 [01:14<00:16,  2.06it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.17it/s]\u001b[A\n","Epoch 14:  83% 156/188 [01:15<00:15,  2.07it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  32% 15/47 [00:03<00:06,  5.11it/s]\u001b[A\n","Epoch 14:  84% 158/188 [01:15<00:14,  2.09it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.11it/s]\u001b[A\n","Epoch 14:  85% 160/188 [01:16<00:13,  2.10it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.21it/s]\u001b[A\n","Epoch 14:  86% 162/188 [01:16<00:12,  2.12it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  45% 21/47 [00:04<00:05,  5.14it/s]\u001b[A\n","Epoch 14:  87% 164/188 [01:16<00:11,  2.13it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.24it/s]\u001b[A\n","Epoch 14:  88% 166/188 [01:17<00:10,  2.15it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.28it/s]\u001b[A\n","Epoch 14:  89% 168/188 [01:17<00:09,  2.16it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  57% 27/47 [00:05<00:04,  4.77it/s]\u001b[A\n","Epoch 14:  90% 170/188 [01:18<00:08,  2.18it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  62% 29/47 [00:05<00:04,  4.15it/s]\u001b[A\n","Epoch 14:  91% 172/188 [01:18<00:07,  2.19it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.17it/s]\u001b[A\n","Epoch 14:  93% 174/188 [01:19<00:06,  2.20it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  70% 33/47 [00:06<00:03,  4.33it/s]\u001b[A\n","Epoch 14:  94% 176/188 [01:19<00:05,  2.21it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  74% 35/47 [00:07<00:02,  4.21it/s]\u001b[A\n","Epoch 14:  95% 178/188 [01:20<00:04,  2.22it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  79% 37/47 [00:07<00:02,  4.42it/s]\u001b[A\n","Epoch 14:  96% 180/188 [01:20<00:03,  2.24it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  83% 39/47 [00:08<00:01,  4.23it/s]\u001b[A\n","Epoch 14:  97% 182/188 [01:21<00:02,  2.25it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  87% 41/47 [00:08<00:01,  4.03it/s]\u001b[A\n","Epoch 14:  98% 184/188 [01:21<00:01,  2.26it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  91% 43/47 [00:09<00:01,  3.87it/s]\u001b[A\n","Epoch 14:  99% 186/188 [01:22<00:00,  2.27it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Validating:  96% 45/47 [00:09<00:00,  3.70it/s]\u001b[A\n","Epoch 14: 100% 188/188 [01:22<00:00,  2.28it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=635.0, train_loss_epoch=599.0, val_loss_step=600.0]\n","Epoch 14: 100% 188/188 [01:22<00:00,  2.27it/s, loss=595, v_num=0, val_loss_epoch=596.0, train_loss_step=483.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Epoch 15:  76% 142/188 [01:14<00:24,  1.90it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.87it/s]\u001b[A\n","Epoch 15:  77% 144/188 [01:15<00:22,  1.92it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:   6% 3/47 [00:00<00:09,  4.51it/s]\u001b[A\n","Epoch 15:  78% 146/188 [01:15<00:21,  1.94it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  11% 5/47 [00:01<00:08,  5.20it/s]\u001b[A\n","Epoch 15:  79% 148/188 [01:15<00:20,  1.95it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.00it/s]\u001b[A\n","Epoch 15:  80% 150/188 [01:16<00:19,  1.97it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  19% 9/47 [00:01<00:08,  4.40it/s]\u001b[A\n","Epoch 15:  81% 152/188 [01:16<00:18,  1.98it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  23% 11/47 [00:02<00:08,  4.10it/s]\u001b[A\n","Epoch 15:  82% 154/188 [01:17<00:17,  1.99it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  28% 13/47 [00:02<00:08,  4.05it/s]\u001b[A\n","Epoch 15:  83% 156/188 [01:17<00:15,  2.01it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  32% 15/47 [00:03<00:08,  3.94it/s]\u001b[A\n","Epoch 15:  84% 158/188 [01:18<00:14,  2.02it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  36% 17/47 [00:04<00:07,  3.94it/s]\u001b[A\n","Epoch 15:  85% 160/188 [01:18<00:13,  2.03it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  40% 19/47 [00:04<00:07,  3.86it/s]\u001b[A\n","Epoch 15:  86% 162/188 [01:19<00:12,  2.04it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  45% 21/47 [00:05<00:06,  4.05it/s]\u001b[A\n","Epoch 15:  87% 164/188 [01:19<00:11,  2.06it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  49% 23/47 [00:05<00:05,  4.15it/s]\u001b[A\n","Epoch 15:  88% 166/188 [01:20<00:10,  2.07it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  53% 25/47 [00:05<00:05,  4.29it/s]\u001b[A\n","Epoch 15:  89% 168/188 [01:20<00:09,  2.08it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  57% 27/47 [00:06<00:05,  3.95it/s]\u001b[A\n","Epoch 15:  90% 170/188 [01:21<00:08,  2.09it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  62% 29/47 [00:07<00:04,  3.82it/s]\u001b[A\n","Epoch 15:  91% 172/188 [01:21<00:07,  2.10it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  66% 31/47 [00:07<00:04,  3.98it/s]\u001b[A\n","Epoch 15:  93% 174/188 [01:22<00:06,  2.12it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  70% 33/47 [00:07<00:03,  4.63it/s]\u001b[A\n","Epoch 15:  94% 176/188 [01:22<00:05,  2.13it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  74% 35/47 [00:08<00:02,  5.29it/s]\u001b[A\n","Epoch 15:  95% 178/188 [01:22<00:04,  2.15it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  79% 37/47 [00:08<00:01,  5.63it/s]\u001b[A\n","Epoch 15:  96% 180/188 [01:23<00:03,  2.16it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.95it/s]\u001b[A\n","Epoch 15:  97% 182/188 [01:23<00:02,  2.18it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  87% 41/47 [00:09<00:01,  5.75it/s]\u001b[A\n","Epoch 15:  98% 184/188 [01:23<00:01,  2.19it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  91% 43/47 [00:09<00:00,  5.47it/s]\u001b[A\n","Epoch 15:  99% 186/188 [01:24<00:00,  2.21it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  96% 45/47 [00:09<00:00,  5.45it/s]\u001b[A\n","Epoch 15: 100% 188/188 [01:24<00:00,  2.22it/s, loss=590, v_num=0, val_loss_epoch=596.0, train_loss_step=552.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Epoch 15: 100% 188/188 [01:24<00:00,  2.22it/s, loss=590, v_num=0, val_loss_epoch=595.0, train_loss_step=560.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Epoch 16:  76% 142/188 [01:14<00:24,  1.90it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.50it/s]\u001b[A\n","Epoch 16:  77% 144/188 [01:14<00:22,  1.92it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.15it/s]\u001b[A\n","Epoch 16:  78% 146/188 [01:15<00:21,  1.94it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.69it/s]\u001b[A\n","Epoch 16:  79% 148/188 [01:15<00:20,  1.96it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.67it/s]\u001b[A\n","Epoch 16:  80% 150/188 [01:16<00:19,  1.97it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.41it/s]\u001b[A\n","Epoch 16:  81% 152/188 [01:16<00:18,  1.99it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.81it/s]\u001b[A\n","Epoch 16:  82% 154/188 [01:16<00:16,  2.00it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:02<00:07,  4.49it/s]\u001b[A\n","Epoch 16:  83% 156/188 [01:17<00:15,  2.02it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.45it/s]\u001b[A\n","Epoch 16:  84% 158/188 [01:17<00:14,  2.03it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:07,  4.27it/s]\u001b[A\n","Epoch 16:  85% 160/188 [01:18<00:13,  2.04it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:04<00:07,  3.95it/s]\u001b[A\n","Epoch 16:  86% 162/188 [01:18<00:12,  2.05it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:04<00:06,  3.86it/s]\u001b[A\n","Epoch 16:  87% 164/188 [01:19<00:11,  2.06it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:05<00:06,  3.92it/s]\u001b[A\n","Epoch 16:  88% 166/188 [01:19<00:10,  2.08it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:05<00:05,  3.98it/s]\u001b[A\n","Epoch 16:  89% 168/188 [01:20<00:09,  2.09it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:06<00:05,  3.96it/s]\u001b[A\n","Epoch 16:  90% 170/188 [01:20<00:08,  2.10it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:06<00:04,  3.81it/s]\u001b[A\n","Epoch 16:  91% 172/188 [01:21<00:07,  2.11it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:07<00:04,  3.73it/s]\u001b[A\n","Epoch 16:  93% 174/188 [01:22<00:06,  2.12it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:07<00:03,  3.79it/s]\u001b[A\n","Epoch 16:  94% 176/188 [01:22<00:05,  2.13it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:08<00:03,  3.88it/s]\u001b[A\n","Epoch 16:  95% 178/188 [01:22<00:04,  2.15it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:08<00:02,  4.54it/s]\u001b[A\n","Epoch 16:  96% 180/188 [01:23<00:03,  2.16it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:08<00:01,  4.84it/s]\u001b[A\n","Epoch 16:  97% 182/188 [01:23<00:02,  2.17it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:09<00:01,  5.13it/s]\u001b[A\n","Epoch 16:  98% 184/188 [01:24<00:01,  2.19it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:09<00:00,  5.36it/s]\u001b[A\n","Epoch 16:  99% 186/188 [01:24<00:00,  2.20it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:10<00:00,  5.41it/s]\u001b[A\n","Epoch 16: 100% 188/188 [01:24<00:00,  2.22it/s, loss=591, v_num=0, val_loss_epoch=595.0, train_loss_step=555.0, train_loss_epoch=598.0, val_loss_step=602.0]\n","Epoch 16: 100% 188/188 [01:25<00:00,  2.21it/s, loss=591, v_num=0, val_loss_epoch=594.0, train_loss_step=465.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Epoch 17:  76% 142/188 [01:11<00:23,  1.99it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:12,  3.60it/s]\u001b[A\n","Epoch 17:  77% 144/188 [01:11<00:21,  2.01it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:   6% 3/47 [00:00<00:07,  5.51it/s]\u001b[A\n","Epoch 17:  78% 146/188 [01:11<00:20,  2.03it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.90it/s]\u001b[A\n","Epoch 17:  79% 148/188 [01:12<00:19,  2.05it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  15% 7/47 [00:01<00:06,  6.12it/s]\u001b[A\n","Epoch 17:  80% 150/188 [01:12<00:18,  2.07it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.18it/s]\u001b[A\n","Epoch 17:  81% 152/188 [01:13<00:17,  2.08it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.77it/s]\u001b[A\n","Epoch 17:  82% 154/188 [01:13<00:16,  2.09it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  28% 13/47 [00:02<00:07,  4.26it/s]\u001b[A\n","Epoch 17:  83% 156/188 [01:14<00:15,  2.11it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  32% 15/47 [00:03<00:08,  3.94it/s]\u001b[A\n","Epoch 17:  84% 158/188 [01:14<00:14,  2.12it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  36% 17/47 [00:03<00:07,  3.95it/s]\u001b[A\n","Epoch 17:  85% 160/188 [01:15<00:13,  2.13it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  40% 19/47 [00:04<00:07,  3.91it/s]\u001b[A\n","Epoch 17:  86% 162/188 [01:15<00:12,  2.14it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  45% 21/47 [00:04<00:06,  3.85it/s]\u001b[A\n","Epoch 17:  87% 164/188 [01:16<00:11,  2.15it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  49% 23/47 [00:05<00:06,  3.75it/s]\u001b[A\n","Epoch 17:  88% 166/188 [01:16<00:10,  2.16it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  53% 25/47 [00:05<00:05,  3.80it/s]\u001b[A\n","Epoch 17:  89% 168/188 [01:17<00:09,  2.17it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  57% 27/47 [00:06<00:05,  3.64it/s]\u001b[A\n","Epoch 17:  90% 170/188 [01:17<00:08,  2.18it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  62% 29/47 [00:06<00:05,  3.59it/s]\u001b[A\n","Epoch 17:  91% 172/188 [01:18<00:07,  2.19it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  66% 31/47 [00:07<00:04,  3.64it/s]\u001b[A\n","Epoch 17:  93% 174/188 [01:18<00:06,  2.21it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  70% 33/47 [00:07<00:03,  4.23it/s]\u001b[A\n","Epoch 17:  94% 176/188 [01:19<00:05,  2.22it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  74% 35/47 [00:08<00:02,  4.76it/s]\u001b[A\n","Epoch 17:  95% 178/188 [01:19<00:04,  2.24it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  79% 37/47 [00:08<00:01,  5.18it/s]\u001b[A\n","Epoch 17:  96% 180/188 [01:19<00:03,  2.25it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.30it/s]\u001b[A\n","Epoch 17:  97% 182/188 [01:20<00:02,  2.26it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  87% 41/47 [00:09<00:01,  5.20it/s]\u001b[A\n","Epoch 17:  98% 184/188 [01:20<00:01,  2.28it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  91% 43/47 [00:09<00:00,  5.17it/s]\u001b[A\n","Epoch 17:  99% 186/188 [01:21<00:00,  2.29it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Validating:  96% 45/47 [00:10<00:00,  5.24it/s]\u001b[A\n","Epoch 17: 100% 188/188 [01:21<00:00,  2.31it/s, loss=581, v_num=0, val_loss_epoch=594.0, train_loss_step=586.0, train_loss_epoch=598.0, val_loss_step=601.0]\n","Epoch 17: 100% 188/188 [01:21<00:00,  2.30it/s, loss=581, v_num=0, val_loss_epoch=593.0, train_loss_step=573.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Epoch 18:  76% 142/188 [01:13<00:23,  1.94it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.87it/s]\u001b[A\n","Epoch 18:  77% 144/188 [01:13<00:22,  1.95it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.21it/s]\u001b[A\n","Epoch 18:  78% 146/188 [01:14<00:21,  1.97it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.77it/s]\u001b[A\n","Epoch 18:  79% 148/188 [01:14<00:20,  1.98it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.00it/s]\u001b[A\n","Epoch 18:  80% 150/188 [01:14<00:18,  2.00it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.17it/s]\u001b[A\n","Epoch 18:  81% 152/188 [01:15<00:17,  2.02it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.53it/s]\u001b[A\n","Epoch 18:  82% 154/188 [01:15<00:16,  2.03it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  28% 13/47 [00:02<00:08,  4.20it/s]\u001b[A\n","Epoch 18:  83% 156/188 [01:16<00:15,  2.04it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.05it/s]\u001b[A\n","Epoch 18:  84% 158/188 [01:16<00:14,  2.06it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  36% 17/47 [00:03<00:07,  4.23it/s]\u001b[A\n","Epoch 18:  85% 160/188 [01:17<00:13,  2.07it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  40% 19/47 [00:04<00:06,  4.41it/s]\u001b[A\n","Epoch 18:  86% 162/188 [01:17<00:12,  2.08it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  45% 21/47 [00:04<00:06,  4.23it/s]\u001b[A\n","Epoch 18:  87% 164/188 [01:18<00:11,  2.10it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  49% 23/47 [00:05<00:05,  4.06it/s]\u001b[A\n","Epoch 18:  88% 166/188 [01:18<00:10,  2.11it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  53% 25/47 [00:05<00:05,  4.27it/s]\u001b[A\n","Epoch 18:  89% 168/188 [01:19<00:09,  2.12it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  57% 27/47 [00:06<00:04,  4.49it/s]\u001b[A\n","Epoch 18:  90% 170/188 [01:19<00:08,  2.14it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  62% 29/47 [00:06<00:03,  4.52it/s]\u001b[A\n","Epoch 18:  91% 172/188 [01:20<00:07,  2.15it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  66% 31/47 [00:07<00:03,  4.22it/s]\u001b[A\n","Epoch 18:  93% 174/188 [01:20<00:06,  2.16it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  70% 33/47 [00:07<00:03,  4.09it/s]\u001b[A\n","Epoch 18:  94% 176/188 [01:21<00:05,  2.17it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  74% 35/47 [00:08<00:02,  4.25it/s]\u001b[A\n","Epoch 18:  95% 178/188 [01:21<00:04,  2.18it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  79% 37/47 [00:08<00:02,  4.71it/s]\u001b[A\n","Epoch 18:  96% 180/188 [01:21<00:03,  2.20it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.03it/s]\u001b[A\n","Epoch 18:  97% 182/188 [01:22<00:02,  2.21it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  87% 41/47 [00:09<00:01,  5.22it/s]\u001b[A\n","Epoch 18:  98% 184/188 [01:22<00:01,  2.23it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  91% 43/47 [00:09<00:00,  4.98it/s]\u001b[A\n","Epoch 18:  99% 186/188 [01:23<00:00,  2.24it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Validating:  96% 45/47 [00:09<00:00,  5.20it/s]\u001b[A\n","Epoch 18: 100% 188/188 [01:23<00:00,  2.25it/s, loss=591, v_num=0, val_loss_epoch=593.0, train_loss_step=641.0, train_loss_epoch=598.0, val_loss_step=598.0]\n","Epoch 18: 100% 188/188 [01:23<00:00,  2.25it/s, loss=591, v_num=0, val_loss_epoch=594.0, train_loss_step=613.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Epoch 19:  76% 142/188 [01:12<00:23,  1.97it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:17,  2.58it/s]\u001b[A\n","Epoch 19:  77% 144/188 [01:12<00:22,  1.98it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:   6% 3/47 [00:00<00:11,  3.76it/s]\u001b[A\n","Epoch 19:  78% 146/188 [01:13<00:21,  1.99it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  11% 5/47 [00:01<00:10,  4.16it/s]\u001b[A\n","Epoch 19:  79% 148/188 [01:13<00:19,  2.01it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  15% 7/47 [00:01<00:09,  4.32it/s]\u001b[A\n","Epoch 19:  80% 150/188 [01:14<00:18,  2.02it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  19% 9/47 [00:02<00:08,  4.39it/s]\u001b[A\n","Epoch 19:  81% 152/188 [01:14<00:17,  2.04it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  23% 11/47 [00:02<00:08,  4.27it/s]\u001b[A\n","Epoch 19:  82% 154/188 [01:15<00:16,  2.05it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  28% 13/47 [00:03<00:08,  4.08it/s]\u001b[A\n","Epoch 19:  83% 156/188 [01:15<00:15,  2.06it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  32% 15/47 [00:03<00:08,  3.98it/s]\u001b[A\n","Epoch 19:  84% 158/188 [01:16<00:14,  2.07it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  36% 17/47 [00:04<00:07,  3.92it/s]\u001b[A\n","Epoch 19:  85% 160/188 [01:16<00:13,  2.09it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  40% 19/47 [00:04<00:07,  3.99it/s]\u001b[A\n","Epoch 19:  86% 162/188 [01:17<00:12,  2.10it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  45% 21/47 [00:05<00:06,  4.09it/s]\u001b[A\n","Epoch 19:  87% 164/188 [01:17<00:11,  2.11it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  49% 23/47 [00:05<00:06,  3.93it/s]\u001b[A\n","Epoch 19:  88% 166/188 [01:18<00:10,  2.12it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  53% 25/47 [00:06<00:05,  3.79it/s]\u001b[A\n","Epoch 19:  89% 168/188 [01:18<00:09,  2.14it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  57% 27/47 [00:06<00:04,  4.48it/s]\u001b[A\n","Epoch 19:  90% 170/188 [01:19<00:08,  2.15it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  62% 29/47 [00:06<00:03,  5.13it/s]\u001b[A\n","Epoch 19:  91% 172/188 [01:19<00:07,  2.17it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  66% 31/47 [00:07<00:02,  5.57it/s]\u001b[A\n","Epoch 19:  93% 174/188 [01:19<00:06,  2.18it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  70% 33/47 [00:07<00:02,  5.89it/s]\u001b[A\n","Epoch 19:  94% 176/188 [01:19<00:05,  2.20it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.93it/s]\u001b[A\n","Epoch 19:  95% 178/188 [01:20<00:04,  2.22it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  79% 37/47 [00:08<00:01,  6.02it/s]\u001b[A\n","Epoch 19:  96% 180/188 [01:20<00:03,  2.23it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  83% 39/47 [00:08<00:01,  6.11it/s]\u001b[A\n","Epoch 19:  97% 182/188 [01:20<00:02,  2.25it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.52it/s]\u001b[A\n","Epoch 19:  98% 184/188 [01:21<00:01,  2.26it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  91% 43/47 [00:09<00:00,  5.53it/s]\u001b[A\n","Epoch 19:  99% 186/188 [01:21<00:00,  2.28it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Validating:  96% 45/47 [00:09<00:00,  5.33it/s]\u001b[A\n","Epoch 19: 100% 188/188 [01:22<00:00,  2.29it/s, loss=602, v_num=0, val_loss_epoch=594.0, train_loss_step=595.0, train_loss_epoch=598.0, val_loss_step=597.0]\n","Epoch 19: 100% 188/188 [01:22<00:00,  2.28it/s, loss=602, v_num=0, val_loss_epoch=592.0, train_loss_step=470.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Epoch 20:  76% 142/188 [01:13<00:23,  1.93it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:20,  2.19it/s]\u001b[A\n","Epoch 20:  77% 144/188 [01:14<00:22,  1.94it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:   6% 3/47 [00:01<00:13,  3.17it/s]\u001b[A\n","Epoch 20:  78% 146/188 [01:14<00:21,  1.95it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  11% 5/47 [00:01<00:11,  3.58it/s]\u001b[A\n","Epoch 20:  79% 148/188 [01:15<00:20,  1.97it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  15% 7/47 [00:02<00:10,  3.79it/s]\u001b[A\n","Epoch 20:  80% 150/188 [01:15<00:19,  1.98it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  19% 9/47 [00:02<00:09,  3.95it/s]\u001b[A\n","Epoch 20:  81% 152/188 [01:16<00:18,  1.99it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  23% 11/47 [00:03<00:09,  3.98it/s]\u001b[A\n","Epoch 20:  82% 154/188 [01:16<00:16,  2.01it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  28% 13/47 [00:03<00:08,  4.02it/s]\u001b[A\n","Epoch 20:  83% 156/188 [01:17<00:15,  2.02it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.07it/s]\u001b[A\n","Epoch 20:  84% 158/188 [01:17<00:14,  2.03it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  36% 17/47 [00:04<00:07,  4.02it/s]\u001b[A\n","Epoch 20:  85% 160/188 [01:18<00:13,  2.04it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  40% 19/47 [00:05<00:07,  3.98it/s]\u001b[A\n","Epoch 20:  86% 162/188 [01:18<00:12,  2.06it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  45% 21/47 [00:05<00:06,  3.80it/s]\u001b[A\n","Epoch 20:  87% 164/188 [01:19<00:11,  2.07it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  49% 23/47 [00:05<00:05,  4.27it/s]\u001b[A\n","Epoch 20:  88% 166/188 [01:19<00:10,  2.08it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  53% 25/47 [00:06<00:04,  4.69it/s]\u001b[A\n","Epoch 20:  89% 168/188 [01:20<00:09,  2.10it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  57% 27/47 [00:06<00:03,  5.03it/s]\u001b[A\n","Epoch 20:  90% 170/188 [01:20<00:08,  2.11it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  62% 29/47 [00:07<00:03,  5.25it/s]\u001b[A\n","Epoch 20:  91% 172/188 [01:20<00:07,  2.13it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  66% 31/47 [00:07<00:03,  5.31it/s]\u001b[A\n","Epoch 20:  93% 174/188 [01:21<00:06,  2.14it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  70% 33/47 [00:07<00:02,  5.24it/s]\u001b[A\n","Epoch 20:  94% 176/188 [01:21<00:05,  2.16it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  74% 35/47 [00:08<00:02,  5.30it/s]\u001b[A\n","Epoch 20:  95% 178/188 [01:21<00:04,  2.17it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  79% 37/47 [00:08<00:01,  5.24it/s]\u001b[A\n","Epoch 20:  96% 180/188 [01:22<00:03,  2.19it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.41it/s]\u001b[A\n","Epoch 20:  97% 182/188 [01:22<00:02,  2.20it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  87% 41/47 [00:09<00:01,  5.81it/s]\u001b[A\n","Epoch 20:  98% 184/188 [01:22<00:01,  2.22it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  91% 43/47 [00:09<00:00,  6.03it/s]\u001b[A\n","Epoch 20:  99% 186/188 [01:23<00:00,  2.23it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Validating:  96% 45/47 [00:09<00:00,  6.04it/s]\u001b[A\n","Epoch 20: 100% 188/188 [01:23<00:00,  2.25it/s, loss=599, v_num=0, val_loss_epoch=592.0, train_loss_step=630.0, train_loss_epoch=597.0, val_loss_step=599.0]\n","Epoch 20: 100% 188/188 [01:23<00:00,  2.24it/s, loss=599, v_num=0, val_loss_epoch=591.0, train_loss_step=628.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Epoch 21:  76% 142/188 [01:10<00:22,  2.02it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.72it/s]\u001b[A\n","Epoch 21:  77% 144/188 [01:11<00:21,  2.03it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:   6% 3/47 [00:00<00:11,  3.85it/s]\u001b[A\n","Epoch 21:  78% 146/188 [01:11<00:20,  2.04it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  11% 5/47 [00:01<00:10,  4.16it/s]\u001b[A\n","Epoch 21:  79% 148/188 [01:11<00:19,  2.06it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  15% 7/47 [00:01<00:09,  4.37it/s]\u001b[A\n","Epoch 21:  80% 150/188 [01:12<00:18,  2.07it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  19% 9/47 [00:02<00:09,  4.06it/s]\u001b[A\n","Epoch 21:  81% 152/188 [01:12<00:17,  2.08it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  23% 11/47 [00:02<00:08,  4.10it/s]\u001b[A\n","Epoch 21:  82% 154/188 [01:13<00:16,  2.10it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  28% 13/47 [00:03<00:08,  3.81it/s]\u001b[A\n","Epoch 21:  83% 156/188 [01:13<00:15,  2.11it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  32% 15/47 [00:03<00:08,  3.69it/s]\u001b[A\n","Epoch 21:  84% 158/188 [01:14<00:14,  2.12it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  36% 17/47 [00:04<00:08,  3.40it/s]\u001b[A\n","Epoch 21:  85% 160/188 [01:15<00:13,  2.13it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  40% 19/47 [00:05<00:07,  3.55it/s]\u001b[A\n","Epoch 21:  86% 162/188 [01:15<00:12,  2.14it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  45% 21/47 [00:05<00:06,  4.14it/s]\u001b[A\n","Epoch 21:  87% 164/188 [01:16<00:11,  2.16it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  49% 23/47 [00:05<00:04,  4.98it/s]\u001b[A\n","Epoch 21:  88% 166/188 [01:16<00:10,  2.17it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  53% 25/47 [00:06<00:04,  5.43it/s]\u001b[A\n","Epoch 21:  89% 168/188 [01:16<00:09,  2.19it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  57% 27/47 [00:06<00:03,  5.78it/s]\u001b[A\n","Epoch 21:  90% 170/188 [01:17<00:08,  2.21it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  62% 29/47 [00:06<00:03,  5.91it/s]\u001b[A\n","Epoch 21:  91% 172/188 [01:17<00:07,  2.22it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  66% 31/47 [00:07<00:02,  5.78it/s]\u001b[A\n","Epoch 21:  93% 174/188 [01:17<00:06,  2.24it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  70% 33/47 [00:07<00:02,  5.40it/s]\u001b[A\n","Epoch 21:  94% 176/188 [01:18<00:05,  2.25it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.29it/s]\u001b[A\n","Epoch 21:  95% 178/188 [01:18<00:04,  2.27it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  79% 37/47 [00:08<00:01,  5.63it/s]\u001b[A\n","Epoch 21:  96% 180/188 [01:18<00:03,  2.28it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.84it/s]\u001b[A\n","Epoch 21:  97% 182/188 [01:19<00:02,  2.30it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.99it/s]\u001b[A\n","Epoch 21:  98% 184/188 [01:19<00:01,  2.32it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  91% 43/47 [00:09<00:00,  5.76it/s]\u001b[A\n","Epoch 21:  99% 186/188 [01:19<00:00,  2.33it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Validating:  96% 45/47 [00:09<00:00,  5.52it/s]\u001b[A\n","Epoch 21: 100% 188/188 [01:20<00:00,  2.34it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=548.0, train_loss_epoch=596.0, val_loss_step=599.0]\n","Epoch 21: 100% 188/188 [01:20<00:00,  2.34it/s, loss=589, v_num=0, val_loss_epoch=591.0, train_loss_step=461.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Epoch 22:  76% 142/188 [01:17<00:25,  1.82it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.84it/s]\u001b[A\n","Epoch 22:  77% 144/188 [01:18<00:23,  1.84it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.34it/s]\u001b[A\n","Epoch 22:  78% 146/188 [01:18<00:22,  1.85it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  11% 5/47 [00:01<00:08,  5.22it/s]\u001b[A\n","Epoch 22:  79% 148/188 [01:19<00:21,  1.87it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.76it/s]\u001b[A\n","Epoch 22:  80% 150/188 [01:19<00:20,  1.89it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.93it/s]\u001b[A\n","Epoch 22:  81% 152/188 [01:19<00:18,  1.91it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  23% 11/47 [00:02<00:05,  6.05it/s]\u001b[A\n","Epoch 22:  82% 154/188 [01:20<00:17,  1.92it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.17it/s]\u001b[A\n","Epoch 22:  83% 156/188 [01:20<00:16,  1.94it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.02it/s]\u001b[A\n","Epoch 22:  84% 158/188 [01:20<00:15,  1.96it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.68it/s]\u001b[A\n","Epoch 22:  85% 160/188 [01:21<00:14,  1.97it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.38it/s]\u001b[A\n","Epoch 22:  86% 162/188 [01:21<00:13,  1.99it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  45% 21/47 [00:03<00:05,  5.12it/s]\u001b[A\n","Epoch 22:  87% 164/188 [01:21<00:11,  2.00it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.51it/s]\u001b[A\n","Epoch 22:  88% 166/188 [01:22<00:10,  2.02it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.83it/s]\u001b[A\n","Epoch 22:  89% 168/188 [01:22<00:09,  2.04it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  57% 27/47 [00:04<00:03,  6.00it/s]\u001b[A\n","Epoch 22:  90% 170/188 [01:22<00:08,  2.05it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  62% 29/47 [00:05<00:02,  6.09it/s]\u001b[A\n","Epoch 22:  91% 172/188 [01:23<00:07,  2.07it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.20it/s]\u001b[A\n","Epoch 22:  93% 174/188 [01:23<00:06,  2.08it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.85it/s]\u001b[A\n","Epoch 22:  94% 176/188 [01:23<00:05,  2.10it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.61it/s]\u001b[A\n","Epoch 22:  95% 178/188 [01:24<00:04,  2.11it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.24it/s]\u001b[A\n","Epoch 22:  96% 180/188 [01:24<00:03,  2.13it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  83% 39/47 [00:06<00:01,  5.14it/s]\u001b[A\n","Epoch 22:  97% 182/188 [01:25<00:02,  2.14it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.33it/s]\u001b[A\n","Epoch 22:  98% 184/188 [01:25<00:01,  2.15it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  91% 43/47 [00:07<00:00,  5.36it/s]\u001b[A\n","Epoch 22:  99% 186/188 [01:25<00:00,  2.17it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.30it/s]\u001b[A\n","Epoch 22: 100% 188/188 [01:26<00:00,  2.18it/s, loss=594, v_num=0, val_loss_epoch=591.0, train_loss_step=626.0, train_loss_epoch=596.0, val_loss_step=596.0]\n","Epoch 22: 100% 188/188 [01:26<00:00,  2.18it/s, loss=594, v_num=0, val_loss_epoch=590.0, train_loss_step=545.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Epoch 23:  76% 142/188 [01:11<00:23,  1.98it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:15,  2.90it/s]\u001b[A\n","Epoch 23:  77% 144/188 [01:12<00:22,  1.99it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.37it/s]\u001b[A\n","Epoch 23:  78% 146/188 [01:12<00:20,  2.01it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.83it/s]\u001b[A\n","Epoch 23:  79% 148/188 [01:13<00:19,  2.02it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  15% 7/47 [00:01<00:08,  4.94it/s]\u001b[A\n","Epoch 23:  80% 150/188 [01:13<00:18,  2.04it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  19% 9/47 [00:01<00:07,  4.96it/s]\u001b[A\n","Epoch 23:  81% 152/188 [01:13<00:17,  2.05it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  23% 11/47 [00:02<00:07,  5.00it/s]\u001b[A\n","Epoch 23:  82% 154/188 [01:14<00:16,  2.07it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.11it/s]\u001b[A\n","Epoch 23:  83% 156/188 [01:14<00:15,  2.09it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  32% 15/47 [00:03<00:06,  5.13it/s]\u001b[A\n","Epoch 23:  84% 158/188 [01:15<00:14,  2.10it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.41it/s]\u001b[A\n","Epoch 23:  85% 160/188 [01:15<00:13,  2.12it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.87it/s]\u001b[A\n","Epoch 23:  86% 162/188 [01:15<00:12,  2.14it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  45% 21/47 [00:04<00:04,  6.02it/s]\u001b[A\n","Epoch 23:  87% 164/188 [01:16<00:11,  2.15it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.96it/s]\u001b[A\n","Epoch 23:  88% 166/188 [01:16<00:10,  2.17it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.07it/s]\u001b[A\n","Epoch 23:  89% 168/188 [01:16<00:09,  2.19it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  57% 27/47 [00:05<00:03,  6.06it/s]\u001b[A\n","Epoch 23:  90% 170/188 [01:17<00:08,  2.20it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.79it/s]\u001b[A\n","Epoch 23:  91% 172/188 [01:17<00:07,  2.22it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.41it/s]\u001b[A\n","Epoch 23:  93% 174/188 [01:17<00:06,  2.23it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.25it/s]\u001b[A\n","Epoch 23:  94% 176/188 [01:18<00:05,  2.25it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.51it/s]\u001b[A\n","Epoch 23:  95% 178/188 [01:18<00:04,  2.26it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.84it/s]\u001b[A\n","Epoch 23:  96% 180/188 [01:18<00:03,  2.28it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  83% 39/47 [00:07<00:01,  6.07it/s]\u001b[A\n","Epoch 23:  97% 182/188 [01:19<00:02,  2.30it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.77it/s]\u001b[A\n","Epoch 23:  98% 184/188 [01:19<00:01,  2.31it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  91% 43/47 [00:07<00:00,  5.73it/s]\u001b[A\n","Epoch 23:  99% 186/188 [01:19<00:00,  2.33it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.69it/s]\u001b[A\n","Epoch 23: 100% 188/188 [01:20<00:00,  2.34it/s, loss=596, v_num=0, val_loss_epoch=590.0, train_loss_step=572.0, train_loss_epoch=595.0, val_loss_step=594.0]\n","Epoch 23: 100% 188/188 [01:20<00:00,  2.33it/s, loss=596, v_num=0, val_loss_epoch=589.0, train_loss_step=521.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Epoch 24:  76% 142/188 [01:15<00:24,  1.87it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:15,  2.98it/s]\u001b[A\n","Epoch 24:  77% 144/188 [01:16<00:23,  1.88it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:   6% 3/47 [00:00<00:09,  4.48it/s]\u001b[A\n","Epoch 24:  78% 146/188 [01:16<00:22,  1.90it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.86it/s]\u001b[A\n","Epoch 24:  79% 148/188 [01:17<00:20,  1.91it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  15% 7/47 [00:01<00:08,  4.96it/s]\u001b[A\n","Epoch 24:  80% 150/188 [01:17<00:19,  1.93it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.18it/s]\u001b[A\n","Epoch 24:  81% 152/188 [01:18<00:18,  1.95it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.23it/s]\u001b[A\n","Epoch 24:  82% 154/188 [01:18<00:17,  1.96it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.34it/s]\u001b[A\n","Epoch 24:  83% 156/188 [01:18<00:16,  1.98it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  32% 15/47 [00:03<00:06,  5.27it/s]\u001b[A\n","Epoch 24:  84% 158/188 [01:19<00:15,  2.00it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.23it/s]\u001b[A\n","Epoch 24:  85% 160/188 [01:19<00:13,  2.01it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.23it/s]\u001b[A\n","Epoch 24:  86% 162/188 [01:19<00:12,  2.03it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.26it/s]\u001b[A\n","Epoch 24:  87% 164/188 [01:20<00:11,  2.04it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.39it/s]\u001b[A\n","Epoch 24:  88% 166/188 [01:20<00:10,  2.06it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.72it/s]\u001b[A\n","Epoch 24:  89% 168/188 [01:20<00:09,  2.07it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.86it/s]\u001b[A\n","Epoch 24:  90% 170/188 [01:21<00:08,  2.09it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  62% 29/47 [00:05<00:02,  6.08it/s]\u001b[A\n","Epoch 24:  91% 172/188 [01:21<00:07,  2.11it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.03it/s]\u001b[A\n","Epoch 24:  93% 174/188 [01:21<00:06,  2.12it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  70% 33/47 [00:06<00:02,  6.02it/s]\u001b[A\n","Epoch 24:  94% 176/188 [01:22<00:05,  2.14it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  74% 35/47 [00:06<00:01,  6.09it/s]\u001b[A\n","Epoch 24:  95% 178/188 [01:22<00:04,  2.15it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  79% 37/47 [00:06<00:01,  6.08it/s]\u001b[A\n","Epoch 24:  96% 180/188 [01:22<00:03,  2.17it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  83% 39/47 [00:07<00:01,  6.11it/s]\u001b[A\n","Epoch 24:  97% 182/188 [01:23<00:02,  2.19it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  87% 41/47 [00:07<00:00,  6.23it/s]\u001b[A\n","Epoch 24:  98% 184/188 [01:23<00:01,  2.20it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  91% 43/47 [00:07<00:00,  5.80it/s]\u001b[A\n","Epoch 24:  99% 186/188 [01:24<00:00,  2.21it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.45it/s]\u001b[A\n","Epoch 24: 100% 188/188 [01:24<00:00,  2.23it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=552.0, train_loss_epoch=595.0, val_loss_step=599.0]\n","Epoch 24: 100% 188/188 [01:24<00:00,  2.22it/s, loss=580, v_num=0, val_loss_epoch=589.0, train_loss_step=533.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Epoch 25:  76% 142/188 [01:12<00:23,  1.95it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:15,  2.96it/s]\u001b[A\n","Epoch 25:  77% 144/188 [01:13<00:22,  1.96it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:   6% 3/47 [00:00<00:09,  4.41it/s]\u001b[A\n","Epoch 25:  78% 146/188 [01:13<00:21,  1.98it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  11% 5/47 [00:01<00:08,  5.03it/s]\u001b[A\n","Epoch 25:  79% 148/188 [01:14<00:20,  2.00it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.44it/s]\u001b[A\n","Epoch 25:  80% 150/188 [01:14<00:18,  2.02it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.78it/s]\u001b[A\n","Epoch 25:  81% 152/188 [01:14<00:17,  2.03it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.96it/s]\u001b[A\n","Epoch 25:  82% 154/188 [01:15<00:16,  2.05it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.99it/s]\u001b[A\n","Epoch 25:  83% 156/188 [01:15<00:15,  2.07it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.10it/s]\u001b[A\n","Epoch 25:  84% 158/188 [01:15<00:14,  2.09it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.86it/s]\u001b[A\n","Epoch 25:  85% 160/188 [01:16<00:13,  2.10it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.48it/s]\u001b[A\n","Epoch 25:  86% 162/188 [01:16<00:12,  2.12it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.41it/s]\u001b[A\n","Epoch 25:  87% 164/188 [01:16<00:11,  2.14it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.57it/s]\u001b[A\n","Epoch 25:  88% 166/188 [01:17<00:10,  2.15it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.88it/s]\u001b[A\n","Epoch 25:  89% 168/188 [01:17<00:09,  2.17it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.87it/s]\u001b[A\n","Epoch 25:  90% 170/188 [01:17<00:08,  2.19it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.91it/s]\u001b[A\n","Epoch 25:  91% 172/188 [01:18<00:07,  2.20it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.13it/s]\u001b[A\n","Epoch 25:  93% 174/188 [01:18<00:06,  2.22it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  70% 33/47 [00:05<00:02,  6.10it/s]\u001b[A\n","Epoch 25:  94% 176/188 [01:18<00:05,  2.23it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.84it/s]\u001b[A\n","Epoch 25:  95% 178/188 [01:19<00:04,  2.25it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.47it/s]\u001b[A\n","Epoch 25:  96% 180/188 [01:19<00:03,  2.26it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  83% 39/47 [00:06<00:01,  5.26it/s]\u001b[A\n","Epoch 25:  97% 182/188 [01:19<00:02,  2.28it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.24it/s]\u001b[A\n","Epoch 25:  98% 184/188 [01:20<00:01,  2.29it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  91% 43/47 [00:07<00:00,  5.13it/s]\u001b[A\n","Epoch 25:  99% 186/188 [01:20<00:00,  2.30it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.24it/s]\u001b[A\n","Epoch 25: 100% 188/188 [01:21<00:00,  2.32it/s, loss=588, v_num=0, val_loss_epoch=589.0, train_loss_step=562.0, train_loss_epoch=594.0, val_loss_step=598.0]\n","Epoch 25: 100% 188/188 [01:21<00:00,  2.31it/s, loss=588, v_num=0, val_loss_epoch=588.0, train_loss_step=604.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Epoch 26:  76% 142/188 [01:14<00:24,  1.90it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:22,  2.03it/s]\u001b[A\n","Epoch 26:  77% 144/188 [01:15<00:23,  1.91it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:   6% 3/47 [00:00<00:11,  3.89it/s]\u001b[A\n","Epoch 26:  78% 146/188 [01:15<00:21,  1.92it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.67it/s]\u001b[A\n","Epoch 26:  79% 148/188 [01:16<00:20,  1.94it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  15% 7/47 [00:01<00:08,  4.92it/s]\u001b[A\n","Epoch 26:  80% 150/188 [01:16<00:19,  1.96it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  19% 9/47 [00:02<00:07,  4.99it/s]\u001b[A\n","Epoch 26:  81% 152/188 [01:17<00:18,  1.97it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.17it/s]\u001b[A\n","Epoch 26:  82% 154/188 [01:17<00:17,  1.99it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.53it/s]\u001b[A\n","Epoch 26:  83% 156/188 [01:17<00:15,  2.01it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  32% 15/47 [00:03<00:05,  5.84it/s]\u001b[A\n","Epoch 26:  84% 158/188 [01:18<00:14,  2.02it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  36% 17/47 [00:03<00:04,  6.05it/s]\u001b[A\n","Epoch 26:  85% 160/188 [01:18<00:13,  2.04it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.62it/s]\u001b[A\n","Epoch 26:  86% 162/188 [01:18<00:12,  2.05it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.30it/s]\u001b[A\n","Epoch 26:  87% 164/188 [01:19<00:11,  2.07it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.30it/s]\u001b[A\n","Epoch 26:  88% 166/188 [01:19<00:10,  2.09it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.67it/s]\u001b[A\n","Epoch 26:  89% 168/188 [01:19<00:09,  2.10it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.96it/s]\u001b[A\n","Epoch 26:  90% 170/188 [01:20<00:08,  2.12it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  62% 29/47 [00:05<00:02,  6.06it/s]\u001b[A\n","Epoch 26:  91% 172/188 [01:20<00:07,  2.13it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.42it/s]\u001b[A\n","Epoch 26:  93% 174/188 [01:21<00:06,  2.15it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.15it/s]\u001b[A\n","Epoch 26:  94% 176/188 [01:21<00:05,  2.16it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.03it/s]\u001b[A\n","Epoch 26:  95% 178/188 [01:21<00:04,  2.17it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  79% 37/47 [00:07<00:02,  4.85it/s]\u001b[A\n","Epoch 26:  96% 180/188 [01:22<00:03,  2.19it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.03it/s]\u001b[A\n","Epoch 26:  97% 182/188 [01:22<00:02,  2.20it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  87% 41/47 [00:07<00:01,  5.12it/s]\u001b[A\n","Epoch 26:  98% 184/188 [01:23<00:01,  2.22it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  91% 43/47 [00:08<00:00,  5.20it/s]\u001b[A\n","Epoch 26:  99% 186/188 [01:23<00:00,  2.23it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.29it/s]\u001b[A\n","Epoch 26: 100% 188/188 [01:23<00:00,  2.24it/s, loss=582, v_num=0, val_loss_epoch=588.0, train_loss_step=640.0, train_loss_epoch=594.0, val_loss_step=595.0]\n","Epoch 26: 100% 188/188 [01:24<00:00,  2.24it/s, loss=582, v_num=0, val_loss_epoch=587.0, train_loss_step=556.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Epoch 27:  76% 142/188 [01:14<00:23,  1.92it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.34it/s]\u001b[A\n","Epoch 27:  77% 144/188 [01:14<00:22,  1.93it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.04it/s]\u001b[A\n","Epoch 27:  78% 146/188 [01:14<00:21,  1.95it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.74it/s]\u001b[A\n","Epoch 27:  79% 148/188 [01:15<00:20,  1.97it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.98it/s]\u001b[A\n","Epoch 27:  80% 150/188 [01:15<00:19,  1.99it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.96it/s]\u001b[A\n","Epoch 27:  81% 152/188 [01:15<00:17,  2.00it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.08it/s]\u001b[A\n","Epoch 27:  82% 154/188 [01:16<00:16,  2.02it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.15it/s]\u001b[A\n","Epoch 27:  83% 156/188 [01:16<00:15,  2.04it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.11it/s]\u001b[A\n","Epoch 27:  84% 158/188 [01:16<00:14,  2.06it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.09it/s]\u001b[A\n","Epoch 27:  85% 160/188 [01:17<00:13,  2.07it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.77it/s]\u001b[A\n","Epoch 27:  86% 162/188 [01:17<00:12,  2.09it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.30it/s]\u001b[A\n","Epoch 27:  87% 164/188 [01:17<00:11,  2.10it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.19it/s]\u001b[A\n","Epoch 27:  88% 166/188 [01:18<00:10,  2.12it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.17it/s]\u001b[A\n","Epoch 27:  89% 168/188 [01:18<00:09,  2.13it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.05it/s]\u001b[A\n","Epoch 27:  90% 170/188 [01:19<00:08,  2.15it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.13it/s]\u001b[A\n","Epoch 27:  91% 172/188 [01:19<00:07,  2.16it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  66% 31/47 [00:05<00:03,  5.08it/s]\u001b[A\n","Epoch 27:  93% 174/188 [01:19<00:06,  2.18it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.15it/s]\u001b[A\n","Epoch 27:  94% 176/188 [01:20<00:05,  2.19it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.18it/s]\u001b[A\n","Epoch 27:  95% 178/188 [01:20<00:04,  2.21it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.10it/s]\u001b[A\n","Epoch 27:  96% 180/188 [01:21<00:03,  2.22it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.08it/s]\u001b[A\n","Epoch 27:  97% 182/188 [01:21<00:02,  2.23it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  87% 41/47 [00:07<00:01,  4.82it/s]\u001b[A\n","Epoch 27:  98% 184/188 [01:21<00:01,  2.25it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.11it/s]\u001b[A\n","Epoch 27:  99% 186/188 [01:22<00:00,  2.26it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.31it/s]\u001b[A\n","Epoch 27: 100% 188/188 [01:22<00:00,  2.27it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=570.0, train_loss_epoch=593.0, val_loss_step=593.0]\n","Epoch 27: 100% 188/188 [01:23<00:00,  2.26it/s, loss=586, v_num=0, val_loss_epoch=587.0, train_loss_step=521.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Epoch 28:  76% 142/188 [01:15<00:24,  1.88it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:17,  2.70it/s]\u001b[A\n","Epoch 28:  77% 144/188 [01:16<00:23,  1.89it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.32it/s]\u001b[A\n","Epoch 28:  78% 146/188 [01:16<00:22,  1.91it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.85it/s]\u001b[A\n","Epoch 28:  79% 148/188 [01:16<00:20,  1.93it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.23it/s]\u001b[A\n","Epoch 28:  80% 150/188 [01:17<00:19,  1.94it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.63it/s]\u001b[A\n","Epoch 28:  81% 152/188 [01:17<00:18,  1.96it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.93it/s]\u001b[A\n","Epoch 28:  82% 154/188 [01:17<00:17,  1.98it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.94it/s]\u001b[A\n","Epoch 28:  83% 156/188 [01:18<00:16,  1.99it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.64it/s]\u001b[A\n","Epoch 28:  84% 158/188 [01:18<00:14,  2.01it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.42it/s]\u001b[A\n","Epoch 28:  85% 160/188 [01:18<00:13,  2.03it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.31it/s]\u001b[A\n","Epoch 28:  86% 162/188 [01:19<00:12,  2.04it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.22it/s]\u001b[A\n","Epoch 28:  87% 164/188 [01:19<00:11,  2.06it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.15it/s]\u001b[A\n","Epoch 28:  88% 166/188 [01:20<00:10,  2.07it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.22it/s]\u001b[A\n","Epoch 28:  89% 168/188 [01:20<00:09,  2.09it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.22it/s]\u001b[A\n","Epoch 28:  90% 170/188 [01:20<00:08,  2.10it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.19it/s]\u001b[A\n","Epoch 28:  91% 172/188 [01:21<00:07,  2.12it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  66% 31/47 [00:05<00:03,  5.32it/s]\u001b[A\n","Epoch 28:  93% 174/188 [01:21<00:06,  2.13it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.27it/s]\u001b[A\n","Epoch 28:  94% 176/188 [01:22<00:05,  2.14it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.44it/s]\u001b[A\n","Epoch 28:  95% 178/188 [01:22<00:04,  2.16it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.84it/s]\u001b[A\n","Epoch 28:  96% 180/188 [01:22<00:03,  2.17it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.09it/s]\u001b[A\n","Epoch 28:  97% 182/188 [01:23<00:02,  2.19it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  87% 41/47 [00:07<00:01,  4.75it/s]\u001b[A\n","Epoch 28:  98% 184/188 [01:23<00:01,  2.20it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.64it/s]\u001b[A\n","Epoch 28:  99% 186/188 [01:24<00:00,  2.21it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.59it/s]\u001b[A\n","Epoch 28: 100% 188/188 [01:24<00:00,  2.22it/s, loss=584, v_num=0, val_loss_epoch=587.0, train_loss_step=525.0, train_loss_epoch=593.0, val_loss_step=595.0]\n","Epoch 28: 100% 188/188 [01:24<00:00,  2.22it/s, loss=584, v_num=0, val_loss_epoch=586.0, train_loss_step=557.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Epoch 29:  76% 142/188 [01:12<00:23,  1.97it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.54it/s]\u001b[A\n","Epoch 29:  77% 144/188 [01:12<00:22,  1.99it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.24it/s]\u001b[A\n","Epoch 29:  78% 146/188 [01:12<00:20,  2.01it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.66it/s]\u001b[A\n","Epoch 29:  79% 148/188 [01:13<00:19,  2.02it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.95it/s]\u001b[A\n","Epoch 29:  80% 150/188 [01:13<00:18,  2.04it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  19% 9/47 [00:01<00:06,  6.05it/s]\u001b[A\n","Epoch 29:  81% 152/188 [01:13<00:17,  2.06it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  23% 11/47 [00:01<00:06,  5.75it/s]\u001b[A\n","Epoch 29:  82% 154/188 [01:14<00:16,  2.08it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.61it/s]\u001b[A\n","Epoch 29:  83% 156/188 [01:14<00:15,  2.09it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.50it/s]\u001b[A\n","Epoch 29:  84% 158/188 [01:14<00:14,  2.11it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.53it/s]\u001b[A\n","Epoch 29:  85% 160/188 [01:15<00:13,  2.13it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.78it/s]\u001b[A\n","Epoch 29:  86% 162/188 [01:15<00:12,  2.14it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.04it/s]\u001b[A\n","Epoch 29:  87% 164/188 [01:15<00:11,  2.16it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.53it/s]\u001b[A\n","Epoch 29:  88% 166/188 [01:16<00:10,  2.18it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.32it/s]\u001b[A\n","Epoch 29:  89% 168/188 [01:16<00:09,  2.19it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.36it/s]\u001b[A\n","Epoch 29:  90% 170/188 [01:17<00:08,  2.21it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.54it/s]\u001b[A\n","Epoch 29:  91% 172/188 [01:17<00:07,  2.22it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.72it/s]\u001b[A\n","Epoch 29:  93% 174/188 [01:17<00:06,  2.24it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.84it/s]\u001b[A\n","Epoch 29:  94% 176/188 [01:18<00:05,  2.25it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.08it/s]\u001b[A\n","Epoch 29:  95% 178/188 [01:18<00:04,  2.27it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  79% 37/47 [00:06<00:02,  4.76it/s]\u001b[A\n","Epoch 29:  96% 180/188 [01:19<00:03,  2.28it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  83% 39/47 [00:07<00:01,  4.50it/s]\u001b[A\n","Epoch 29:  97% 182/188 [01:19<00:02,  2.29it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  87% 41/47 [00:07<00:01,  4.63it/s]\u001b[A\n","Epoch 29:  98% 184/188 [01:19<00:01,  2.30it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.33it/s]\u001b[A\n","Epoch 29:  99% 186/188 [01:20<00:00,  2.31it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.17it/s]\u001b[A\n","Epoch 29: 100% 188/188 [01:20<00:00,  2.32it/s, loss=590, v_num=0, val_loss_epoch=586.0, train_loss_step=599.0, train_loss_epoch=593.0, val_loss_step=588.0]\n","Epoch 29: 100% 188/188 [01:21<00:00,  2.31it/s, loss=590, v_num=0, val_loss_epoch=585.0, train_loss_step=550.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Epoch 30:  76% 142/188 [01:15<00:24,  1.88it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.84it/s]\u001b[A\n","Epoch 30:  77% 144/188 [01:16<00:23,  1.89it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.24it/s]\u001b[A\n","Epoch 30:  78% 146/188 [01:16<00:22,  1.90it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  11% 5/47 [00:01<00:08,  5.04it/s]\u001b[A\n","Epoch 30:  79% 148/188 [01:16<00:20,  1.92it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.47it/s]\u001b[A\n","Epoch 30:  80% 150/188 [01:17<00:19,  1.94it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.83it/s]\u001b[A\n","Epoch 30:  81% 152/188 [01:17<00:18,  1.96it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.97it/s]\u001b[A\n","Epoch 30:  82% 154/188 [01:17<00:17,  1.98it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.05it/s]\u001b[A\n","Epoch 30:  83% 156/188 [01:18<00:16,  1.99it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.13it/s]\u001b[A\n","Epoch 30:  84% 158/188 [01:18<00:14,  2.01it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  36% 17/47 [00:03<00:04,  6.18it/s]\u001b[A\n","Epoch 30:  85% 160/188 [01:18<00:13,  2.03it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.19it/s]\u001b[A\n","Epoch 30:  86% 162/188 [01:19<00:12,  2.04it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.87it/s]\u001b[A\n","Epoch 30:  87% 164/188 [01:19<00:11,  2.06it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  49% 23/47 [00:04<00:03,  6.02it/s]\u001b[A\n","Epoch 30:  88% 166/188 [01:19<00:10,  2.08it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.18it/s]\u001b[A\n","Epoch 30:  89% 168/188 [01:20<00:09,  2.09it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  57% 27/47 [00:04<00:03,  6.22it/s]\u001b[A\n","Epoch 30:  90% 170/188 [01:20<00:08,  2.11it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  62% 29/47 [00:05<00:02,  6.27it/s]\u001b[A\n","Epoch 30:  91% 172/188 [01:20<00:07,  2.13it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.53it/s]\u001b[A\n","Epoch 30:  93% 174/188 [01:21<00:06,  2.14it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  70% 33/47 [00:05<00:02,  4.84it/s]\u001b[A\n","Epoch 30:  94% 176/188 [01:21<00:05,  2.15it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  74% 35/47 [00:06<00:02,  4.40it/s]\u001b[A\n","Epoch 30:  95% 178/188 [01:22<00:04,  2.16it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  79% 37/47 [00:06<00:02,  4.06it/s]\u001b[A\n","Epoch 30:  96% 180/188 [01:22<00:03,  2.17it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  83% 39/47 [00:07<00:02,  3.85it/s]\u001b[A\n","Epoch 30:  97% 182/188 [01:23<00:02,  2.18it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  87% 41/47 [00:07<00:01,  3.88it/s]\u001b[A\n","Epoch 30:  98% 184/188 [01:23<00:01,  2.19it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  91% 43/47 [00:08<00:01,  3.92it/s]\u001b[A\n","Epoch 30:  99% 186/188 [01:24<00:00,  2.20it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Validating:  96% 45/47 [00:09<00:00,  3.89it/s]\u001b[A\n","Epoch 30: 100% 188/188 [01:24<00:00,  2.21it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=636.0, train_loss_epoch=592.0, val_loss_step=593.0]\n","Epoch 30: 100% 188/188 [01:25<00:00,  2.21it/s, loss=573, v_num=0, val_loss_epoch=585.0, train_loss_step=491.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Epoch 31:  76% 142/188 [01:12<00:23,  1.96it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.30it/s]\u001b[A\n","Epoch 31:  77% 144/188 [01:12<00:22,  1.98it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.07it/s]\u001b[A\n","Epoch 31:  78% 146/188 [01:13<00:21,  2.00it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.47it/s]\u001b[A\n","Epoch 31:  79% 148/188 [01:13<00:19,  2.01it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.47it/s]\u001b[A\n","Epoch 31:  80% 150/188 [01:13<00:18,  2.03it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.32it/s]\u001b[A\n","Epoch 31:  81% 152/188 [01:14<00:17,  2.05it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.55it/s]\u001b[A\n","Epoch 31:  82% 154/188 [01:14<00:16,  2.07it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.82it/s]\u001b[A\n","Epoch 31:  83% 156/188 [01:14<00:15,  2.08it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.90it/s]\u001b[A\n","Epoch 31:  84% 158/188 [01:15<00:14,  2.10it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  36% 17/47 [00:03<00:04,  6.12it/s]\u001b[A\n","Epoch 31:  85% 160/188 [01:15<00:13,  2.12it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.09it/s]\u001b[A\n","Epoch 31:  86% 162/188 [01:15<00:12,  2.13it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.09it/s]\u001b[A\n","Epoch 31:  87% 164/188 [01:16<00:11,  2.15it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  49% 23/47 [00:04<00:03,  6.16it/s]\u001b[A\n","Epoch 31:  88% 166/188 [01:16<00:10,  2.17it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.15it/s]\u001b[A\n","Epoch 31:  89% 168/188 [01:16<00:09,  2.19it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.68it/s]\u001b[A\n","Epoch 31:  90% 170/188 [01:17<00:08,  2.20it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.02it/s]\u001b[A\n","Epoch 31:  91% 172/188 [01:17<00:07,  2.21it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  66% 31/47 [00:05<00:03,  4.69it/s]\u001b[A\n","Epoch 31:  93% 174/188 [01:18<00:06,  2.22it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  70% 33/47 [00:06<00:03,  4.46it/s]\u001b[A\n","Epoch 31:  94% 176/188 [01:18<00:05,  2.24it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  74% 35/47 [00:06<00:02,  4.46it/s]\u001b[A\n","Epoch 31:  95% 178/188 [01:19<00:04,  2.25it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  79% 37/47 [00:07<00:02,  4.48it/s]\u001b[A\n","Epoch 31:  96% 180/188 [01:19<00:03,  2.26it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  83% 39/47 [00:07<00:01,  4.57it/s]\u001b[A\n","Epoch 31:  97% 182/188 [01:19<00:02,  2.28it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  87% 41/47 [00:07<00:01,  4.50it/s]\u001b[A\n","Epoch 31:  98% 184/188 [01:20<00:01,  2.29it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.45it/s]\u001b[A\n","Epoch 31:  99% 186/188 [01:20<00:00,  2.30it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.52it/s]\u001b[A\n","Epoch 31: 100% 188/188 [01:21<00:00,  2.31it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=576.0, train_loss_epoch=592.0, val_loss_step=592.0]\n","Epoch 31: 100% 188/188 [01:21<00:00,  2.30it/s, loss=581, v_num=0, val_loss_epoch=585.0, train_loss_step=455.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Epoch 32:  76% 142/188 [01:08<00:22,  2.06it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.50it/s]\u001b[A\n","Epoch 32:  77% 144/188 [01:09<00:21,  2.08it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.10it/s]\u001b[A\n","Epoch 32:  78% 146/188 [01:09<00:20,  2.10it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.78it/s]\u001b[A\n","Epoch 32:  79% 148/188 [01:09<00:18,  2.12it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.94it/s]\u001b[A\n","Epoch 32:  80% 150/188 [01:10<00:17,  2.14it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  19% 9/47 [00:01<00:06,  6.00it/s]\u001b[A\n","Epoch 32:  81% 152/188 [01:10<00:16,  2.16it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.11it/s]\u001b[A\n","Epoch 32:  82% 154/188 [01:10<00:15,  2.17it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.14it/s]\u001b[A\n","Epoch 32:  83% 156/188 [01:11<00:14,  2.19it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.85it/s]\u001b[A\n","Epoch 32:  84% 158/188 [01:11<00:13,  2.21it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.03it/s]\u001b[A\n","Epoch 32:  85% 160/188 [01:11<00:12,  2.23it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.05it/s]\u001b[A\n","Epoch 32:  86% 162/188 [01:12<00:11,  2.24it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.09it/s]\u001b[A\n","Epoch 32:  87% 164/188 [01:12<00:10,  2.26it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  49% 23/47 [00:03<00:03,  6.09it/s]\u001b[A\n","Epoch 32:  88% 166/188 [01:12<00:09,  2.28it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.00it/s]\u001b[A\n","Epoch 32:  89% 168/188 [01:13<00:08,  2.30it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  57% 27/47 [00:04<00:03,  6.01it/s]\u001b[A\n","Epoch 32:  90% 170/188 [01:13<00:07,  2.31it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  62% 29/47 [00:04<00:02,  6.15it/s]\u001b[A\n","Epoch 32:  91% 172/188 [01:13<00:06,  2.33it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.07it/s]\u001b[A\n","Epoch 32:  93% 174/188 [01:14<00:05,  2.35it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.91it/s]\u001b[A\n","Epoch 32:  94% 176/188 [01:14<00:05,  2.36it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  74% 35/47 [00:05<00:02,  5.04it/s]\u001b[A\n","Epoch 32:  95% 178/188 [01:14<00:04,  2.37it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.47it/s]\u001b[A\n","Epoch 32:  96% 180/188 [01:15<00:03,  2.39it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  83% 39/47 [00:06<00:01,  5.77it/s]\u001b[A\n","Epoch 32:  97% 182/188 [01:15<00:02,  2.41it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  87% 41/47 [00:06<00:01,  5.95it/s]\u001b[A\n","Epoch 32:  98% 184/188 [01:15<00:01,  2.42it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  91% 43/47 [00:07<00:00,  6.00it/s]\u001b[A\n","Epoch 32:  99% 186/188 [01:16<00:00,  2.44it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Validating:  96% 45/47 [00:07<00:00,  6.01it/s]\u001b[A\n","Epoch 32: 100% 188/188 [01:16<00:00,  2.45it/s, loss=586, v_num=0, val_loss_epoch=585.0, train_loss_step=571.0, train_loss_epoch=591.0, val_loss_step=597.0]\n","Epoch 32: 100% 188/188 [01:16<00:00,  2.45it/s, loss=586, v_num=0, val_loss_epoch=584.0, train_loss_step=509.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Epoch 33:  76% 142/188 [01:06<00:21,  2.15it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:17,  2.68it/s]\u001b[A\n","Epoch 33:  77% 144/188 [01:06<00:20,  2.16it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:   6% 3/47 [00:00<00:11,  3.84it/s]\u001b[A\n","Epoch 33:  78% 146/188 [01:07<00:19,  2.17it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  11% 5/47 [00:01<00:09,  4.23it/s]\u001b[A\n","Epoch 33:  79% 148/188 [01:07<00:18,  2.19it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  15% 7/47 [00:01<00:08,  4.45it/s]\u001b[A\n","Epoch 33:  80% 150/188 [01:08<00:17,  2.20it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  19% 9/47 [00:02<00:08,  4.56it/s]\u001b[A\n","Epoch 33:  81% 152/188 [01:08<00:16,  2.22it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.64it/s]\u001b[A\n","Epoch 33:  82% 154/188 [01:08<00:15,  2.23it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  28% 13/47 [00:02<00:07,  4.63it/s]\u001b[A\n","Epoch 33:  83% 156/188 [01:09<00:14,  2.25it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.55it/s]\u001b[A\n","Epoch 33:  84% 158/188 [01:09<00:13,  2.26it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.47it/s]\u001b[A\n","Epoch 33:  85% 160/188 [01:10<00:12,  2.28it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  40% 19/47 [00:04<00:06,  4.31it/s]\u001b[A\n","Epoch 33:  86% 162/188 [01:10<00:11,  2.29it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  45% 21/47 [00:04<00:06,  4.27it/s]\u001b[A\n","Epoch 33:  87% 164/188 [01:11<00:10,  2.30it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  49% 23/47 [00:05<00:05,  4.20it/s]\u001b[A\n","Epoch 33:  88% 166/188 [01:11<00:09,  2.31it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  53% 25/47 [00:05<00:04,  4.94it/s]\u001b[A\n","Epoch 33:  89% 168/188 [01:12<00:08,  2.33it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  57% 27/47 [00:06<00:03,  5.43it/s]\u001b[A\n","Epoch 33:  90% 170/188 [01:12<00:07,  2.35it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  62% 29/47 [00:06<00:03,  5.72it/s]\u001b[A\n","Epoch 33:  91% 172/188 [01:12<00:06,  2.37it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.99it/s]\u001b[A\n","Epoch 33:  93% 174/188 [01:13<00:05,  2.38it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.98it/s]\u001b[A\n","Epoch 33:  94% 176/188 [01:13<00:05,  2.40it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.97it/s]\u001b[A\n","Epoch 33:  95% 178/188 [01:13<00:04,  2.42it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  79% 37/47 [00:07<00:01,  6.06it/s]\u001b[A\n","Epoch 33:  96% 180/188 [01:14<00:03,  2.43it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.91it/s]\u001b[A\n","Epoch 33:  97% 182/188 [01:14<00:02,  2.45it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  87% 41/47 [00:08<00:00,  6.05it/s]\u001b[A\n","Epoch 33:  98% 184/188 [01:14<00:01,  2.46it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  91% 43/47 [00:08<00:00,  6.12it/s]\u001b[A\n","Epoch 33:  99% 186/188 [01:15<00:00,  2.48it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Validating:  96% 45/47 [00:08<00:00,  6.12it/s]\u001b[A\n","Epoch 33: 100% 188/188 [01:15<00:00,  2.50it/s, loss=600, v_num=0, val_loss_epoch=584.0, train_loss_step=678.0, train_loss_epoch=590.0, val_loss_step=602.0]\n","Epoch 33: 100% 188/188 [01:15<00:00,  2.49it/s, loss=600, v_num=0, val_loss_epoch=583.0, train_loss_step=573.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Epoch 34:  76% 142/188 [01:08<00:22,  2.07it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.44it/s]\u001b[A\n","Epoch 34:  77% 144/188 [01:09<00:21,  2.08it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.09it/s]\u001b[A\n","Epoch 34:  78% 146/188 [01:09<00:20,  2.10it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.59it/s]\u001b[A\n","Epoch 34:  79% 148/188 [01:09<00:18,  2.12it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.72it/s]\u001b[A\n","Epoch 34:  80% 150/188 [01:10<00:17,  2.14it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.76it/s]\u001b[A\n","Epoch 34:  81% 152/188 [01:10<00:16,  2.15it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  23% 11/47 [00:01<00:06,  5.89it/s]\u001b[A\n","Epoch 34:  82% 154/188 [01:10<00:15,  2.17it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.91it/s]\u001b[A\n","Epoch 34:  83% 156/188 [01:11<00:14,  2.19it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.97it/s]\u001b[A\n","Epoch 34:  84% 158/188 [01:11<00:13,  2.21it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.06it/s]\u001b[A\n","Epoch 34:  85% 160/188 [01:11<00:12,  2.23it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.96it/s]\u001b[A\n","Epoch 34:  86% 162/188 [01:12<00:11,  2.24it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.93it/s]\u001b[A\n","Epoch 34:  87% 164/188 [01:12<00:10,  2.26it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  49% 23/47 [00:03<00:04,  5.96it/s]\u001b[A\n","Epoch 34:  88% 166/188 [01:12<00:09,  2.28it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.82it/s]\u001b[A\n","Epoch 34:  89% 168/188 [01:13<00:08,  2.29it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.92it/s]\u001b[A\n","Epoch 34:  90% 170/188 [01:13<00:07,  2.31it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  62% 29/47 [00:04<00:03,  5.91it/s]\u001b[A\n","Epoch 34:  91% 172/188 [01:13<00:06,  2.33it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.81it/s]\u001b[A\n","Epoch 34:  93% 174/188 [01:14<00:05,  2.34it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.87it/s]\u001b[A\n","Epoch 34:  94% 176/188 [01:14<00:05,  2.36it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.35it/s]\u001b[A\n","Epoch 34:  95% 178/188 [01:15<00:04,  2.37it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  79% 37/47 [00:06<00:02,  4.68it/s]\u001b[A\n","Epoch 34:  96% 180/188 [01:15<00:03,  2.38it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  83% 39/47 [00:07<00:01,  4.49it/s]\u001b[A\n","Epoch 34:  97% 182/188 [01:16<00:02,  2.39it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  87% 41/47 [00:07<00:01,  4.44it/s]\u001b[A\n","Epoch 34:  98% 184/188 [01:16<00:01,  2.41it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  91% 43/47 [00:07<00:00,  4.33it/s]\u001b[A\n","Epoch 34:  99% 186/188 [01:16<00:00,  2.42it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.17it/s]\u001b[A\n","Epoch 34: 100% 188/188 [01:17<00:00,  2.43it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=598.0, train_loss_epoch=590.0, val_loss_step=588.0]\n","Epoch 34: 100% 188/188 [01:17<00:00,  2.42it/s, loss=589, v_num=0, val_loss_epoch=583.0, train_loss_step=525.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Epoch 35:  76% 142/188 [01:07<00:21,  2.11it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.51it/s]\u001b[A\n","Epoch 35:  77% 144/188 [01:07<00:20,  2.13it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.02it/s]\u001b[A\n","Epoch 35:  78% 146/188 [01:08<00:19,  2.14it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.73it/s]\u001b[A\n","Epoch 35:  79% 148/188 [01:08<00:18,  2.16it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.89it/s]\u001b[A\n","Epoch 35:  80% 150/188 [01:08<00:17,  2.18it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.91it/s]\u001b[A\n","Epoch 35:  81% 152/188 [01:09<00:16,  2.20it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.04it/s]\u001b[A\n","Epoch 35:  82% 154/188 [01:09<00:15,  2.22it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.01it/s]\u001b[A\n","Epoch 35:  83% 156/188 [01:09<00:14,  2.24it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.98it/s]\u001b[A\n","Epoch 35:  84% 158/188 [01:10<00:13,  2.26it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.06it/s]\u001b[A\n","Epoch 35:  85% 160/188 [01:10<00:12,  2.27it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.13it/s]\u001b[A\n","Epoch 35:  86% 162/188 [01:10<00:11,  2.29it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.97it/s]\u001b[A\n","Epoch 35:  87% 164/188 [01:11<00:10,  2.31it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  49% 23/47 [00:03<00:03,  6.03it/s]\u001b[A\n","Epoch 35:  88% 166/188 [01:11<00:09,  2.33it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.08it/s]\u001b[A\n","Epoch 35:  89% 168/188 [01:11<00:08,  2.34it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.84it/s]\u001b[A\n","Epoch 35:  90% 170/188 [01:12<00:07,  2.36it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  62% 29/47 [00:04<00:03,  5.95it/s]\u001b[A\n","Epoch 35:  91% 172/188 [01:12<00:06,  2.38it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.96it/s]\u001b[A\n","Epoch 35:  93% 174/188 [01:12<00:05,  2.39it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.82it/s]\u001b[A\n","Epoch 35:  94% 176/188 [01:13<00:04,  2.41it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  74% 35/47 [00:05<00:02,  5.89it/s]\u001b[A\n","Epoch 35:  95% 178/188 [01:13<00:04,  2.42it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.65it/s]\u001b[A\n","Epoch 35:  96% 180/188 [01:13<00:03,  2.44it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  83% 39/47 [00:06<00:01,  5.76it/s]\u001b[A\n","Epoch 35:  97% 182/188 [01:14<00:02,  2.46it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  87% 41/47 [00:06<00:01,  5.86it/s]\u001b[A\n","Epoch 35:  98% 184/188 [01:14<00:01,  2.47it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  91% 43/47 [00:07<00:00,  5.94it/s]\u001b[A\n","Epoch 35:  99% 186/188 [01:14<00:00,  2.49it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Validating:  96% 45/47 [00:07<00:00,  5.97it/s]\u001b[A\n","Epoch 35: 100% 188/188 [01:15<00:00,  2.50it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=544.0, train_loss_epoch=590.0, val_loss_step=591.0]\n","Epoch 35: 100% 188/188 [01:15<00:00,  2.50it/s, loss=585, v_num=0, val_loss_epoch=582.0, train_loss_step=507.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Epoch 36:  76% 142/188 [01:08<00:22,  2.06it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:14,  3.27it/s]\u001b[A\n","Epoch 36:  77% 144/188 [01:09<00:21,  2.07it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.02it/s]\u001b[A\n","Epoch 36:  78% 146/188 [01:09<00:20,  2.09it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.48it/s]\u001b[A\n","Epoch 36:  79% 148/188 [01:10<00:18,  2.11it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.73it/s]\u001b[A\n","Epoch 36:  80% 150/188 [01:10<00:17,  2.13it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.48it/s]\u001b[A\n","Epoch 36:  81% 152/188 [01:10<00:16,  2.15it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.93it/s]\u001b[A\n","Epoch 36:  82% 154/188 [01:11<00:15,  2.16it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  28% 13/47 [00:02<00:07,  4.66it/s]\u001b[A\n","Epoch 36:  83% 156/188 [01:11<00:14,  2.17it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.48it/s]\u001b[A\n","Epoch 36:  84% 158/188 [01:12<00:13,  2.19it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.38it/s]\u001b[A\n","Epoch 36:  85% 160/188 [01:12<00:12,  2.20it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  40% 19/47 [00:03<00:06,  4.34it/s]\u001b[A\n","Epoch 36:  86% 162/188 [01:13<00:11,  2.22it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  45% 21/47 [00:04<00:05,  4.38it/s]\u001b[A\n","Epoch 36:  87% 164/188 [01:13<00:10,  2.23it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  49% 23/47 [00:04<00:05,  4.42it/s]\u001b[A\n","Epoch 36:  88% 166/188 [01:14<00:09,  2.24it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  53% 25/47 [00:05<00:04,  4.52it/s]\u001b[A\n","Epoch 36:  89% 168/188 [01:14<00:08,  2.26it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  57% 27/47 [00:05<00:04,  4.50it/s]\u001b[A\n","Epoch 36:  90% 170/188 [01:14<00:07,  2.27it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  62% 29/47 [00:06<00:03,  4.52it/s]\u001b[A\n","Epoch 36:  91% 172/188 [01:15<00:07,  2.28it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.20it/s]\u001b[A\n","Epoch 36:  93% 174/188 [01:15<00:06,  2.29it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  70% 33/47 [00:07<00:03,  4.10it/s]\u001b[A\n","Epoch 36:  94% 176/188 [01:16<00:05,  2.31it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  74% 35/47 [00:07<00:02,  4.17it/s]\u001b[A\n","Epoch 36:  95% 178/188 [01:16<00:04,  2.32it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  79% 37/47 [00:08<00:02,  4.40it/s]\u001b[A\n","Epoch 36:  96% 180/188 [01:17<00:03,  2.33it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.13it/s]\u001b[A\n","Epoch 36:  97% 182/188 [01:17<00:02,  2.35it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.54it/s]\u001b[A\n","Epoch 36:  98% 184/188 [01:17<00:01,  2.36it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  91% 43/47 [00:09<00:00,  5.92it/s]\u001b[A\n","Epoch 36:  99% 186/188 [01:18<00:00,  2.38it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  96% 45/47 [00:09<00:00,  5.99it/s]\u001b[A\n","Epoch 36: 100% 188/188 [01:18<00:00,  2.39it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=584.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Epoch 36: 100% 188/188 [01:18<00:00,  2.39it/s, loss=572, v_num=0, val_loss_epoch=582.0, train_loss_step=526.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Epoch 37:  76% 142/188 [01:07<00:21,  2.11it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:12,  3.60it/s]\u001b[A\n","Epoch 37:  77% 144/188 [01:07<00:20,  2.13it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.31it/s]\u001b[A\n","Epoch 37:  78% 146/188 [01:07<00:19,  2.15it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.61it/s]\u001b[A\n","Epoch 37:  79% 148/188 [01:08<00:18,  2.17it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.84it/s]\u001b[A\n","Epoch 37:  80% 150/188 [01:08<00:17,  2.19it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.93it/s]\u001b[A\n","Epoch 37:  81% 152/188 [01:08<00:16,  2.20it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  23% 11/47 [00:01<00:06,  5.73it/s]\u001b[A\n","Epoch 37:  82% 154/188 [01:09<00:15,  2.22it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.87it/s]\u001b[A\n","Epoch 37:  83% 156/188 [01:09<00:14,  2.24it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.94it/s]\u001b[A\n","Epoch 37:  84% 158/188 [01:09<00:13,  2.26it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  36% 17/47 [00:02<00:05,  5.95it/s]\u001b[A\n","Epoch 37:  85% 160/188 [01:10<00:12,  2.27it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.82it/s]\u001b[A\n","Epoch 37:  86% 162/188 [01:10<00:11,  2.29it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  45% 21/47 [00:03<00:04,  5.72it/s]\u001b[A\n","Epoch 37:  87% 164/188 [01:11<00:10,  2.31it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.74it/s]\u001b[A\n","Epoch 37:  88% 166/188 [01:11<00:09,  2.33it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.87it/s]\u001b[A\n","Epoch 37:  89% 168/188 [01:11<00:08,  2.34it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.96it/s]\u001b[A\n","Epoch 37:  90% 170/188 [01:12<00:07,  2.36it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  62% 29/47 [00:04<00:02,  6.07it/s]\u001b[A\n","Epoch 37:  91% 172/188 [01:12<00:06,  2.38it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.96it/s]\u001b[A\n","Epoch 37:  93% 174/188 [01:12<00:05,  2.39it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.89it/s]\u001b[A\n","Epoch 37:  94% 176/188 [01:13<00:04,  2.41it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.06it/s]\u001b[A\n","Epoch 37:  95% 178/188 [01:13<00:04,  2.42it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  79% 37/47 [00:06<00:02,  4.80it/s]\u001b[A\n","Epoch 37:  96% 180/188 [01:13<00:03,  2.43it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  83% 39/47 [00:06<00:01,  4.70it/s]\u001b[A\n","Epoch 37:  97% 182/188 [01:14<00:02,  2.44it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  87% 41/47 [00:07<00:01,  4.44it/s]\u001b[A\n","Epoch 37:  98% 184/188 [01:14<00:01,  2.46it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  91% 43/47 [00:07<00:00,  4.42it/s]\u001b[A\n","Epoch 37:  99% 186/188 [01:15<00:00,  2.47it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.44it/s]\u001b[A\n","Epoch 37: 100% 188/188 [01:15<00:00,  2.48it/s, loss=584, v_num=0, val_loss_epoch=582.0, train_loss_step=561.0, train_loss_epoch=589.0, val_loss_step=589.0]\n","Epoch 37: 100% 188/188 [01:16<00:00,  2.47it/s, loss=584, v_num=0, val_loss_epoch=581.0, train_loss_step=429.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Epoch 38:  76% 142/188 [01:08<00:22,  2.06it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:12,  3.57it/s]\u001b[A\n","Epoch 38:  77% 144/188 [01:09<00:21,  2.07it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.22it/s]\u001b[A\n","Epoch 38:  78% 146/188 [01:09<00:20,  2.09it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.69it/s]\u001b[A\n","Epoch 38:  79% 148/188 [01:10<00:18,  2.11it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.82it/s]\u001b[A\n","Epoch 38:  80% 150/188 [01:10<00:17,  2.13it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.87it/s]\u001b[A\n","Epoch 38:  81% 152/188 [01:10<00:16,  2.15it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.05it/s]\u001b[A\n","Epoch 38:  82% 154/188 [01:11<00:15,  2.17it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.93it/s]\u001b[A\n","Epoch 38:  83% 156/188 [01:11<00:14,  2.18it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.01it/s]\u001b[A\n","Epoch 38:  84% 158/188 [01:11<00:13,  2.20it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.07it/s]\u001b[A\n","Epoch 38:  85% 160/188 [01:12<00:12,  2.22it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.09it/s]\u001b[A\n","Epoch 38:  86% 162/188 [01:12<00:11,  2.24it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.11it/s]\u001b[A\n","Epoch 38:  87% 164/188 [01:12<00:10,  2.25it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  49% 23/47 [00:03<00:03,  6.19it/s]\u001b[A\n","Epoch 38:  88% 166/188 [01:13<00:09,  2.27it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.10it/s]\u001b[A\n","Epoch 38:  89% 168/188 [01:13<00:08,  2.29it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  57% 27/47 [00:04<00:03,  6.05it/s]\u001b[A\n","Epoch 38:  90% 170/188 [01:13<00:07,  2.30it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  62% 29/47 [00:04<00:02,  6.18it/s]\u001b[A\n","Epoch 38:  91% 172/188 [01:14<00:06,  2.32it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.05it/s]\u001b[A\n","Epoch 38:  93% 174/188 [01:14<00:05,  2.34it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  70% 33/47 [00:05<00:02,  6.13it/s]\u001b[A\n","Epoch 38:  94% 176/188 [01:14<00:05,  2.35it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  74% 35/47 [00:05<00:01,  6.04it/s]\u001b[A\n","Epoch 38:  95% 178/188 [01:15<00:04,  2.37it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  79% 37/47 [00:06<00:01,  6.10it/s]\u001b[A\n","Epoch 38:  96% 180/188 [01:15<00:03,  2.39it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  83% 39/47 [00:06<00:01,  6.13it/s]\u001b[A\n","Epoch 38:  97% 182/188 [01:15<00:02,  2.40it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  87% 41/47 [00:06<00:00,  6.11it/s]\u001b[A\n","Epoch 38:  98% 184/188 [01:16<00:01,  2.42it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  91% 43/47 [00:07<00:00,  6.10it/s]\u001b[A\n","Epoch 38:  99% 186/188 [01:16<00:00,  2.44it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Validating:  96% 45/47 [00:07<00:00,  6.17it/s]\u001b[A\n","Epoch 38: 100% 188/188 [01:16<00:00,  2.45it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=618.0, train_loss_epoch=588.0, val_loss_step=589.0]\n","Epoch 38: 100% 188/188 [01:16<00:00,  2.44it/s, loss=583, v_num=0, val_loss_epoch=578.0, train_loss_step=548.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Epoch 39:  76% 142/188 [01:07<00:21,  2.11it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:20,  2.27it/s]\u001b[A\n","Epoch 39:  77% 144/188 [01:07<00:20,  2.12it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:   6% 3/47 [00:00<00:12,  3.65it/s]\u001b[A\n","Epoch 39:  78% 146/188 [01:08<00:19,  2.13it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  11% 5/47 [00:01<00:10,  3.96it/s]\u001b[A\n","Epoch 39:  79% 148/188 [01:08<00:18,  2.15it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  15% 7/47 [00:01<00:09,  4.04it/s]\u001b[A\n","Epoch 39:  80% 150/188 [01:09<00:17,  2.16it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  19% 9/47 [00:02<00:09,  4.02it/s]\u001b[A\n","Epoch 39:  81% 152/188 [01:09<00:16,  2.17it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  23% 11/47 [00:02<00:08,  4.00it/s]\u001b[A\n","Epoch 39:  82% 154/188 [01:10<00:15,  2.19it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  28% 13/47 [00:03<00:08,  4.07it/s]\u001b[A\n","Epoch 39:  83% 156/188 [01:10<00:14,  2.20it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  32% 15/47 [00:03<00:06,  4.96it/s]\u001b[A\n","Epoch 39:  84% 158/188 [01:11<00:13,  2.22it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  36% 17/47 [00:04<00:05,  5.51it/s]\u001b[A\n","Epoch 39:  85% 160/188 [01:11<00:12,  2.24it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  40% 19/47 [00:04<00:04,  5.75it/s]\u001b[A\n","Epoch 39:  86% 162/188 [01:11<00:11,  2.26it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  45% 21/47 [00:04<00:04,  6.03it/s]\u001b[A\n","Epoch 39:  87% 164/188 [01:12<00:10,  2.28it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  49% 23/47 [00:04<00:03,  6.06it/s]\u001b[A\n","Epoch 39:  88% 166/188 [01:12<00:09,  2.29it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  53% 25/47 [00:05<00:03,  5.96it/s]\u001b[A\n","Epoch 39:  89% 168/188 [01:12<00:08,  2.31it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  57% 27/47 [00:05<00:03,  6.07it/s]\u001b[A\n","Epoch 39:  90% 170/188 [01:13<00:07,  2.33it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  62% 29/47 [00:05<00:02,  6.06it/s]\u001b[A\n","Epoch 39:  91% 172/188 [01:13<00:06,  2.34it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.89it/s]\u001b[A\n","Epoch 39:  93% 174/188 [01:13<00:05,  2.36it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  70% 33/47 [00:06<00:02,  6.04it/s]\u001b[A\n","Epoch 39:  94% 176/188 [01:14<00:05,  2.38it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  74% 35/47 [00:07<00:01,  6.10it/s]\u001b[A\n","Epoch 39:  95% 178/188 [01:14<00:04,  2.39it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  79% 37/47 [00:07<00:01,  6.05it/s]\u001b[A\n","Epoch 39:  96% 180/188 [01:14<00:03,  2.41it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  83% 39/47 [00:07<00:01,  6.04it/s]\u001b[A\n","Epoch 39:  97% 182/188 [01:15<00:02,  2.42it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  87% 41/47 [00:07<00:00,  6.13it/s]\u001b[A\n","Epoch 39:  98% 184/188 [01:15<00:01,  2.44it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  91% 43/47 [00:08<00:00,  6.07it/s]\u001b[A\n","Epoch 39:  99% 186/188 [01:15<00:00,  2.46it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Validating:  96% 45/47 [00:08<00:00,  6.25it/s]\u001b[A\n","Epoch 39: 100% 188/188 [01:16<00:00,  2.47it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=606.0, train_loss_epoch=588.0, val_loss_step=586.0]\n","Epoch 39: 100% 188/188 [01:16<00:00,  2.47it/s, loss=594, v_num=0, val_loss_epoch=578.0, train_loss_step=522.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Epoch 40:  76% 142/188 [01:10<00:22,  2.01it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:18,  2.44it/s]\u001b[A\n","Epoch 40:  77% 144/188 [01:11<00:21,  2.02it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:   6% 3/47 [00:00<00:11,  3.75it/s]\u001b[A\n","Epoch 40:  78% 146/188 [01:11<00:20,  2.03it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  11% 5/47 [00:01<00:09,  4.21it/s]\u001b[A\n","Epoch 40:  79% 148/188 [01:12<00:19,  2.05it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  15% 7/47 [00:01<00:09,  4.39it/s]\u001b[A\n","Epoch 40:  80% 150/188 [01:12<00:18,  2.06it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  19% 9/47 [00:02<00:08,  4.50it/s]\u001b[A\n","Epoch 40:  81% 152/188 [01:13<00:17,  2.08it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  23% 11/47 [00:02<00:08,  4.49it/s]\u001b[A\n","Epoch 40:  82% 154/188 [01:13<00:16,  2.09it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  28% 13/47 [00:03<00:07,  4.46it/s]\u001b[A\n","Epoch 40:  83% 156/188 [01:14<00:15,  2.11it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.39it/s]\u001b[A\n","Epoch 40:  84% 158/188 [01:14<00:14,  2.12it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.37it/s]\u001b[A\n","Epoch 40:  85% 160/188 [01:14<00:13,  2.13it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  40% 19/47 [00:04<00:06,  4.37it/s]\u001b[A\n","Epoch 40:  86% 162/188 [01:15<00:12,  2.15it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  45% 21/47 [00:04<00:06,  4.24it/s]\u001b[A\n","Epoch 40:  87% 164/188 [01:15<00:11,  2.16it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  49% 23/47 [00:05<00:05,  4.28it/s]\u001b[A\n","Epoch 40:  88% 166/188 [01:16<00:10,  2.18it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  53% 25/47 [00:05<00:04,  4.93it/s]\u001b[A\n","Epoch 40:  89% 168/188 [01:16<00:09,  2.19it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  57% 27/47 [00:06<00:03,  5.44it/s]\u001b[A\n","Epoch 40:  90% 170/188 [01:16<00:08,  2.21it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  62% 29/47 [00:06<00:03,  5.76it/s]\u001b[A\n","Epoch 40:  91% 172/188 [01:17<00:07,  2.22it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.84it/s]\u001b[A\n","Epoch 40:  93% 174/188 [01:17<00:06,  2.24it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  70% 33/47 [00:07<00:02,  6.11it/s]\u001b[A\n","Epoch 40:  94% 176/188 [01:17<00:05,  2.26it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.81it/s]\u001b[A\n","Epoch 40:  95% 178/188 [01:18<00:04,  2.27it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.40it/s]\u001b[A\n","Epoch 40:  96% 180/188 [01:18<00:03,  2.29it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.22it/s]\u001b[A\n","Epoch 40:  97% 182/188 [01:19<00:02,  2.30it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.15it/s]\u001b[A\n","Epoch 40:  98% 184/188 [01:19<00:01,  2.31it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  91% 43/47 [00:08<00:00,  5.13it/s]\u001b[A\n","Epoch 40:  99% 186/188 [01:19<00:00,  2.33it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Validating:  96% 45/47 [00:09<00:00,  5.28it/s]\u001b[A\n","Epoch 40: 100% 188/188 [01:20<00:00,  2.34it/s, loss=585, v_num=0, val_loss_epoch=578.0, train_loss_step=577.0, train_loss_epoch=587.0, val_loss_step=584.0]\n","Epoch 40: 100% 188/188 [01:20<00:00,  2.34it/s, loss=585, v_num=0, val_loss_epoch=579.0, train_loss_step=564.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Epoch 41:  76% 142/188 [01:07<00:21,  2.11it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.44it/s]\u001b[A\n","Epoch 41:  77% 144/188 [01:07<00:20,  2.12it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.22it/s]\u001b[A\n","Epoch 41:  78% 146/188 [01:08<00:19,  2.14it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  11% 5/47 [00:00<00:08,  5.20it/s]\u001b[A\n","Epoch 41:  79% 148/188 [01:08<00:18,  2.16it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.20it/s]\u001b[A\n","Epoch 41:  80% 150/188 [01:08<00:17,  2.17it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.29it/s]\u001b[A\n","Epoch 41:  81% 152/188 [01:09<00:16,  2.19it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.25it/s]\u001b[A\n","Epoch 41:  82% 154/188 [01:09<00:15,  2.21it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.23it/s]\u001b[A\n","Epoch 41:  83% 156/188 [01:10<00:14,  2.23it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.35it/s]\u001b[A\n","Epoch 41:  84% 158/188 [01:10<00:13,  2.24it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.85it/s]\u001b[A\n","Epoch 41:  85% 160/188 [01:10<00:12,  2.25it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  40% 19/47 [00:03<00:05,  4.70it/s]\u001b[A\n","Epoch 41:  86% 162/188 [01:11<00:11,  2.27it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  45% 21/47 [00:04<00:05,  4.60it/s]\u001b[A\n","Epoch 41:  87% 164/188 [01:11<00:10,  2.28it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  49% 23/47 [00:04<00:05,  4.59it/s]\u001b[A\n","Epoch 41:  88% 166/188 [01:12<00:09,  2.30it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  53% 25/47 [00:05<00:04,  4.59it/s]\u001b[A\n","Epoch 41:  89% 168/188 [01:12<00:08,  2.31it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  57% 27/47 [00:05<00:04,  4.62it/s]\u001b[A\n","Epoch 41:  90% 170/188 [01:13<00:07,  2.32it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  62% 29/47 [00:05<00:03,  4.63it/s]\u001b[A\n","Epoch 41:  91% 172/188 [01:13<00:06,  2.34it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.53it/s]\u001b[A\n","Epoch 41:  93% 174/188 [01:14<00:05,  2.35it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  70% 33/47 [00:06<00:03,  4.35it/s]\u001b[A\n","Epoch 41:  94% 176/188 [01:14<00:05,  2.36it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  74% 35/47 [00:07<00:02,  4.34it/s]\u001b[A\n","Epoch 41:  95% 178/188 [01:14<00:04,  2.37it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  79% 37/47 [00:07<00:02,  4.34it/s]\u001b[A\n","Epoch 41:  96% 180/188 [01:15<00:03,  2.39it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  83% 39/47 [00:08<00:01,  4.30it/s]\u001b[A\n","Epoch 41:  97% 182/188 [01:15<00:02,  2.40it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  87% 41/47 [00:08<00:01,  4.10it/s]\u001b[A\n","Epoch 41:  98% 184/188 [01:16<00:01,  2.41it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  91% 43/47 [00:09<00:00,  4.23it/s]\u001b[A\n","Epoch 41:  99% 186/188 [01:16<00:00,  2.42it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Validating:  96% 45/47 [00:09<00:00,  4.67it/s]\u001b[A\n","Epoch 41: 100% 188/188 [01:17<00:00,  2.43it/s, loss=582, v_num=0, val_loss_epoch=579.0, train_loss_step=604.0, train_loss_epoch=587.0, val_loss_step=592.0]\n","Epoch 41: 100% 188/188 [01:17<00:00,  2.43it/s, loss=582, v_num=0, val_loss_epoch=576.0, train_loss_step=504.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Epoch 42:  76% 142/188 [01:10<00:22,  2.00it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.76it/s]\u001b[A\n","Epoch 42:  77% 144/188 [01:11<00:21,  2.01it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.36it/s]\u001b[A\n","Epoch 42:  78% 146/188 [01:11<00:20,  2.03it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  11% 5/47 [00:01<00:07,  5.30it/s]\u001b[A\n","Epoch 42:  79% 148/188 [01:12<00:19,  2.05it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.62it/s]\u001b[A\n","Epoch 42:  80% 150/188 [01:12<00:18,  2.07it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.86it/s]\u001b[A\n","Epoch 42:  81% 152/188 [01:12<00:17,  2.09it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  23% 11/47 [00:02<00:05,  6.03it/s]\u001b[A\n","Epoch 42:  82% 154/188 [01:13<00:16,  2.10it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.98it/s]\u001b[A\n","Epoch 42:  83% 156/188 [01:13<00:15,  2.12it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.08it/s]\u001b[A\n","Epoch 42:  84% 158/188 [01:13<00:14,  2.14it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.65it/s]\u001b[A\n","Epoch 42:  85% 160/188 [01:14<00:12,  2.15it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  40% 19/47 [00:03<00:05,  5.10it/s]\u001b[A\n","Epoch 42:  86% 162/188 [01:14<00:11,  2.17it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  45% 21/47 [00:03<00:05,  5.14it/s]\u001b[A\n","Epoch 42:  87% 164/188 [01:15<00:10,  2.19it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.13it/s]\u001b[A\n","Epoch 42:  88% 166/188 [01:15<00:09,  2.20it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.10it/s]\u001b[A\n","Epoch 42:  89% 168/188 [01:15<00:09,  2.22it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.37it/s]\u001b[A\n","Epoch 42:  90% 170/188 [01:16<00:08,  2.23it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.57it/s]\u001b[A\n","Epoch 42:  91% 172/188 [01:16<00:07,  2.25it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  66% 31/47 [00:05<00:03,  5.18it/s]\u001b[A\n","Epoch 42:  93% 174/188 [01:16<00:06,  2.26it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  70% 33/47 [00:06<00:02,  4.70it/s]\u001b[A\n","Epoch 42:  94% 176/188 [01:17<00:05,  2.27it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  74% 35/47 [00:06<00:02,  4.56it/s]\u001b[A\n","Epoch 42:  95% 178/188 [01:17<00:04,  2.29it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  79% 37/47 [00:07<00:02,  4.49it/s]\u001b[A\n","Epoch 42:  96% 180/188 [01:18<00:03,  2.30it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  83% 39/47 [00:07<00:01,  4.44it/s]\u001b[A\n","Epoch 42:  97% 182/188 [01:18<00:02,  2.31it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  87% 41/47 [00:08<00:01,  4.47it/s]\u001b[A\n","Epoch 42:  98% 184/188 [01:19<00:01,  2.32it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.56it/s]\u001b[A\n","Epoch 42:  99% 186/188 [01:19<00:00,  2.34it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.38it/s]\u001b[A\n","Epoch 42: 100% 188/188 [01:20<00:00,  2.35it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=565.0, train_loss_epoch=585.0, val_loss_step=590.0]\n","Epoch 42: 100% 188/188 [01:20<00:00,  2.34it/s, loss=586, v_num=0, val_loss_epoch=578.0, train_loss_step=582.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Epoch 43:  76% 142/188 [01:08<00:22,  2.08it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.34it/s]\u001b[A\n","Epoch 43:  77% 144/188 [01:08<00:21,  2.09it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.17it/s]\u001b[A\n","Epoch 43:  78% 146/188 [01:09<00:19,  2.11it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.64it/s]\u001b[A\n","Epoch 43:  79% 148/188 [01:09<00:18,  2.13it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.78it/s]\u001b[A\n","Epoch 43:  80% 150/188 [01:09<00:17,  2.15it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.95it/s]\u001b[A\n","Epoch 43:  81% 152/188 [01:10<00:16,  2.17it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.07it/s]\u001b[A\n","Epoch 43:  82% 154/188 [01:10<00:15,  2.19it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  28% 13/47 [00:02<00:05,  5.92it/s]\u001b[A\n","Epoch 43:  83% 156/188 [01:10<00:14,  2.20it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.06it/s]\u001b[A\n","Epoch 43:  84% 158/188 [01:11<00:13,  2.22it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.06it/s]\u001b[A\n","Epoch 43:  85% 160/188 [01:11<00:12,  2.24it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.05it/s]\u001b[A\n","Epoch 43:  86% 162/188 [01:11<00:11,  2.26it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.12it/s]\u001b[A\n","Epoch 43:  87% 164/188 [01:12<00:10,  2.28it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  49% 23/47 [00:03<00:04,  5.90it/s]\u001b[A\n","Epoch 43:  88% 166/188 [01:12<00:09,  2.29it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.87it/s]\u001b[A\n","Epoch 43:  89% 168/188 [01:12<00:08,  2.31it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  57% 27/47 [00:04<00:03,  6.07it/s]\u001b[A\n","Epoch 43:  90% 170/188 [01:13<00:07,  2.33it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  62% 29/47 [00:04<00:02,  6.17it/s]\u001b[A\n","Epoch 43:  91% 172/188 [01:13<00:06,  2.34it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.09it/s]\u001b[A\n","Epoch 43:  93% 174/188 [01:13<00:05,  2.36it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  70% 33/47 [00:05<00:02,  5.86it/s]\u001b[A\n","Epoch 43:  94% 176/188 [01:14<00:05,  2.38it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  74% 35/47 [00:05<00:01,  6.05it/s]\u001b[A\n","Epoch 43:  95% 178/188 [01:14<00:04,  2.39it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  79% 37/47 [00:06<00:01,  6.05it/s]\u001b[A\n","Epoch 43:  96% 180/188 [01:14<00:03,  2.41it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  83% 39/47 [00:06<00:01,  5.99it/s]\u001b[A\n","Epoch 43:  97% 182/188 [01:15<00:02,  2.42it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  87% 41/47 [00:06<00:00,  6.09it/s]\u001b[A\n","Epoch 43:  98% 184/188 [01:15<00:01,  2.44it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  91% 43/47 [00:07<00:00,  6.11it/s]\u001b[A\n","Epoch 43:  99% 186/188 [01:15<00:00,  2.46it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Validating:  96% 45/47 [00:07<00:00,  6.04it/s]\u001b[A\n","Epoch 43: 100% 188/188 [01:16<00:00,  2.47it/s, loss=591, v_num=0, val_loss_epoch=578.0, train_loss_step=610.0, train_loss_epoch=585.0, val_loss_step=582.0]\n","Epoch 43: 100% 188/188 [01:16<00:00,  2.47it/s, loss=591, v_num=0, val_loss_epoch=577.0, train_loss_step=521.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Epoch 44:  76% 142/188 [01:09<00:22,  2.05it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:18,  2.44it/s]\u001b[A\n","Epoch 44:  77% 144/188 [01:09<00:21,  2.06it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:   6% 3/47 [00:00<00:12,  3.57it/s]\u001b[A\n","Epoch 44:  78% 146/188 [01:10<00:20,  2.07it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  11% 5/47 [00:01<00:10,  3.88it/s]\u001b[A\n","Epoch 44:  79% 148/188 [01:10<00:19,  2.09it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  15% 7/47 [00:01<00:09,  4.03it/s]\u001b[A\n","Epoch 44:  80% 150/188 [01:11<00:18,  2.10it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  19% 9/47 [00:02<00:08,  4.42it/s]\u001b[A\n","Epoch 44:  81% 152/188 [01:11<00:16,  2.12it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.19it/s]\u001b[A\n","Epoch 44:  82% 154/188 [01:12<00:15,  2.14it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.58it/s]\u001b[A\n","Epoch 44:  83% 156/188 [01:12<00:14,  2.15it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  32% 15/47 [00:03<00:05,  5.58it/s]\u001b[A\n","Epoch 44:  84% 158/188 [01:12<00:13,  2.17it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.79it/s]\u001b[A\n","Epoch 44:  85% 160/188 [01:13<00:12,  2.19it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.79it/s]\u001b[A\n","Epoch 44:  86% 162/188 [01:13<00:11,  2.21it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.79it/s]\u001b[A\n","Epoch 44:  87% 164/188 [01:13<00:10,  2.22it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.84it/s]\u001b[A\n","Epoch 44:  88% 166/188 [01:14<00:09,  2.24it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.93it/s]\u001b[A\n","Epoch 44:  89% 168/188 [01:14<00:08,  2.26it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  57% 27/47 [00:05<00:03,  6.02it/s]\u001b[A\n","Epoch 44:  90% 170/188 [01:14<00:07,  2.27it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.91it/s]\u001b[A\n","Epoch 44:  91% 172/188 [01:15<00:06,  2.29it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  66% 31/47 [00:05<00:02,  5.92it/s]\u001b[A\n","Epoch 44:  93% 174/188 [01:15<00:06,  2.31it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  70% 33/47 [00:06<00:02,  6.01it/s]\u001b[A\n","Epoch 44:  94% 176/188 [01:15<00:05,  2.32it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  74% 35/47 [00:06<00:01,  6.18it/s]\u001b[A\n","Epoch 44:  95% 178/188 [01:16<00:04,  2.34it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  79% 37/47 [00:06<00:01,  6.08it/s]\u001b[A\n","Epoch 44:  96% 180/188 [01:16<00:03,  2.36it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  83% 39/47 [00:07<00:01,  6.03it/s]\u001b[A\n","Epoch 44:  97% 182/188 [01:16<00:02,  2.37it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  87% 41/47 [00:07<00:00,  6.11it/s]\u001b[A\n","Epoch 44:  98% 184/188 [01:17<00:01,  2.39it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  91% 43/47 [00:07<00:00,  6.15it/s]\u001b[A\n","Epoch 44:  99% 186/188 [01:17<00:00,  2.40it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.99it/s]\u001b[A\n","Epoch 44: 100% 188/188 [01:17<00:00,  2.42it/s, loss=582, v_num=0, val_loss_epoch=577.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=582.0]\n","Epoch 44: 100% 188/188 [01:17<00:00,  2.41it/s, loss=582, v_num=0, val_loss_epoch=574.0, train_loss_step=561.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Epoch 45:  76% 142/188 [01:06<00:21,  2.13it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.35it/s]\u001b[A\n","Epoch 45:  77% 144/188 [01:07<00:20,  2.15it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.00it/s]\u001b[A\n","Epoch 45:  78% 146/188 [01:07<00:19,  2.17it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.76it/s]\u001b[A\n","Epoch 45:  79% 148/188 [01:07<00:18,  2.19it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.86it/s]\u001b[A\n","Epoch 45:  80% 150/188 [01:08<00:17,  2.21it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.84it/s]\u001b[A\n","Epoch 45:  81% 152/188 [01:08<00:16,  2.22it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.80it/s]\u001b[A\n","Epoch 45:  82% 154/188 [01:08<00:15,  2.24it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  28% 13/47 [00:02<00:07,  4.58it/s]\u001b[A\n","Epoch 45:  83% 156/188 [01:09<00:14,  2.25it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.57it/s]\u001b[A\n","Epoch 45:  84% 158/188 [01:09<00:13,  2.26it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.48it/s]\u001b[A\n","Epoch 45:  85% 160/188 [01:10<00:12,  2.28it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  40% 19/47 [00:03<00:06,  4.37it/s]\u001b[A\n","Epoch 45:  86% 162/188 [01:10<00:11,  2.29it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  45% 21/47 [00:04<00:05,  4.51it/s]\u001b[A\n","Epoch 45:  87% 164/188 [01:11<00:10,  2.31it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  49% 23/47 [00:04<00:05,  4.60it/s]\u001b[A\n","Epoch 45:  88% 166/188 [01:11<00:09,  2.32it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  53% 25/47 [00:05<00:05,  4.34it/s]\u001b[A\n","Epoch 45:  89% 168/188 [01:12<00:08,  2.33it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  57% 27/47 [00:05<00:04,  4.41it/s]\u001b[A\n","Epoch 45:  90% 170/188 [01:12<00:07,  2.34it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  62% 29/47 [00:06<00:04,  4.43it/s]\u001b[A\n","Epoch 45:  91% 172/188 [01:12<00:06,  2.36it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.40it/s]\u001b[A\n","Epoch 45:  93% 174/188 [01:13<00:05,  2.37it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  70% 33/47 [00:07<00:03,  4.33it/s]\u001b[A\n","Epoch 45:  94% 176/188 [01:13<00:05,  2.38it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  74% 35/47 [00:07<00:02,  4.30it/s]\u001b[A\n","Epoch 45:  95% 178/188 [01:14<00:04,  2.39it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  79% 37/47 [00:08<00:02,  4.37it/s]\u001b[A\n","Epoch 45:  96% 180/188 [01:14<00:03,  2.41it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.03it/s]\u001b[A\n","Epoch 45:  97% 182/188 [01:15<00:02,  2.42it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.44it/s]\u001b[A\n","Epoch 45:  98% 184/188 [01:15<00:01,  2.44it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  91% 43/47 [00:09<00:00,  5.82it/s]\u001b[A\n","Epoch 45:  99% 186/188 [01:15<00:00,  2.46it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Validating:  96% 45/47 [00:09<00:00,  6.05it/s]\u001b[A\n","Epoch 45: 100% 188/188 [01:16<00:00,  2.47it/s, loss=607, v_num=0, val_loss_epoch=574.0, train_loss_step=646.0, train_loss_epoch=584.0, val_loss_step=591.0]\n","Epoch 45: 100% 188/188 [01:16<00:00,  2.46it/s, loss=607, v_num=0, val_loss_epoch=576.0, train_loss_step=567.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Epoch 46:  76% 142/188 [01:08<00:22,  2.07it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:20,  2.28it/s]\u001b[A\n","Epoch 46:  77% 144/188 [01:09<00:21,  2.08it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:   6% 3/47 [00:00<00:12,  3.56it/s]\u001b[A\n","Epoch 46:  78% 146/188 [01:09<00:20,  2.10it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.78it/s]\u001b[A\n","Epoch 46:  79% 148/188 [01:09<00:18,  2.12it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.32it/s]\u001b[A\n","Epoch 46:  80% 150/188 [01:10<00:17,  2.14it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.74it/s]\u001b[A\n","Epoch 46:  81% 152/188 [01:10<00:16,  2.16it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.98it/s]\u001b[A\n","Epoch 46:  82% 154/188 [01:10<00:15,  2.17it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.02it/s]\u001b[A\n","Epoch 46:  83% 156/188 [01:11<00:14,  2.19it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.08it/s]\u001b[A\n","Epoch 46:  84% 158/188 [01:11<00:13,  2.21it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  36% 17/47 [00:03<00:04,  6.19it/s]\u001b[A\n","Epoch 46:  85% 160/188 [01:11<00:12,  2.23it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.12it/s]\u001b[A\n","Epoch 46:  86% 162/188 [01:12<00:11,  2.25it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.22it/s]\u001b[A\n","Epoch 46:  87% 164/188 [01:12<00:10,  2.26it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  49% 23/47 [00:04<00:03,  6.26it/s]\u001b[A\n","Epoch 46:  88% 166/188 [01:12<00:09,  2.28it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.66it/s]\u001b[A\n","Epoch 46:  89% 168/188 [01:13<00:08,  2.29it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  57% 27/47 [00:04<00:04,  4.99it/s]\u001b[A\n","Epoch 46:  90% 170/188 [01:13<00:07,  2.31it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  62% 29/47 [00:05<00:03,  4.73it/s]\u001b[A\n","Epoch 46:  91% 172/188 [01:14<00:06,  2.32it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  66% 31/47 [00:05<00:03,  4.66it/s]\u001b[A\n","Epoch 46:  93% 174/188 [01:14<00:05,  2.33it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  70% 33/47 [00:06<00:03,  4.60it/s]\u001b[A\n","Epoch 46:  94% 176/188 [01:14<00:05,  2.35it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  74% 35/47 [00:06<00:02,  4.68it/s]\u001b[A\n","Epoch 46:  95% 178/188 [01:15<00:04,  2.36it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  79% 37/47 [00:07<00:02,  4.71it/s]\u001b[A\n","Epoch 46:  96% 180/188 [01:15<00:03,  2.37it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  83% 39/47 [00:07<00:01,  4.73it/s]\u001b[A\n","Epoch 46:  97% 182/188 [01:16<00:02,  2.39it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  87% 41/47 [00:07<00:01,  4.63it/s]\u001b[A\n","Epoch 46:  98% 184/188 [01:16<00:01,  2.40it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.64it/s]\u001b[A\n","Epoch 46:  99% 186/188 [01:17<00:00,  2.41it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.64it/s]\u001b[A\n","Epoch 46: 100% 188/188 [01:17<00:00,  2.42it/s, loss=586, v_num=0, val_loss_epoch=576.0, train_loss_step=670.0, train_loss_epoch=584.0, val_loss_step=584.0]\n","Epoch 46: 100% 188/188 [01:17<00:00,  2.42it/s, loss=586, v_num=0, val_loss_epoch=573.0, train_loss_step=491.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Epoch 47:  76% 142/188 [01:06<00:21,  2.14it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:15,  3.06it/s]\u001b[A\n","Epoch 47:  77% 144/188 [01:06<00:20,  2.16it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:   6% 3/47 [00:00<00:08,  4.97it/s]\u001b[A\n","Epoch 47:  78% 146/188 [01:07<00:19,  2.18it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.66it/s]\u001b[A\n","Epoch 47:  79% 148/188 [01:07<00:18,  2.20it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.93it/s]\u001b[A\n","Epoch 47:  80% 150/188 [01:07<00:17,  2.21it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  19% 9/47 [00:01<00:06,  6.02it/s]\u001b[A\n","Epoch 47:  81% 152/188 [01:08<00:16,  2.23it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.08it/s]\u001b[A\n","Epoch 47:  82% 154/188 [01:08<00:15,  2.25it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.13it/s]\u001b[A\n","Epoch 47:  83% 156/188 [01:08<00:14,  2.27it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.25it/s]\u001b[A\n","Epoch 47:  84% 158/188 [01:09<00:13,  2.29it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.14it/s]\u001b[A\n","Epoch 47:  85% 160/188 [01:09<00:12,  2.31it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.07it/s]\u001b[A\n","Epoch 47:  86% 162/188 [01:09<00:11,  2.32it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.18it/s]\u001b[A\n","Epoch 47:  87% 164/188 [01:10<00:10,  2.34it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  49% 23/47 [00:03<00:03,  6.13it/s]\u001b[A\n","Epoch 47:  88% 166/188 [01:10<00:09,  2.36it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.01it/s]\u001b[A\n","Epoch 47:  89% 168/188 [01:10<00:08,  2.38it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  57% 27/47 [00:04<00:03,  6.04it/s]\u001b[A\n","Epoch 47:  90% 170/188 [01:10<00:07,  2.39it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  62% 29/47 [00:04<00:02,  6.11it/s]\u001b[A\n","Epoch 47:  91% 172/188 [01:11<00:06,  2.41it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.15it/s]\u001b[A\n","Epoch 47:  93% 174/188 [01:11<00:05,  2.43it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  70% 33/47 [00:05<00:02,  6.29it/s]\u001b[A\n","Epoch 47:  94% 176/188 [01:11<00:04,  2.45it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  74% 35/47 [00:05<00:01,  6.19it/s]\u001b[A\n","Epoch 47:  95% 178/188 [01:12<00:04,  2.46it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.96it/s]\u001b[A\n","Epoch 47:  96% 180/188 [01:12<00:03,  2.48it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  83% 39/47 [00:06<00:01,  6.04it/s]\u001b[A\n","Epoch 47:  97% 182/188 [01:12<00:02,  2.49it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  87% 41/47 [00:06<00:01,  5.96it/s]\u001b[A\n","Epoch 47:  98% 184/188 [01:13<00:01,  2.51it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  91% 43/47 [00:07<00:00,  6.00it/s]\u001b[A\n","Epoch 47:  99% 186/188 [01:13<00:00,  2.53it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  96% 45/47 [00:07<00:00,  6.16it/s]\u001b[A\n","Epoch 47: 100% 188/188 [01:13<00:00,  2.54it/s, loss=584, v_num=0, val_loss_epoch=573.0, train_loss_step=577.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Epoch 47: 100% 188/188 [01:14<00:00,  2.54it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=506.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Epoch 48:  76% 142/188 [01:05<00:21,  2.16it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:12,  3.60it/s]\u001b[A\n","Epoch 48:  77% 144/188 [01:06<00:20,  2.17it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.24it/s]\u001b[A\n","Epoch 48:  78% 146/188 [01:06<00:19,  2.19it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.66it/s]\u001b[A\n","Epoch 48:  79% 148/188 [01:06<00:18,  2.21it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.37it/s]\u001b[A\n","Epoch 48:  80% 150/188 [01:07<00:17,  2.22it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  19% 9/47 [00:01<00:07,  4.92it/s]\u001b[A\n","Epoch 48:  81% 152/188 [01:07<00:16,  2.24it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.61it/s]\u001b[A\n","Epoch 48:  82% 154/188 [01:08<00:15,  2.25it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  28% 13/47 [00:02<00:07,  4.60it/s]\u001b[A\n","Epoch 48:  83% 156/188 [01:08<00:14,  2.27it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.37it/s]\u001b[A\n","Epoch 48:  84% 158/188 [01:09<00:13,  2.28it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.35it/s]\u001b[A\n","Epoch 48:  85% 160/188 [01:09<00:12,  2.30it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  40% 19/47 [00:04<00:06,  4.24it/s]\u001b[A\n","Epoch 48:  86% 162/188 [01:10<00:11,  2.31it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  45% 21/47 [00:04<00:06,  4.22it/s]\u001b[A\n","Epoch 48:  87% 164/188 [01:10<00:10,  2.32it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  49% 23/47 [00:05<00:05,  4.16it/s]\u001b[A\n","Epoch 48:  88% 166/188 [01:11<00:09,  2.33it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  53% 25/47 [00:05<00:05,  4.23it/s]\u001b[A\n","Epoch 48:  89% 168/188 [01:11<00:08,  2.34it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  57% 27/47 [00:06<00:04,  4.04it/s]\u001b[A\n","Epoch 48:  90% 170/188 [01:12<00:07,  2.36it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  62% 29/47 [00:06<00:04,  3.91it/s]\u001b[A\n","Epoch 48:  91% 172/188 [01:12<00:06,  2.37it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  66% 31/47 [00:07<00:04,  3.84it/s]\u001b[A\n","Epoch 48:  93% 174/188 [01:13<00:05,  2.38it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  70% 33/47 [00:07<00:03,  3.56it/s]\u001b[A\n","Epoch 48:  94% 176/188 [01:13<00:05,  2.38it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  74% 35/47 [00:08<00:03,  3.48it/s]\u001b[A\n","Epoch 48:  95% 178/188 [01:14<00:04,  2.39it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  79% 37/47 [00:08<00:02,  3.70it/s]\u001b[A\n","Epoch 48:  96% 180/188 [01:14<00:03,  2.40it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  83% 39/47 [00:09<00:02,  3.74it/s]\u001b[A\n","Epoch 48:  97% 182/188 [01:15<00:02,  2.41it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  87% 41/47 [00:09<00:01,  3.65it/s]\u001b[A\n","Epoch 48:  98% 184/188 [01:15<00:01,  2.42it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  91% 43/47 [00:10<00:00,  4.02it/s]\u001b[A\n","Epoch 48:  99% 186/188 [01:16<00:00,  2.43it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Validating:  96% 45/47 [00:10<00:00,  4.17it/s]\u001b[A\n","Epoch 48: 100% 188/188 [01:16<00:00,  2.44it/s, loss=584, v_num=0, val_loss_epoch=571.0, train_loss_step=597.0, train_loss_epoch=581.0, val_loss_step=583.0]\n","Epoch 48: 100% 188/188 [01:17<00:00,  2.44it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=585.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Epoch 49:  76% 142/188 [01:06<00:21,  2.13it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.52it/s]\u001b[A\n","Epoch 49:  77% 144/188 [01:06<00:20,  2.15it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.15it/s]\u001b[A\n","Epoch 49:  78% 146/188 [01:07<00:19,  2.17it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.49it/s]\u001b[A\n","Epoch 49:  79% 148/188 [01:07<00:18,  2.19it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.86it/s]\u001b[A\n","Epoch 49:  80% 150/188 [01:07<00:17,  2.21it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.87it/s]\u001b[A\n","Epoch 49:  81% 152/188 [01:08<00:16,  2.22it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  23% 11/47 [00:01<00:06,  5.90it/s]\u001b[A\n","Epoch 49:  82% 154/188 [01:08<00:15,  2.24it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.06it/s]\u001b[A\n","Epoch 49:  83% 156/188 [01:08<00:14,  2.26it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.09it/s]\u001b[A\n","Epoch 49:  84% 158/188 [01:09<00:13,  2.28it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.13it/s]\u001b[A\n","Epoch 49:  85% 160/188 [01:09<00:12,  2.30it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.10it/s]\u001b[A\n","Epoch 49:  86% 162/188 [01:09<00:11,  2.32it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.25it/s]\u001b[A\n","Epoch 49:  87% 164/188 [01:10<00:10,  2.33it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  49% 23/47 [00:03<00:03,  6.07it/s]\u001b[A\n","Epoch 49:  88% 166/188 [01:10<00:09,  2.35it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.06it/s]\u001b[A\n","Epoch 49:  89% 168/188 [01:10<00:08,  2.37it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  57% 27/47 [00:04<00:03,  6.20it/s]\u001b[A\n","Epoch 49:  90% 170/188 [01:11<00:07,  2.39it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  62% 29/47 [00:04<00:02,  6.21it/s]\u001b[A\n","Epoch 49:  91% 172/188 [01:11<00:06,  2.40it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.17it/s]\u001b[A\n","Epoch 49:  93% 174/188 [01:11<00:05,  2.42it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  70% 33/47 [00:05<00:02,  6.07it/s]\u001b[A\n","Epoch 49:  94% 176/188 [01:12<00:04,  2.44it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  74% 35/47 [00:05<00:01,  6.12it/s]\u001b[A\n","Epoch 49:  95% 178/188 [01:12<00:04,  2.45it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  79% 37/47 [00:06<00:01,  6.11it/s]\u001b[A\n","Epoch 49:  96% 180/188 [01:12<00:03,  2.47it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  83% 39/47 [00:06<00:01,  6.19it/s]\u001b[A\n","Epoch 49:  97% 182/188 [01:13<00:02,  2.49it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  87% 41/47 [00:06<00:01,  5.75it/s]\u001b[A\n","Epoch 49:  98% 184/188 [01:13<00:01,  2.50it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  91% 43/47 [00:07<00:00,  4.91it/s]\u001b[A\n","Epoch 49:  99% 186/188 [01:14<00:00,  2.51it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Validating:  96% 45/47 [00:07<00:00,  4.69it/s]\u001b[A\n","Epoch 49: 100% 188/188 [01:14<00:00,  2.52it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=528.0, train_loss_epoch=581.0, val_loss_step=584.0]\n","Epoch 49: 100% 188/188 [01:14<00:00,  2.51it/s, loss=584, v_num=0, val_loss_epoch=569.0, train_loss_step=493.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Epoch 50:  76% 142/188 [01:06<00:21,  2.14it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:18,  2.48it/s]\u001b[A\n","Epoch 50:  77% 144/188 [01:07<00:20,  2.14it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:   6% 3/47 [00:00<00:12,  3.61it/s]\u001b[A\n","Epoch 50:  78% 146/188 [01:07<00:19,  2.16it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  11% 5/47 [00:01<00:11,  3.77it/s]\u001b[A\n","Epoch 50:  79% 148/188 [01:08<00:18,  2.17it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  15% 7/47 [00:01<00:10,  3.93it/s]\u001b[A\n","Epoch 50:  80% 150/188 [01:08<00:17,  2.19it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  19% 9/47 [00:02<00:09,  4.17it/s]\u001b[A\n","Epoch 50:  81% 152/188 [01:08<00:16,  2.20it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.88it/s]\u001b[A\n","Epoch 50:  82% 154/188 [01:09<00:15,  2.22it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.45it/s]\u001b[A\n","Epoch 50:  83% 156/188 [01:09<00:14,  2.24it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  32% 15/47 [00:03<00:05,  5.72it/s]\u001b[A\n","Epoch 50:  84% 158/188 [01:09<00:13,  2.26it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.85it/s]\u001b[A\n","Epoch 50:  85% 160/188 [01:10<00:12,  2.27it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.01it/s]\u001b[A\n","Epoch 50:  86% 162/188 [01:10<00:11,  2.29it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  45% 21/47 [00:04<00:04,  6.14it/s]\u001b[A\n","Epoch 50:  87% 164/188 [01:10<00:10,  2.31it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  49% 23/47 [00:04<00:03,  6.20it/s]\u001b[A\n","Epoch 50:  88% 166/188 [01:11<00:09,  2.33it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  53% 25/47 [00:04<00:03,  6.14it/s]\u001b[A\n","Epoch 50:  89% 168/188 [01:11<00:08,  2.35it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  57% 27/47 [00:05<00:03,  6.18it/s]\u001b[A\n","Epoch 50:  90% 170/188 [01:11<00:07,  2.36it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  62% 29/47 [00:05<00:02,  6.17it/s]\u001b[A\n","Epoch 50:  91% 172/188 [01:12<00:06,  2.38it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.00it/s]\u001b[A\n","Epoch 50:  93% 174/188 [01:12<00:05,  2.40it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  70% 33/47 [00:06<00:02,  6.06it/s]\u001b[A\n","Epoch 50:  94% 176/188 [01:12<00:04,  2.41it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  74% 35/47 [00:06<00:01,  6.11it/s]\u001b[A\n","Epoch 50:  95% 178/188 [01:13<00:04,  2.43it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  79% 37/47 [00:06<00:01,  5.99it/s]\u001b[A\n","Epoch 50:  96% 180/188 [01:13<00:03,  2.44it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  83% 39/47 [00:07<00:01,  6.00it/s]\u001b[A\n","Epoch 50:  97% 182/188 [01:13<00:02,  2.46it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  87% 41/47 [00:07<00:00,  6.07it/s]\u001b[A\n","Epoch 50:  98% 184/188 [01:14<00:01,  2.48it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  91% 43/47 [00:07<00:00,  6.10it/s]\u001b[A\n","Epoch 50:  99% 186/188 [01:14<00:00,  2.49it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Validating:  96% 45/47 [00:08<00:00,  6.21it/s]\u001b[A\n","Epoch 50: 100% 188/188 [01:14<00:00,  2.51it/s, loss=574, v_num=0, val_loss_epoch=569.0, train_loss_step=576.0, train_loss_epoch=580.0, val_loss_step=582.0]\n","Epoch 50: 100% 188/188 [01:15<00:00,  2.50it/s, loss=574, v_num=0, val_loss_epoch=570.0, train_loss_step=556.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Epoch 51:  76% 142/188 [01:09<00:22,  2.05it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.44it/s]\u001b[A\n","Epoch 51:  77% 144/188 [01:09<00:21,  2.07it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.07it/s]\u001b[A\n","Epoch 51:  78% 146/188 [01:09<00:20,  2.09it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.67it/s]\u001b[A\n","Epoch 51:  79% 148/188 [01:10<00:19,  2.10it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.83it/s]\u001b[A\n","Epoch 51:  80% 150/188 [01:10<00:17,  2.12it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.91it/s]\u001b[A\n","Epoch 51:  81% 152/188 [01:10<00:16,  2.14it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.18it/s]\u001b[A\n","Epoch 51:  82% 154/188 [01:11<00:15,  2.16it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.13it/s]\u001b[A\n","Epoch 51:  83% 156/188 [01:11<00:14,  2.18it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  32% 15/47 [00:02<00:05,  5.86it/s]\u001b[A\n","Epoch 51:  84% 158/188 [01:12<00:13,  2.19it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.82it/s]\u001b[A\n","Epoch 51:  85% 160/188 [01:12<00:12,  2.21it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  40% 19/47 [00:03<00:05,  4.72it/s]\u001b[A\n","Epoch 51:  86% 162/188 [01:12<00:11,  2.22it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  45% 21/47 [00:04<00:05,  4.39it/s]\u001b[A\n","Epoch 51:  87% 164/188 [01:13<00:10,  2.23it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  49% 23/47 [00:04<00:05,  4.51it/s]\u001b[A\n","Epoch 51:  88% 166/188 [01:13<00:09,  2.25it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  53% 25/47 [00:04<00:04,  4.43it/s]\u001b[A\n","Epoch 51:  89% 168/188 [01:14<00:08,  2.26it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  57% 27/47 [00:05<00:04,  4.55it/s]\u001b[A\n","Epoch 51:  90% 170/188 [01:14<00:07,  2.27it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  62% 29/47 [00:05<00:03,  4.53it/s]\u001b[A\n","Epoch 51:  91% 172/188 [01:15<00:06,  2.29it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.54it/s]\u001b[A\n","Epoch 51:  93% 174/188 [01:15<00:06,  2.30it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  70% 33/47 [00:06<00:03,  4.60it/s]\u001b[A\n","Epoch 51:  94% 176/188 [01:16<00:05,  2.31it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  74% 35/47 [00:07<00:02,  4.62it/s]\u001b[A\n","Epoch 51:  95% 178/188 [01:16<00:04,  2.33it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  79% 37/47 [00:07<00:02,  4.58it/s]\u001b[A\n","Epoch 51:  96% 180/188 [01:16<00:03,  2.34it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  83% 39/47 [00:07<00:01,  4.41it/s]\u001b[A\n","Epoch 51:  97% 182/188 [01:17<00:02,  2.35it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  87% 41/47 [00:08<00:01,  4.32it/s]\u001b[A\n","Epoch 51:  98% 184/188 [01:17<00:01,  2.36it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.15it/s]\u001b[A\n","Epoch 51:  99% 186/188 [01:18<00:00,  2.37it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Validating:  96% 45/47 [00:09<00:00,  4.25it/s]\u001b[A\n","Epoch 51: 100% 188/188 [01:18<00:00,  2.39it/s, loss=581, v_num=0, val_loss_epoch=570.0, train_loss_step=574.0, train_loss_epoch=580.0, val_loss_step=579.0]\n","Epoch 51: 100% 188/188 [01:19<00:00,  2.38it/s, loss=581, v_num=0, val_loss_epoch=569.0, train_loss_step=494.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Epoch 52:  76% 142/188 [01:06<00:21,  2.14it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.33it/s]\u001b[A\n","Epoch 52:  77% 144/188 [01:06<00:20,  2.16it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.04it/s]\u001b[A\n","Epoch 52:  78% 146/188 [01:07<00:19,  2.17it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.70it/s]\u001b[A\n","Epoch 52:  79% 148/188 [01:07<00:18,  2.19it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.83it/s]\u001b[A\n","Epoch 52:  80% 150/188 [01:07<00:17,  2.21it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  19% 9/47 [00:01<00:06,  6.02it/s]\u001b[A\n","Epoch 52:  81% 152/188 [01:08<00:16,  2.23it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.10it/s]\u001b[A\n","Epoch 52:  82% 154/188 [01:08<00:15,  2.25it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.06it/s]\u001b[A\n","Epoch 52:  83% 156/188 [01:08<00:14,  2.27it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.07it/s]\u001b[A\n","Epoch 52:  84% 158/188 [01:09<00:13,  2.29it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  36% 17/47 [00:02<00:05,  5.89it/s]\u001b[A\n","Epoch 52:  85% 160/188 [01:09<00:12,  2.30it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.91it/s]\u001b[A\n","Epoch 52:  86% 162/188 [01:09<00:11,  2.32it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.10it/s]\u001b[A\n","Epoch 52:  87% 164/188 [01:10<00:10,  2.34it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  49% 23/47 [00:03<00:04,  5.98it/s]\u001b[A\n","Epoch 52:  88% 166/188 [01:10<00:09,  2.36it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.93it/s]\u001b[A\n","Epoch 52:  89% 168/188 [01:10<00:08,  2.37it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.96it/s]\u001b[A\n","Epoch 52:  90% 170/188 [01:11<00:07,  2.39it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  62% 29/47 [00:04<00:03,  5.96it/s]\u001b[A\n","Epoch 52:  91% 172/188 [01:11<00:06,  2.41it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  66% 31/47 [00:05<00:02,  6.07it/s]\u001b[A\n","Epoch 52:  93% 174/188 [01:11<00:05,  2.42it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  70% 33/47 [00:05<00:02,  6.02it/s]\u001b[A\n","Epoch 52:  94% 176/188 [01:12<00:04,  2.44it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  74% 35/47 [00:05<00:01,  6.22it/s]\u001b[A\n","Epoch 52:  95% 178/188 [01:12<00:04,  2.46it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  79% 37/47 [00:06<00:01,  6.06it/s]\u001b[A\n","Epoch 52:  96% 180/188 [01:12<00:03,  2.47it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  83% 39/47 [00:06<00:01,  6.03it/s]\u001b[A\n","Epoch 52:  97% 182/188 [01:13<00:02,  2.49it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  87% 41/47 [00:06<00:00,  6.06it/s]\u001b[A\n","Epoch 52:  98% 184/188 [01:13<00:01,  2.51it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  91% 43/47 [00:07<00:00,  6.09it/s]\u001b[A\n","Epoch 52:  99% 186/188 [01:13<00:00,  2.52it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Validating:  96% 45/47 [00:07<00:00,  5.98it/s]\u001b[A\n","Epoch 52: 100% 188/188 [01:14<00:00,  2.54it/s, loss=570, v_num=0, val_loss_epoch=569.0, train_loss_step=548.0, train_loss_epoch=579.0, val_loss_step=582.0]\n","Epoch 52: 100% 188/188 [01:14<00:00,  2.53it/s, loss=570, v_num=0, val_loss_epoch=567.0, train_loss_step=560.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Epoch 53:  76% 142/188 [01:08<00:22,  2.07it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:13,  3.50it/s]\u001b[A\n","Epoch 53:  77% 144/188 [01:09<00:21,  2.09it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.06it/s]\u001b[A\n","Epoch 53:  78% 146/188 [01:09<00:19,  2.10it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.67it/s]\u001b[A\n","Epoch 53:  79% 148/188 [01:09<00:18,  2.12it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.87it/s]\u001b[A\n","Epoch 53:  80% 150/188 [01:10<00:17,  2.14it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  19% 9/47 [00:01<00:07,  4.86it/s]\u001b[A\n","Epoch 53:  81% 152/188 [01:10<00:16,  2.15it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  23% 11/47 [00:02<00:07,  4.69it/s]\u001b[A\n","Epoch 53:  82% 154/188 [01:10<00:15,  2.17it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  28% 13/47 [00:02<00:07,  4.52it/s]\u001b[A\n","Epoch 53:  83% 156/188 [01:11<00:14,  2.18it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  32% 15/47 [00:03<00:07,  4.47it/s]\u001b[A\n","Epoch 53:  84% 158/188 [01:11<00:13,  2.20it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.38it/s]\u001b[A\n","Epoch 53:  85% 160/188 [01:12<00:12,  2.21it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  40% 19/47 [00:04<00:06,  4.42it/s]\u001b[A\n","Epoch 53:  86% 162/188 [01:12<00:11,  2.23it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  45% 21/47 [00:04<00:05,  4.53it/s]\u001b[A\n","Epoch 53:  87% 164/188 [01:13<00:10,  2.24it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  49% 23/47 [00:04<00:05,  4.42it/s]\u001b[A\n","Epoch 53:  88% 166/188 [01:13<00:09,  2.25it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  53% 25/47 [00:05<00:04,  4.44it/s]\u001b[A\n","Epoch 53:  89% 168/188 [01:14<00:08,  2.27it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  57% 27/47 [00:05<00:04,  4.47it/s]\u001b[A\n","Epoch 53:  90% 170/188 [01:14<00:07,  2.28it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  62% 29/47 [00:06<00:04,  4.26it/s]\u001b[A\n","Epoch 53:  91% 172/188 [01:15<00:06,  2.29it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.32it/s]\u001b[A\n","Epoch 53:  93% 174/188 [01:15<00:06,  2.30it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  70% 33/47 [00:07<00:03,  4.33it/s]\u001b[A\n","Epoch 53:  94% 176/188 [01:16<00:05,  2.31it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  74% 35/47 [00:07<00:02,  4.52it/s]\u001b[A\n","Epoch 53:  95% 178/188 [01:16<00:04,  2.33it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.21it/s]\u001b[A\n","Epoch 53:  96% 180/188 [01:16<00:03,  2.35it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  83% 39/47 [00:08<00:01,  5.49it/s]\u001b[A\n","Epoch 53:  97% 182/188 [01:17<00:02,  2.36it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.84it/s]\u001b[A\n","Epoch 53:  98% 184/188 [01:17<00:01,  2.38it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  91% 43/47 [00:08<00:00,  5.94it/s]\u001b[A\n","Epoch 53:  99% 186/188 [01:17<00:00,  2.39it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Validating:  96% 45/47 [00:09<00:00,  6.07it/s]\u001b[A\n","Epoch 53: 100% 188/188 [01:18<00:00,  2.41it/s, loss=566, v_num=0, val_loss_epoch=567.0, train_loss_step=542.0, train_loss_epoch=578.0, val_loss_step=582.0]\n","Epoch 53: 100% 188/188 [01:18<00:00,  2.40it/s, loss=566, v_num=0, val_loss_epoch=564.0, train_loss_step=539.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Epoch 54:  76% 142/188 [01:07<00:21,  2.11it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:12,  3.58it/s]\u001b[A\n","Epoch 54:  77% 144/188 [01:07<00:20,  2.13it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:   6% 3/47 [00:00<00:08,  5.06it/s]\u001b[A\n","Epoch 54:  78% 146/188 [01:08<00:19,  2.15it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  11% 5/47 [00:00<00:07,  5.71it/s]\u001b[A\n","Epoch 54:  79% 148/188 [01:08<00:18,  2.17it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  15% 7/47 [00:01<00:06,  5.79it/s]\u001b[A\n","Epoch 54:  80% 150/188 [01:08<00:17,  2.18it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.89it/s]\u001b[A\n","Epoch 54:  81% 152/188 [01:09<00:16,  2.20it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  23% 11/47 [00:01<00:05,  6.06it/s]\u001b[A\n","Epoch 54:  82% 154/188 [01:09<00:15,  2.22it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  28% 13/47 [00:02<00:05,  6.24it/s]\u001b[A\n","Epoch 54:  83% 156/188 [01:09<00:14,  2.24it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  32% 15/47 [00:02<00:05,  6.04it/s]\u001b[A\n","Epoch 54:  84% 158/188 [01:09<00:13,  2.26it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  36% 17/47 [00:02<00:04,  6.04it/s]\u001b[A\n","Epoch 54:  85% 160/188 [01:10<00:12,  2.28it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  40% 19/47 [00:03<00:04,  6.12it/s]\u001b[A\n","Epoch 54:  86% 162/188 [01:10<00:11,  2.29it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  45% 21/47 [00:03<00:04,  6.11it/s]\u001b[A\n","Epoch 54:  87% 164/188 [01:10<00:10,  2.31it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  49% 23/47 [00:03<00:04,  5.98it/s]\u001b[A\n","Epoch 54:  88% 166/188 [01:11<00:09,  2.33it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  53% 25/47 [00:04<00:03,  5.99it/s]\u001b[A\n","Epoch 54:  89% 168/188 [01:11<00:08,  2.34it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  57% 27/47 [00:04<00:03,  5.38it/s]\u001b[A\n","Epoch 54:  90% 170/188 [01:12<00:07,  2.36it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  62% 29/47 [00:05<00:03,  4.76it/s]\u001b[A\n","Epoch 54:  91% 172/188 [01:12<00:06,  2.37it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  66% 31/47 [00:05<00:03,  4.49it/s]\u001b[A\n","Epoch 54:  93% 174/188 [01:13<00:05,  2.38it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  70% 33/47 [00:06<00:03,  4.38it/s]\u001b[A\n","Epoch 54:  94% 176/188 [01:13<00:05,  2.39it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  74% 35/47 [00:06<00:02,  4.14it/s]\u001b[A\n","Epoch 54:  95% 178/188 [01:14<00:04,  2.40it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  79% 37/47 [00:07<00:02,  3.99it/s]\u001b[A\n","Epoch 54:  96% 180/188 [01:14<00:03,  2.42it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  83% 39/47 [00:07<00:01,  4.10it/s]\u001b[A\n","Epoch 54:  97% 182/188 [01:15<00:02,  2.43it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  87% 41/47 [00:08<00:01,  4.14it/s]\u001b[A\n","Epoch 54:  98% 184/188 [01:15<00:01,  2.44it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.15it/s]\u001b[A\n","Epoch 54:  99% 186/188 [01:15<00:00,  2.45it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.17it/s]\u001b[A\n","Epoch 54: 100% 188/188 [01:16<00:00,  2.46it/s, loss=570, v_num=0, val_loss_epoch=564.0, train_loss_step=615.0, train_loss_epoch=577.0, val_loss_step=580.0]\n","Epoch 54: 100% 188/188 [01:16<00:00,  2.45it/s, loss=570, v_num=0, val_loss_epoch=563.0, train_loss_step=531.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Epoch 55:  76% 142/188 [01:13<00:23,  1.93it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.84it/s]\u001b[A\n","Epoch 55:  77% 144/188 [01:13<00:22,  1.95it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.17it/s]\u001b[A\n","Epoch 55:  78% 146/188 [01:14<00:21,  1.96it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.74it/s]\u001b[A\n","Epoch 55:  79% 148/188 [01:14<00:20,  1.98it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  15% 7/47 [00:01<00:08,  5.00it/s]\u001b[A\n","Epoch 55:  80% 150/188 [01:15<00:19,  2.00it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.07it/s]\u001b[A\n","Epoch 55:  81% 152/188 [01:15<00:17,  2.01it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  23% 11/47 [00:02<00:07,  5.09it/s]\u001b[A\n","Epoch 55:  82% 154/188 [01:15<00:16,  2.03it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.24it/s]\u001b[A\n","Epoch 55:  83% 156/188 [01:16<00:15,  2.05it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  32% 15/47 [00:03<00:05,  5.59it/s]\u001b[A\n","Epoch 55:  84% 158/188 [01:16<00:14,  2.06it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.80it/s]\u001b[A\n","Epoch 55:  85% 160/188 [01:16<00:13,  2.08it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.73it/s]\u001b[A\n","Epoch 55:  86% 162/188 [01:17<00:12,  2.10it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.33it/s]\u001b[A\n","Epoch 55:  87% 164/188 [01:17<00:11,  2.11it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.16it/s]\u001b[A\n","Epoch 55:  88% 166/188 [01:18<00:10,  2.12it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.06it/s]\u001b[A\n","Epoch 55:  89% 168/188 [01:18<00:09,  2.14it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.08it/s]\u001b[A\n","Epoch 55:  90% 170/188 [01:18<00:08,  2.15it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  62% 29/47 [00:05<00:03,  4.88it/s]\u001b[A\n","Epoch 55:  91% 172/188 [01:19<00:07,  2.17it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.82it/s]\u001b[A\n","Epoch 55:  93% 174/188 [01:19<00:06,  2.18it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  70% 33/47 [00:06<00:02,  4.83it/s]\u001b[A\n","Epoch 55:  94% 176/188 [01:20<00:05,  2.19it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  74% 35/47 [00:06<00:02,  4.94it/s]\u001b[A\n","Epoch 55:  95% 178/188 [01:20<00:04,  2.21it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.04it/s]\u001b[A\n","Epoch 55:  96% 180/188 [01:20<00:03,  2.22it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.09it/s]\u001b[A\n","Epoch 55:  97% 182/188 [01:21<00:02,  2.24it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.58it/s]\u001b[A\n","Epoch 55:  98% 184/188 [01:21<00:01,  2.25it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  91% 43/47 [00:08<00:00,  5.75it/s]\u001b[A\n","Epoch 55:  99% 186/188 [01:22<00:00,  2.27it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Validating:  96% 45/47 [00:08<00:00,  4.92it/s]\u001b[A\n","Epoch 55: 100% 188/188 [01:22<00:00,  2.28it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=580.0, train_loss_epoch=575.0, val_loss_step=577.0]\n","Epoch 55: 100% 188/188 [01:22<00:00,  2.27it/s, loss=575, v_num=0, val_loss_epoch=563.0, train_loss_step=472.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Epoch 56:  76% 142/188 [01:20<00:25,  1.77it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.85it/s]\u001b[A\n","Epoch 56:  77% 144/188 [01:20<00:24,  1.79it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.31it/s]\u001b[A\n","Epoch 56:  78% 146/188 [01:21<00:23,  1.80it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  11% 5/47 [00:01<00:08,  4.73it/s]\u001b[A\n","Epoch 56:  79% 148/188 [01:21<00:22,  1.82it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  15% 7/47 [00:01<00:08,  4.95it/s]\u001b[A\n","Epoch 56:  80% 150/188 [01:21<00:20,  1.83it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.09it/s]\u001b[A\n","Epoch 56:  81% 152/188 [01:22<00:19,  1.85it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.14it/s]\u001b[A\n","Epoch 56:  82% 154/188 [01:22<00:18,  1.86it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.21it/s]\u001b[A\n","Epoch 56:  83% 156/188 [01:22<00:17,  1.88it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  32% 15/47 [00:03<00:06,  5.24it/s]\u001b[A\n","Epoch 56:  84% 158/188 [01:23<00:15,  1.90it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.73it/s]\u001b[A\n","Epoch 56:  85% 160/188 [01:23<00:14,  1.91it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  40% 19/47 [00:03<00:04,  5.86it/s]\u001b[A\n","Epoch 56:  86% 162/188 [01:23<00:13,  1.93it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.92it/s]\u001b[A\n","Epoch 56:  87% 164/188 [01:24<00:12,  1.94it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.44it/s]\u001b[A\n","Epoch 56:  88% 166/188 [01:24<00:11,  1.96it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  53% 25/47 [00:04<00:04,  5.25it/s]\u001b[A\n","Epoch 56:  89% 168/188 [01:25<00:10,  1.97it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.19it/s]\u001b[A\n","Epoch 56:  90% 170/188 [01:25<00:09,  1.99it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.26it/s]\u001b[A\n","Epoch 56:  91% 172/188 [01:25<00:07,  2.00it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  66% 31/47 [00:06<00:03,  5.15it/s]\u001b[A\n","Epoch 56:  93% 174/188 [01:26<00:06,  2.02it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  70% 33/47 [00:06<00:02,  4.94it/s]\u001b[A\n","Epoch 56:  94% 176/188 [01:26<00:05,  2.03it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  74% 35/47 [00:07<00:02,  4.10it/s]\u001b[A\n","Epoch 56:  95% 178/188 [01:27<00:04,  2.04it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  79% 37/47 [00:07<00:02,  3.91it/s]\u001b[A\n","Epoch 56:  96% 180/188 [01:27<00:03,  2.05it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  83% 39/47 [00:08<00:02,  3.90it/s]\u001b[A\n","Epoch 56:  97% 182/188 [01:28<00:02,  2.06it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  87% 41/47 [00:08<00:01,  3.81it/s]\u001b[A\n","Epoch 56:  98% 184/188 [01:28<00:01,  2.07it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  91% 43/47 [00:09<00:01,  3.66it/s]\u001b[A\n","Epoch 56:  99% 186/188 [01:29<00:00,  2.08it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Validating:  96% 45/47 [00:09<00:00,  3.61it/s]\u001b[A\n","Epoch 56: 100% 188/188 [01:30<00:00,  2.09it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=613.0, train_loss_epoch=575.0, val_loss_step=572.0]\n","Epoch 56: 100% 188/188 [01:30<00:00,  2.08it/s, loss=554, v_num=0, val_loss_epoch=563.0, train_loss_step=531.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Epoch 57:  76% 142/188 [01:15<00:24,  1.87it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:17,  2.66it/s]\u001b[A\n","Epoch 57:  77% 144/188 [01:16<00:23,  1.88it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.25it/s]\u001b[A\n","Epoch 57:  78% 146/188 [01:16<00:22,  1.90it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  11% 5/47 [00:01<00:07,  5.29it/s]\u001b[A\n","Epoch 57:  79% 148/188 [01:17<00:20,  1.92it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  15% 7/47 [00:01<00:07,  5.60it/s]\u001b[A\n","Epoch 57:  80% 150/188 [01:17<00:19,  1.94it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  19% 9/47 [00:01<00:06,  5.51it/s]\u001b[A\n","Epoch 57:  81% 152/188 [01:17<00:18,  1.95it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.27it/s]\u001b[A\n","Epoch 57:  82% 154/188 [01:18<00:17,  1.97it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.26it/s]\u001b[A\n","Epoch 57:  83% 156/188 [01:18<00:16,  1.98it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  32% 15/47 [00:03<00:06,  4.68it/s]\u001b[A\n","Epoch 57:  84% 158/188 [01:19<00:15,  1.99it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  36% 17/47 [00:03<00:07,  4.21it/s]\u001b[A\n","Epoch 57:  85% 160/188 [01:19<00:13,  2.01it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  40% 19/47 [00:04<00:06,  4.01it/s]\u001b[A\n","Epoch 57:  86% 162/188 [01:20<00:12,  2.02it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  45% 21/47 [00:04<00:06,  4.04it/s]\u001b[A\n","Epoch 57:  87% 164/188 [01:20<00:11,  2.03it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  49% 23/47 [00:05<00:05,  4.24it/s]\u001b[A\n","Epoch 57:  88% 166/188 [01:21<00:10,  2.05it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  53% 25/47 [00:05<00:05,  4.21it/s]\u001b[A\n","Epoch 57:  89% 168/188 [01:21<00:09,  2.06it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  57% 27/47 [00:05<00:04,  4.20it/s]\u001b[A\n","Epoch 57:  90% 170/188 [01:22<00:08,  2.07it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  62% 29/47 [00:06<00:04,  4.03it/s]\u001b[A\n","Epoch 57:  91% 172/188 [01:22<00:07,  2.08it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  66% 31/47 [00:06<00:03,  4.05it/s]\u001b[A\n","Epoch 57:  93% 174/188 [01:23<00:06,  2.09it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  70% 33/47 [00:07<00:03,  4.12it/s]\u001b[A\n","Epoch 57:  94% 176/188 [01:23<00:05,  2.10it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  74% 35/47 [00:07<00:02,  4.02it/s]\u001b[A\n","Epoch 57:  95% 178/188 [01:24<00:04,  2.11it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  79% 37/47 [00:08<00:02,  3.73it/s]\u001b[A\n","Epoch 57:  96% 180/188 [01:24<00:03,  2.12it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  83% 39/47 [00:09<00:02,  3.62it/s]\u001b[A\n","Epoch 57:  97% 182/188 [01:25<00:02,  2.13it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  87% 41/47 [00:09<00:01,  3.71it/s]\u001b[A\n","Epoch 57:  98% 184/188 [01:25<00:01,  2.15it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  91% 43/47 [00:10<00:00,  4.54it/s]\u001b[A\n","Epoch 57:  99% 186/188 [01:26<00:00,  2.16it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Validating:  96% 45/47 [00:10<00:00,  5.04it/s]\u001b[A\n","Epoch 57: 100% 188/188 [01:26<00:00,  2.18it/s, loss=569, v_num=0, val_loss_epoch=563.0, train_loss_step=533.0, train_loss_epoch=574.0, val_loss_step=581.0]\n","Epoch 57: 100% 188/188 [01:26<00:00,  2.17it/s, loss=569, v_num=0, val_loss_epoch=560.0, train_loss_step=526.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Epoch 58:  76% 142/188 [01:18<00:25,  1.80it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:23,  1.97it/s]\u001b[A\n","Epoch 58:  77% 144/188 [01:19<00:24,  1.81it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:   6% 3/47 [00:01<00:15,  2.82it/s]\u001b[A\n","Epoch 58:  78% 146/188 [01:20<00:23,  1.82it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  11% 5/47 [00:01<00:12,  3.23it/s]\u001b[A\n","Epoch 58:  79% 148/188 [01:20<00:21,  1.83it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  15% 7/47 [00:02<00:10,  3.78it/s]\u001b[A\n","Epoch 58:  80% 150/188 [01:21<00:20,  1.85it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  19% 9/47 [00:02<00:08,  4.67it/s]\u001b[A\n","Epoch 58:  81% 152/188 [01:21<00:19,  1.87it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  23% 11/47 [00:02<00:06,  5.26it/s]\u001b[A\n","Epoch 58:  82% 154/188 [01:21<00:18,  1.88it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  28% 13/47 [00:03<00:06,  5.45it/s]\u001b[A\n","Epoch 58:  83% 156/188 [01:22<00:16,  1.90it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  32% 15/47 [00:03<00:06,  5.27it/s]\u001b[A\n","Epoch 58:  84% 158/188 [01:22<00:15,  1.91it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  36% 17/47 [00:03<00:05,  5.37it/s]\u001b[A\n","Epoch 58:  85% 160/188 [01:22<00:14,  1.93it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  40% 19/47 [00:04<00:05,  5.52it/s]\u001b[A\n","Epoch 58:  86% 162/188 [01:23<00:13,  1.95it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  45% 21/47 [00:04<00:04,  5.64it/s]\u001b[A\n","Epoch 58:  87% 164/188 [01:23<00:12,  1.96it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  49% 23/47 [00:04<00:04,  5.95it/s]\u001b[A\n","Epoch 58:  88% 166/188 [01:23<00:11,  1.98it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  53% 25/47 [00:05<00:03,  6.10it/s]\u001b[A\n","Epoch 58:  89% 168/188 [01:24<00:10,  1.99it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  57% 27/47 [00:05<00:03,  6.00it/s]\u001b[A\n","Epoch 58:  90% 170/188 [01:24<00:08,  2.01it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  62% 29/47 [00:05<00:03,  5.97it/s]\u001b[A\n","Epoch 58:  91% 172/188 [01:24<00:07,  2.03it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.60it/s]\u001b[A\n","Epoch 58:  93% 174/188 [01:25<00:06,  2.04it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.15it/s]\u001b[A\n","Epoch 58:  94% 176/188 [01:25<00:05,  2.05it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  74% 35/47 [00:07<00:02,  5.09it/s]\u001b[A\n","Epoch 58:  95% 178/188 [01:26<00:04,  2.07it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  79% 37/47 [00:07<00:01,  5.31it/s]\u001b[A\n","Epoch 58:  96% 180/188 [01:26<00:03,  2.08it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  83% 39/47 [00:07<00:01,  5.53it/s]\u001b[A\n","Epoch 58:  97% 182/188 [01:26<00:02,  2.10it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  87% 41/47 [00:08<00:01,  5.96it/s]\u001b[A\n","Epoch 58:  98% 184/188 [01:27<00:01,  2.11it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  91% 43/47 [00:08<00:00,  5.52it/s]\u001b[A\n","Epoch 58:  99% 186/188 [01:27<00:00,  2.13it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Validating:  96% 45/47 [00:08<00:00,  5.20it/s]\u001b[A\n","Epoch 58: 100% 188/188 [01:27<00:00,  2.14it/s, loss=561, v_num=0, val_loss_epoch=560.0, train_loss_step=606.0, train_loss_epoch=573.0, val_loss_step=577.0]\n","Epoch 58: 100% 188/188 [01:28<00:00,  2.13it/s, loss=561, v_num=0, val_loss_epoch=558.0, train_loss_step=524.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Epoch 59:  76% 142/188 [01:17<00:25,  1.82it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:16,  2.73it/s]\u001b[A\n","Epoch 59:  77% 144/188 [01:18<00:23,  1.84it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:   6% 3/47 [00:00<00:10,  4.04it/s]\u001b[A\n","Epoch 59:  78% 146/188 [01:18<00:22,  1.85it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  11% 5/47 [00:01<00:09,  4.56it/s]\u001b[A\n","Epoch 59:  79% 148/188 [01:19<00:21,  1.87it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  15% 7/47 [00:01<00:08,  4.94it/s]\u001b[A\n","Epoch 59:  80% 150/188 [01:19<00:20,  1.88it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  19% 9/47 [00:01<00:07,  5.03it/s]\u001b[A\n","Epoch 59:  81% 152/188 [01:19<00:18,  1.90it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  23% 11/47 [00:02<00:07,  5.06it/s]\u001b[A\n","Epoch 59:  82% 154/188 [01:20<00:17,  1.92it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  28% 13/47 [00:02<00:06,  5.05it/s]\u001b[A\n","Epoch 59:  83% 156/188 [01:20<00:16,  1.93it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  32% 15/47 [00:03<00:06,  4.87it/s]\u001b[A\n","Epoch 59:  84% 158/188 [01:21<00:15,  1.95it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  36% 17/47 [00:03<00:06,  4.86it/s]\u001b[A\n","Epoch 59:  85% 160/188 [01:21<00:14,  1.96it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  40% 19/47 [00:03<00:05,  4.84it/s]\u001b[A\n","Epoch 59:  86% 162/188 [01:21<00:13,  1.98it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  45% 21/47 [00:04<00:05,  4.88it/s]\u001b[A\n","Epoch 59:  87% 164/188 [01:22<00:12,  1.99it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  49% 23/47 [00:04<00:04,  4.97it/s]\u001b[A\n","Epoch 59:  88% 166/188 [01:22<00:10,  2.01it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  53% 25/47 [00:05<00:04,  5.33it/s]\u001b[A\n","Epoch 59:  89% 168/188 [01:23<00:09,  2.02it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  57% 27/47 [00:05<00:03,  5.68it/s]\u001b[A\n","Epoch 59:  90% 170/188 [01:23<00:08,  2.04it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  62% 29/47 [00:05<00:02,  6.00it/s]\u001b[A\n","Epoch 59:  91% 172/188 [01:23<00:07,  2.05it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  66% 31/47 [00:06<00:02,  5.51it/s]\u001b[A\n","Epoch 59:  93% 174/188 [01:24<00:06,  2.07it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  70% 33/47 [00:06<00:02,  5.30it/s]\u001b[A\n","Epoch 59:  94% 176/188 [01:24<00:05,  2.08it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  74% 35/47 [00:06<00:02,  5.23it/s]\u001b[A\n","Epoch 59:  95% 178/188 [01:24<00:04,  2.10it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  79% 37/47 [00:07<00:02,  4.83it/s]\u001b[A\n","Epoch 59:  96% 180/188 [01:25<00:03,  2.11it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  83% 39/47 [00:07<00:01,  4.57it/s]\u001b[A\n","Epoch 59:  97% 182/188 [01:25<00:02,  2.12it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  87% 41/47 [00:08<00:01,  4.47it/s]\u001b[A\n","Epoch 59:  98% 184/188 [01:26<00:01,  2.13it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  91% 43/47 [00:08<00:00,  4.54it/s]\u001b[A\n","Epoch 59:  99% 186/188 [01:26<00:00,  2.14it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Validating:  96% 45/47 [00:09<00:00,  4.24it/s]\u001b[A\n","Epoch 59: 100% 188/188 [01:27<00:00,  2.15it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=608.0, train_loss_epoch=572.0, val_loss_step=564.0]\n","Epoch 59: 100% 188/188 [01:27<00:00,  2.14it/s, loss=576, v_num=0, val_loss_epoch=558.0, train_loss_step=546.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Epoch 60:  76% 142/188 [01:20<00:25,  1.77it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Validating:   2% 1/47 [00:00<00:23,  1.98it/s]\u001b[A\n","Epoch 60:  77% 144/188 [01:20<00:24,  1.78it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:   6% 3/47 [00:01<00:13,  3.19it/s]\u001b[A\n","Epoch 60:  78% 146/188 [01:21<00:23,  1.79it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  11% 5/47 [00:01<00:11,  3.71it/s]\u001b[A\n","Epoch 60:  79% 148/188 [01:21<00:22,  1.81it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  15% 7/47 [00:02<00:10,  3.65it/s]\u001b[A\n","Epoch 60:  80% 150/188 [01:22<00:20,  1.82it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  19% 9/47 [00:02<00:10,  3.74it/s]\u001b[A\n","Epoch 60:  81% 152/188 [01:22<00:19,  1.83it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  23% 11/47 [00:03<00:09,  3.72it/s]\u001b[A\n","Epoch 60:  82% 154/188 [01:23<00:18,  1.84it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  28% 13/47 [00:03<00:09,  3.70it/s]\u001b[A\n","Epoch 60:  83% 156/188 [01:24<00:17,  1.86it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  32% 15/47 [00:04<00:08,  3.67it/s]\u001b[A\n","Epoch 60:  84% 158/188 [01:24<00:16,  1.87it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  36% 17/47 [00:04<00:07,  3.81it/s]\u001b[A\n","Epoch 60:  85% 160/188 [01:25<00:14,  1.88it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  40% 19/47 [00:05<00:07,  3.71it/s]\u001b[A\n","Epoch 60:  86% 162/188 [01:25<00:13,  1.89it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  45% 21/47 [00:05<00:07,  3.60it/s]\u001b[A\n","Epoch 60:  87% 164/188 [01:26<00:12,  1.90it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  49% 23/47 [00:06<00:06,  3.57it/s]\u001b[A\n","Epoch 60:  88% 166/188 [01:26<00:11,  1.91it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  53% 25/47 [00:06<00:05,  3.81it/s]\u001b[A\n","Epoch 60:  89% 168/188 [01:27<00:10,  1.93it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  57% 27/47 [00:07<00:04,  4.36it/s]\u001b[A\n","Epoch 60:  90% 170/188 [01:27<00:09,  1.94it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  62% 29/47 [00:07<00:03,  4.87it/s]\u001b[A\n","Epoch 60:  91% 172/188 [01:27<00:08,  1.95it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  66% 31/47 [00:08<00:03,  5.04it/s]\u001b[A\n","Epoch 60:  93% 174/188 [01:28<00:07,  1.97it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  70% 33/47 [00:08<00:02,  5.43it/s]\u001b[A\n","Epoch 60:  94% 176/188 [01:28<00:06,  1.98it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  74% 35/47 [00:08<00:02,  5.81it/s]\u001b[A\n","Epoch 60:  95% 178/188 [01:29<00:05,  2.00it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  79% 37/47 [00:09<00:01,  5.72it/s]\u001b[A\n","Epoch 60:  96% 180/188 [01:29<00:03,  2.01it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  83% 39/47 [00:09<00:01,  5.54it/s]\u001b[A\n","Epoch 60:  97% 182/188 [01:29<00:02,  2.03it/s, loss=567, v_num=0, val_loss_epoch=558.0, train_loss_step=572.0, train_loss_epoch=569.0, val_loss_step=572.0]\n","Validating:  87% 41/47 [00:09<00:01,  5.39it/s]\u001b[A"]}]},{"cell_type":"code","source":["# copy path file dari baris terakhir output cell di atas.\n","\n","best_ssl_model_ckpt = \"/content/outputs/2023-06-11/19-48-11/lightning_logs/version_0/checkpoints/6-104.ckpt\""],"metadata":{"id":"nAskwksGLXVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**WARNING : Edit dulu**\n","\n","```1. /content/saint/utils/utils.py, line 42```\n","```\n","def auroc(self):\n","  return AUROC(num_classes=self.num_classes, task='multiclass')\n","```\n","```2. /content/saint/src/trainer.py, line 81```\n","```\n","def on_training_epoch_end(self, training_step_outputs):\n","```\n","```4. /content/saint/src/trainer.py, line 107```\n","```\n","def on_test_epoch_end(self):\n","```\n","```5. /content/saint/src/trainer.py, line 16```\n","```\n","super().__init__()\n","self.validation_step_outputs = []\n","self.transformer = transformer\n","```\n","```6. /content/saint/src/trainer.py, line 88```\n","```\n","def validation_step(self, batch, batch_idx):\n","    val_loss = self._shared_step(batch, self.valid_metric)\n","\n","    # log the outputs!\n","    self.log(f'val_loss', val_loss, on_step=False, \n","              on_epoch=True, prog_bar=True, logger=True)\n","    \n","    self.log(f'val_{self.metric}_epoch', self.valid_metric.compute(), prog_bar=True,)\n","    \n","    self.validation_step_outputs.append(val_loss)\n","    return val_loss\n","```\n","```7. /content/saint/src/trainer.py, line 98```\n","```\n","def on_validation_epoch_end(self):\n","    self.log(f'val_{self.metric}_epoch', self.valid_metric.compute(), prog_bar=True,)\n","\n","    # reset after each epoch\n","    self.valid_metric.reset()\n","    \n","    epoch_average = torch.stack(self.validation_step_outputs).mean()\n","    self.log(\"validation_epoch_average\", epoch_average)\n","    self.validation_step_outputs.clear()  # free memory\n","```\n","```Reference : https://github.com/Lightning-AI/lightning/discussions/17182```\n","\n","```8. /content/saint/src/train.py, line 42```\n","```\n","trainer.fit(model, dataloaders['train_loader'], dataloaders['validation_loader'])\n","```"],"metadata":{"id":"pl-QqhdQBdvr"}},{"cell_type":"code","source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"I9ogLvZUJHr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SUP from SSL\n","\n","!python /content/saint/main.py experiment=supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_sup \\\n","  experiment.pretrained_checkpoint={best_ssl_model_ckpt}"],"metadata":{"id":"kyF5SYkKLY2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686515980676,"user_tz":-420,"elapsed":278031,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"2b64d7de-a36a-44de-ccdc-b4eea5a887eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 20:35:06.465433: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-11 20:35:08.182616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/main.py:11: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Global seed set to 1234\n","Initializing supervised task using pretrained model:\n","/content/outputs/2023-06-11/19-48-11/lightning_logs/version_0/checkpoints/6-104.ckpt\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","\n","  | Name         | Type             | Params\n","--------------------------------------------------\n","0 | transformer  | Encoder          | 65.0 K\n","1 | embedding    | Embedding        | 3.2 K \n","2 | fc           | Linear           | 165   \n","3 | criterion    | CrossEntropyLoss | 0     \n","4 | train_metric | Accuracy         | 0     \n","5 | valid_metric | Accuracy         | 0     \n","6 | test_metric  | Accuracy         | 0     \n","--------------------------------------------------\n","68.3 K    Trainable params\n","0         Non-trainable params\n","68.3 K    Total params\n","0.273     Total estimated model params size (MB)\n","Global seed set to 1234\n","Epoch 0:  58% 7/12 [00:02<00:02,  2.46it/s, loss=1.68, v_num=0, val_loss=1.760, val_acc_epoch=0.250, train_loss_step=1.520]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  75% 9/12 [00:03<00:01,  2.84it/s, loss=1.68, v_num=0, val_loss=1.760, val_acc_epoch=0.250, train_loss_step=1.520]\n","Validating:  40% 2/5 [00:00<00:00,  4.71it/s]\u001b[A\n","Epoch 0:  92% 11/12 [00:03<00:00,  3.18it/s, loss=1.68, v_num=0, val_loss=1.760, val_acc_epoch=0.250, train_loss_step=1.520]\n","Validating:  80% 4/5 [00:00<00:00,  6.23it/s]\u001b[A\n","Epoch 0: 100% 12/12 [00:03<00:00,  3.16it/s, loss=1.68, v_num=0, val_loss=1.680, val_acc_epoch=0.253, train_loss_step=1.850, train_loss_epoch=1.660, train_acc_epoch=0.235]\n","Epoch 1:  67% 8/12 [00:02<00:01,  3.17it/s, loss=1.63, v_num=0, val_loss=1.680, val_acc_epoch=0.253, train_loss_step=1.430, train_loss_epoch=1.660, train_acc_epoch=0.235]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.89it/s]\u001b[A\n","Epoch 1:  83% 10/12 [00:02<00:00,  3.54it/s, loss=1.63, v_num=0, val_loss=1.680, val_acc_epoch=0.253, train_loss_step=1.430, train_loss_epoch=1.660, train_acc_epoch=0.235]\n","Epoch 1: 100% 12/12 [00:03<00:00,  3.84it/s, loss=1.63, v_num=0, val_loss=1.660, val_acc_epoch=0.193, train_loss_step=1.350, train_loss_epoch=1.610, train_acc_epoch=0.230]\n","Epoch 2:  67% 8/12 [00:01<00:00,  4.78it/s, loss=1.59, v_num=0, val_loss=1.660, val_acc_epoch=0.193, train_loss_step=1.490, train_loss_epoch=1.610, train_acc_epoch=0.230]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.73it/s]\u001b[A\n","Epoch 2:  83% 10/12 [00:01<00:00,  5.02it/s, loss=1.59, v_num=0, val_loss=1.660, val_acc_epoch=0.193, train_loss_step=1.490, train_loss_epoch=1.610, train_acc_epoch=0.230]\n","Epoch 2: 100% 12/12 [00:02<00:00,  5.23it/s, loss=1.59, v_num=0, val_loss=1.640, val_acc_epoch=0.227, train_loss_step=1.310, train_loss_epoch=1.560, train_acc_epoch=0.255]\n","Epoch 3:  67% 8/12 [00:01<00:00,  4.97it/s, loss=1.55, v_num=0, val_loss=1.640, val_acc_epoch=0.227, train_loss_step=1.410, train_loss_epoch=1.560, train_acc_epoch=0.255]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 3:  83% 10/12 [00:01<00:00,  5.30it/s, loss=1.55, v_num=0, val_loss=1.640, val_acc_epoch=0.227, train_loss_step=1.410, train_loss_epoch=1.560, train_acc_epoch=0.255]\n","Epoch 3: 100% 12/12 [00:02<00:00,  5.86it/s, loss=1.55, v_num=0, val_loss=1.640, val_acc_epoch=0.227, train_loss_step=1.410, train_loss_epoch=1.560, train_acc_epoch=0.255]\n","Epoch 3: 100% 12/12 [00:02<00:00,  5.61it/s, loss=1.55, v_num=0, val_loss=1.630, val_acc_epoch=0.260, train_loss_step=1.570, train_loss_epoch=1.550, train_acc_epoch=0.270]\n","Epoch 4:  67% 8/12 [00:01<00:00,  4.53it/s, loss=1.53, v_num=0, val_loss=1.630, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.550, train_acc_epoch=0.270]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 4:  83% 10/12 [00:02<00:00,  4.94it/s, loss=1.53, v_num=0, val_loss=1.630, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.550, train_acc_epoch=0.270]\n","Epoch 4: 100% 12/12 [00:02<00:00,  5.49it/s, loss=1.53, v_num=0, val_loss=1.630, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.550, train_acc_epoch=0.270]\n","Epoch 4: 100% 12/12 [00:02<00:00,  5.19it/s, loss=1.53, v_num=0, val_loss=1.630, val_acc_epoch=0.253, train_loss_step=1.370, train_loss_epoch=1.550, train_acc_epoch=0.275]\n","Epoch 5:  67% 8/12 [00:01<00:00,  4.65it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.253, train_loss_step=1.430, train_loss_epoch=1.550, train_acc_epoch=0.275]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.94it/s]\u001b[A\n","Epoch 5:  83% 10/12 [00:02<00:00,  4.73it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.253, train_loss_step=1.430, train_loss_epoch=1.550, train_acc_epoch=0.275]\n","Validating:  60% 3/5 [00:00<00:00,  6.33it/s]\u001b[A\n","Epoch 5: 100% 12/12 [00:02<00:00,  4.70it/s, loss=1.52, v_num=0, val_loss=1.620, val_acc_epoch=0.247, train_loss_step=1.280, train_loss_epoch=1.530, train_acc_epoch=0.330]\n","Epoch 6:  67% 8/12 [00:02<00:01,  3.34it/s, loss=1.5, v_num=0, val_loss=1.620, val_acc_epoch=0.247, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.330]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.39it/s]\u001b[A\n","Epoch 6:  83% 10/12 [00:02<00:00,  3.50it/s, loss=1.5, v_num=0, val_loss=1.620, val_acc_epoch=0.247, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.330]\n","Validating:  60% 3/5 [00:00<00:00,  5.80it/s]\u001b[A\n","Epoch 6: 100% 12/12 [00:03<00:00,  3.88it/s, loss=1.5, v_num=0, val_loss=1.620, val_acc_epoch=0.247, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.330]\n","Epoch 6: 100% 12/12 [00:03<00:00,  3.67it/s, loss=1.5, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.250, train_loss_epoch=1.530, train_acc_epoch=0.285]\n","Epoch 7:  67% 8/12 [00:02<00:01,  3.50it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.285]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 7:  83% 10/12 [00:02<00:00,  3.86it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.285]\n","Epoch 7: 100% 12/12 [00:02<00:00,  4.30it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.420, train_loss_epoch=1.530, train_acc_epoch=0.285]\n","Epoch 7: 100% 12/12 [00:02<00:00,  4.12it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.430, train_loss_epoch=1.570, train_acc_epoch=0.275]\n","Epoch 8:  67% 8/12 [00:01<00:00,  4.20it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.440, train_loss_epoch=1.570, train_acc_epoch=0.275]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.86it/s]\u001b[A\n","Epoch 8:  83% 10/12 [00:02<00:00,  4.50it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.440, train_loss_epoch=1.570, train_acc_epoch=0.275]\n","Epoch 8: 100% 12/12 [00:02<00:00,  4.75it/s, loss=1.52, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.260, train_loss_epoch=1.550, train_acc_epoch=0.300]\n","Epoch 9:  67% 8/12 [00:01<00:00,  4.78it/s, loss=1.51, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.330, train_loss_epoch=1.550, train_acc_epoch=0.300]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.32it/s]\u001b[A\n","Epoch 9:  83% 10/12 [00:01<00:00,  5.05it/s, loss=1.51, v_num=0, val_loss=1.630, val_acc_epoch=0.240, train_loss_step=1.330, train_loss_epoch=1.550, train_acc_epoch=0.300]\n","Epoch 9: 100% 12/12 [00:02<00:00,  5.29it/s, loss=1.51, v_num=0, val_loss=1.630, val_acc_epoch=0.233, train_loss_step=1.290, train_loss_epoch=1.510, train_acc_epoch=0.335]\n","Epoch 10:  67% 8/12 [00:01<00:00,  4.25it/s, loss=1.5, v_num=0, val_loss=1.630, val_acc_epoch=0.233, train_loss_step=1.400, train_loss_epoch=1.510, train_acc_epoch=0.335]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.97it/s]\u001b[A\n","Epoch 10:  83% 10/12 [00:02<00:00,  4.56it/s, loss=1.5, v_num=0, val_loss=1.630, val_acc_epoch=0.233, train_loss_step=1.400, train_loss_epoch=1.510, train_acc_epoch=0.335]\n","Epoch 10: 100% 12/12 [00:02<00:00,  4.83it/s, loss=1.5, v_num=0, val_loss=1.620, val_acc_epoch=0.233, train_loss_step=1.460, train_loss_epoch=1.510, train_acc_epoch=0.295]\n","Epoch 11:  67% 8/12 [00:02<00:01,  3.91it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.233, train_loss_step=1.360, train_loss_epoch=1.510, train_acc_epoch=0.295]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.44it/s]\u001b[A\n","Epoch 11:  83% 10/12 [00:02<00:00,  4.05it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.233, train_loss_step=1.360, train_loss_epoch=1.510, train_acc_epoch=0.295]\n","Validating:  60% 3/5 [00:00<00:00,  6.13it/s]\u001b[A\n","Epoch 11: 100% 12/12 [00:02<00:00,  4.42it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.233, train_loss_step=1.360, train_loss_epoch=1.510, train_acc_epoch=0.295]\n","Epoch 11: 100% 12/12 [00:02<00:00,  4.12it/s, loss=1.48, v_num=0, val_loss=1.630, val_acc_epoch=0.247, train_loss_step=1.220, train_loss_epoch=1.490, train_acc_epoch=0.320]\n","Epoch 12:  67% 8/12 [00:02<00:01,  3.33it/s, loss=1.48, v_num=0, val_loss=1.630, val_acc_epoch=0.247, train_loss_step=1.410, train_loss_epoch=1.490, train_acc_epoch=0.320]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.88it/s]\u001b[A\n","Epoch 12:  83% 10/12 [00:02<00:00,  3.59it/s, loss=1.48, v_num=0, val_loss=1.630, val_acc_epoch=0.247, train_loss_step=1.410, train_loss_epoch=1.490, train_acc_epoch=0.320]\n","Validating:  60% 3/5 [00:00<00:00,  6.68it/s]\u001b[A\n","Epoch 12: 100% 12/12 [00:03<00:00,  3.78it/s, loss=1.48, v_num=0, val_loss=1.630, val_acc_epoch=0.227, train_loss_step=1.360, train_loss_epoch=1.520, train_acc_epoch=0.310]\n","Epoch 13:  67% 8/12 [00:02<00:01,  4.00it/s, loss=1.49, v_num=0, val_loss=1.630, val_acc_epoch=0.227, train_loss_step=1.430, train_loss_epoch=1.520, train_acc_epoch=0.310]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 13:  83% 10/12 [00:02<00:00,  4.41it/s, loss=1.49, v_num=0, val_loss=1.630, val_acc_epoch=0.227, train_loss_step=1.430, train_loss_epoch=1.520, train_acc_epoch=0.310]\n","Epoch 13: 100% 12/12 [00:02<00:00,  4.96it/s, loss=1.49, v_num=0, val_loss=1.630, val_acc_epoch=0.227, train_loss_step=1.430, train_loss_epoch=1.520, train_acc_epoch=0.310]\n","Epoch 13: 100% 12/12 [00:02<00:00,  4.77it/s, loss=1.49, v_num=0, val_loss=1.620, val_acc_epoch=0.227, train_loss_step=1.480, train_loss_epoch=1.510, train_acc_epoch=0.320]\n","Epoch 14:  67% 8/12 [00:01<00:00,  4.15it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.227, train_loss_step=1.440, train_loss_epoch=1.510, train_acc_epoch=0.320]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.36it/s]\u001b[A\n","Epoch 14:  83% 10/12 [00:02<00:00,  4.24it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.227, train_loss_step=1.440, train_loss_epoch=1.510, train_acc_epoch=0.320]\n","Validating:  60% 3/5 [00:00<00:00,  6.03it/s]\u001b[A\n","Epoch 14: 100% 12/12 [00:02<00:00,  4.59it/s, loss=1.48, v_num=0, val_loss=1.620, val_acc_epoch=0.227, train_loss_step=1.440, train_loss_epoch=1.510, train_acc_epoch=0.320]\n","Epoch 14: 100% 12/12 [00:02<00:00,  4.32it/s, loss=1.48, v_num=0, val_loss=1.610, val_acc_epoch=0.253, train_loss_step=1.270, train_loss_epoch=1.490, train_acc_epoch=0.280]\n","Epoch 15:  67% 8/12 [00:02<00:01,  3.12it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.253, train_loss_step=1.300, train_loss_epoch=1.490, train_acc_epoch=0.280]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.47it/s]\u001b[A\n","Epoch 15:  83% 10/12 [00:02<00:00,  3.36it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.253, train_loss_step=1.300, train_loss_epoch=1.490, train_acc_epoch=0.280]\n","Validating:  60% 3/5 [00:00<00:00,  6.40it/s]\u001b[A\n","Epoch 15: 100% 12/12 [00:03<00:00,  3.57it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.267, train_loss_step=1.400, train_loss_epoch=1.490, train_acc_epoch=0.305]\n","Epoch 16:  67% 8/12 [00:01<00:00,  4.31it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.267, train_loss_step=1.460, train_loss_epoch=1.490, train_acc_epoch=0.305]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.39it/s]\u001b[A\n","Epoch 16:  83% 10/12 [00:02<00:00,  4.38it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.267, train_loss_step=1.460, train_loss_epoch=1.490, train_acc_epoch=0.305]\n","Validating:  60% 3/5 [00:00<00:00,  6.14it/s]\u001b[A\n","Epoch 16: 100% 12/12 [00:02<00:00,  4.71it/s, loss=1.47, v_num=0, val_loss=1.610, val_acc_epoch=0.267, train_loss_step=1.460, train_loss_epoch=1.490, train_acc_epoch=0.305]\n","Epoch 16: 100% 12/12 [00:02<00:00,  4.39it/s, loss=1.47, v_num=0, val_loss=1.600, val_acc_epoch=0.273, train_loss_step=1.290, train_loss_epoch=1.510, train_acc_epoch=0.300]\n","Epoch 17:  67% 8/12 [00:02<00:01,  3.41it/s, loss=1.46, v_num=0, val_loss=1.600, val_acc_epoch=0.273, train_loss_step=1.420, train_loss_epoch=1.510, train_acc_epoch=0.300]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.99it/s]\u001b[A\n","Epoch 17:  83% 10/12 [00:02<00:00,  3.69it/s, loss=1.46, v_num=0, val_loss=1.600, val_acc_epoch=0.273, train_loss_step=1.420, train_loss_epoch=1.510, train_acc_epoch=0.300]\n","Validating:  60% 3/5 [00:00<00:00,  7.10it/s]\u001b[A\n","Epoch 17: 100% 12/12 [00:03<00:00,  3.89it/s, loss=1.46, v_num=0, val_loss=1.600, val_acc_epoch=0.260, train_loss_step=1.100, train_loss_epoch=1.470, train_acc_epoch=0.340]\n","Epoch 18:  67% 8/12 [00:02<00:01,  3.87it/s, loss=1.44, v_num=0, val_loss=1.600, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.470, train_acc_epoch=0.340]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 18:  83% 10/12 [00:02<00:00,  4.31it/s, loss=1.44, v_num=0, val_loss=1.600, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.470, train_acc_epoch=0.340]\n","Epoch 18: 100% 12/12 [00:02<00:00,  4.86it/s, loss=1.44, v_num=0, val_loss=1.600, val_acc_epoch=0.260, train_loss_step=1.390, train_loss_epoch=1.470, train_acc_epoch=0.340]\n","Epoch 18: 100% 12/12 [00:02<00:00,  4.68it/s, loss=1.44, v_num=0, val_loss=1.590, val_acc_epoch=0.253, train_loss_step=1.210, train_loss_epoch=1.470, train_acc_epoch=0.305]\n","Epoch 19:  67% 8/12 [00:01<00:00,  4.57it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.253, train_loss_step=1.410, train_loss_epoch=1.470, train_acc_epoch=0.305]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 19:  83% 10/12 [00:02<00:00,  4.98it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.253, train_loss_step=1.410, train_loss_epoch=1.470, train_acc_epoch=0.305]\n","Epoch 19: 100% 12/12 [00:02<00:00,  5.49it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.253, train_loss_step=1.410, train_loss_epoch=1.470, train_acc_epoch=0.305]\n","Epoch 19: 100% 12/12 [00:02<00:00,  5.18it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.310, train_loss_epoch=1.460, train_acc_epoch=0.305]\n","Epoch 20:  67% 8/12 [00:01<00:00,  4.94it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.360, train_loss_epoch=1.460, train_acc_epoch=0.305]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.29it/s]\u001b[A\n","Epoch 20:  83% 10/12 [00:01<00:00,  5.17it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.360, train_loss_epoch=1.460, train_acc_epoch=0.305]\n","Epoch 20: 100% 12/12 [00:02<00:00,  5.38it/s, loss=1.43, v_num=0, val_loss=1.600, val_acc_epoch=0.267, train_loss_step=1.230, train_loss_epoch=1.460, train_acc_epoch=0.375]\n","Epoch 21:  67% 8/12 [00:01<00:00,  4.32it/s, loss=1.43, v_num=0, val_loss=1.600, val_acc_epoch=0.267, train_loss_step=1.340, train_loss_epoch=1.460, train_acc_epoch=0.375]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 21:  83% 10/12 [00:02<00:00,  4.63it/s, loss=1.43, v_num=0, val_loss=1.600, val_acc_epoch=0.267, train_loss_step=1.340, train_loss_epoch=1.460, train_acc_epoch=0.375]\n","Epoch 21: 100% 12/12 [00:02<00:00,  5.10it/s, loss=1.43, v_num=0, val_loss=1.600, val_acc_epoch=0.267, train_loss_step=1.340, train_loss_epoch=1.460, train_acc_epoch=0.375]\n","Epoch 21: 100% 12/12 [00:02<00:00,  4.86it/s, loss=1.43, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.330, train_loss_epoch=1.430, train_acc_epoch=0.420]\n","Epoch 22:  67% 8/12 [00:01<00:00,  4.32it/s, loss=1.42, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.420]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.75it/s]\u001b[A\n","Epoch 22:  83% 10/12 [00:02<00:00,  4.41it/s, loss=1.42, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.420]\n","Validating:  60% 3/5 [00:00<00:00,  5.85it/s]\u001b[A\n","Epoch 22: 100% 12/12 [00:02<00:00,  4.75it/s, loss=1.42, v_num=0, val_loss=1.590, val_acc_epoch=0.287, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.420]\n","Epoch 22: 100% 12/12 [00:02<00:00,  4.41it/s, loss=1.42, v_num=0, val_loss=1.580, val_acc_epoch=0.313, train_loss_step=1.450, train_loss_epoch=1.440, train_acc_epoch=0.375]\n","Epoch 23:  67% 8/12 [00:02<00:01,  3.41it/s, loss=1.42, v_num=0, val_loss=1.580, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.440, train_acc_epoch=0.375]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.06it/s]\u001b[A\n","Epoch 23:  83% 10/12 [00:02<00:00,  3.67it/s, loss=1.42, v_num=0, val_loss=1.580, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.440, train_acc_epoch=0.375]\n","Validating:  60% 3/5 [00:00<00:00,  6.87it/s]\u001b[A\n","Epoch 23: 100% 12/12 [00:02<00:00,  4.07it/s, loss=1.42, v_num=0, val_loss=1.580, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.440, train_acc_epoch=0.375]\n","Epoch 23: 100% 12/12 [00:03<00:00,  3.87it/s, loss=1.42, v_num=0, val_loss=1.570, val_acc_epoch=0.293, train_loss_step=1.460, train_loss_epoch=1.400, train_acc_epoch=0.450]\n","Epoch 24:  67% 8/12 [00:01<00:00,  4.04it/s, loss=1.41, v_num=0, val_loss=1.570, val_acc_epoch=0.293, train_loss_step=1.260, train_loss_epoch=1.400, train_acc_epoch=0.450]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  6.11it/s]\u001b[A\n","Epoch 24:  83% 10/12 [00:02<00:00,  4.45it/s, loss=1.41, v_num=0, val_loss=1.570, val_acc_epoch=0.293, train_loss_step=1.260, train_loss_epoch=1.400, train_acc_epoch=0.450]\n","Epoch 24: 100% 12/12 [00:02<00:00,  4.74it/s, loss=1.41, v_num=0, val_loss=1.570, val_acc_epoch=0.307, train_loss_step=1.330, train_loss_epoch=1.430, train_acc_epoch=0.395]\n","Epoch 25:  67% 8/12 [00:01<00:00,  4.46it/s, loss=1.4, v_num=0, val_loss=1.570, val_acc_epoch=0.307, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.395]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  6.11it/s]\u001b[A\n","Epoch 25:  83% 10/12 [00:02<00:00,  4.85it/s, loss=1.4, v_num=0, val_loss=1.570, val_acc_epoch=0.307, train_loss_step=1.300, train_loss_epoch=1.430, train_acc_epoch=0.395]\n","Epoch 25: 100% 12/12 [00:02<00:00,  5.20it/s, loss=1.4, v_num=0, val_loss=1.550, val_acc_epoch=0.300, train_loss_step=1.240, train_loss_epoch=1.420, train_acc_epoch=0.355]\n","Epoch 26:  67% 8/12 [00:01<00:00,  4.97it/s, loss=1.41, v_num=0, val_loss=1.550, val_acc_epoch=0.300, train_loss_step=1.320, train_loss_epoch=1.420, train_acc_epoch=0.355]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.57it/s]\u001b[A\n","Epoch 26:  83% 10/12 [00:01<00:00,  5.18it/s, loss=1.41, v_num=0, val_loss=1.550, val_acc_epoch=0.300, train_loss_step=1.320, train_loss_epoch=1.420, train_acc_epoch=0.355]\n","Epoch 26: 100% 12/12 [00:02<00:00,  5.34it/s, loss=1.41, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.350, train_loss_epoch=1.410, train_acc_epoch=0.385]\n","Epoch 27:  67% 8/12 [00:01<00:00,  4.39it/s, loss=1.38, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.410, train_acc_epoch=0.385]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 27:  83% 10/12 [00:02<00:00,  4.71it/s, loss=1.38, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.410, train_acc_epoch=0.385]\n","Epoch 27: 100% 12/12 [00:02<00:00,  5.28it/s, loss=1.38, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.410, train_acc_epoch=0.385]\n","Epoch 27: 100% 12/12 [00:02<00:00,  5.06it/s, loss=1.38, v_num=0, val_loss=1.520, val_acc_epoch=0.300, train_loss_step=1.320, train_loss_epoch=1.390, train_acc_epoch=0.425]\n","Epoch 28:  67% 8/12 [00:01<00:00,  4.75it/s, loss=1.38, v_num=0, val_loss=1.520, val_acc_epoch=0.300, train_loss_step=1.310, train_loss_epoch=1.390, train_acc_epoch=0.425]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.52it/s]\u001b[A\n","Epoch 28:  83% 10/12 [00:02<00:00,  4.75it/s, loss=1.38, v_num=0, val_loss=1.520, val_acc_epoch=0.300, train_loss_step=1.310, train_loss_epoch=1.390, train_acc_epoch=0.425]\n","Validating:  60% 3/5 [00:00<00:00,  5.96it/s]\u001b[A\n","Epoch 28: 100% 12/12 [00:02<00:00,  5.08it/s, loss=1.38, v_num=0, val_loss=1.520, val_acc_epoch=0.300, train_loss_step=1.310, train_loss_epoch=1.390, train_acc_epoch=0.425]\n","Epoch 28: 100% 12/12 [00:02<00:00,  4.72it/s, loss=1.38, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.370, train_acc_epoch=0.415]\n","Epoch 29:  67% 8/12 [00:02<00:01,  3.53it/s, loss=1.37, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.240, train_loss_epoch=1.370, train_acc_epoch=0.415]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.87it/s]\u001b[A\n","Epoch 29:  83% 10/12 [00:02<00:00,  3.77it/s, loss=1.37, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.240, train_loss_epoch=1.370, train_acc_epoch=0.415]\n","Validating:  60% 3/5 [00:00<00:00,  6.72it/s]\u001b[A\n","Epoch 29: 100% 12/12 [00:03<00:00,  3.98it/s, loss=1.37, v_num=0, val_loss=1.550, val_acc_epoch=0.313, train_loss_step=1.320, train_loss_epoch=1.370, train_acc_epoch=0.410]\n","Epoch 30:  67% 8/12 [00:02<00:01,  3.50it/s, loss=1.36, v_num=0, val_loss=1.550, val_acc_epoch=0.313, train_loss_step=1.200, train_loss_epoch=1.370, train_acc_epoch=0.410]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.59it/s]\u001b[A\n","Epoch 30:  83% 10/12 [00:02<00:00,  3.89it/s, loss=1.36, v_num=0, val_loss=1.550, val_acc_epoch=0.313, train_loss_step=1.200, train_loss_epoch=1.370, train_acc_epoch=0.410]\n","Epoch 30: 100% 12/12 [00:02<00:00,  4.25it/s, loss=1.36, v_num=0, val_loss=1.550, val_acc_epoch=0.307, train_loss_step=1.230, train_loss_epoch=1.360, train_acc_epoch=0.410]\n","Epoch 31:  67% 8/12 [00:01<00:00,  4.56it/s, loss=1.33, v_num=0, val_loss=1.550, val_acc_epoch=0.307, train_loss_step=1.240, train_loss_epoch=1.360, train_acc_epoch=0.410]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.56it/s]\u001b[A\n","Epoch 31:  83% 10/12 [00:02<00:00,  4.80it/s, loss=1.33, v_num=0, val_loss=1.550, val_acc_epoch=0.307, train_loss_step=1.240, train_loss_epoch=1.360, train_acc_epoch=0.410]\n","Epoch 31: 100% 12/12 [00:02<00:00,  5.04it/s, loss=1.33, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.040, train_loss_epoch=1.340, train_acc_epoch=0.435]\n","Epoch 32:  67% 8/12 [00:01<00:00,  4.58it/s, loss=1.31, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.210, train_loss_epoch=1.340, train_acc_epoch=0.435]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 32:  83% 10/12 [00:02<00:00,  4.96it/s, loss=1.31, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.210, train_loss_epoch=1.340, train_acc_epoch=0.435]\n","Epoch 32: 100% 12/12 [00:02<00:00,  5.51it/s, loss=1.31, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.210, train_loss_epoch=1.340, train_acc_epoch=0.435]\n","Epoch 32: 100% 12/12 [00:02<00:00,  5.23it/s, loss=1.31, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.240, train_loss_epoch=1.320, train_acc_epoch=0.425]\n","Epoch 33:  67% 8/12 [00:01<00:00,  4.66it/s, loss=1.31, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.220, train_loss_epoch=1.320, train_acc_epoch=0.425]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 33:  83% 10/12 [00:01<00:00,  5.01it/s, loss=1.31, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.220, train_loss_epoch=1.320, train_acc_epoch=0.425]\n","Epoch 33: 100% 12/12 [00:02<00:00,  5.53it/s, loss=1.31, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.220, train_loss_epoch=1.320, train_acc_epoch=0.425]\n","Epoch 33: 100% 12/12 [00:02<00:00,  5.25it/s, loss=1.31, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.130, train_loss_epoch=1.330, train_acc_epoch=0.395]\n","Epoch 34:  67% 8/12 [00:01<00:00,  4.13it/s, loss=1.29, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.330, train_acc_epoch=0.395]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.92it/s]\u001b[A\n","Epoch 34:  83% 10/12 [00:02<00:00,  4.31it/s, loss=1.29, v_num=0, val_loss=1.530, val_acc_epoch=0.300, train_loss_step=1.220, train_loss_epoch=1.330, train_acc_epoch=0.395]\n","Epoch 34: 100% 12/12 [00:02<00:00,  4.49it/s, loss=1.29, v_num=0, val_loss=1.550, val_acc_epoch=0.293, train_loss_step=0.974, train_loss_epoch=1.310, train_acc_epoch=0.420]\n","Epoch 35:  67% 8/12 [00:02<00:01,  3.77it/s, loss=1.29, v_num=0, val_loss=1.550, val_acc_epoch=0.293, train_loss_step=1.120, train_loss_epoch=1.310, train_acc_epoch=0.420]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.40it/s]\u001b[A\n","Epoch 35:  83% 10/12 [00:02<00:00,  3.88it/s, loss=1.29, v_num=0, val_loss=1.550, val_acc_epoch=0.293, train_loss_step=1.120, train_loss_epoch=1.310, train_acc_epoch=0.420]\n","Validating:  60% 3/5 [00:00<00:00,  5.82it/s]\u001b[A\n","Epoch 35: 100% 12/12 [00:03<00:00,  3.99it/s, loss=1.29, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.260, train_loss_epoch=1.290, train_acc_epoch=0.470]\n","Epoch 36:  67% 8/12 [00:01<00:00,  4.08it/s, loss=1.28, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.130, train_loss_epoch=1.290, train_acc_epoch=0.470]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.26it/s]\u001b[A\n","Epoch 36:  83% 10/12 [00:02<00:00,  4.43it/s, loss=1.28, v_num=0, val_loss=1.540, val_acc_epoch=0.300, train_loss_step=1.130, train_loss_epoch=1.290, train_acc_epoch=0.470]\n","Epoch 36: 100% 12/12 [00:02<00:00,  4.73it/s, loss=1.28, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=0.988, train_loss_epoch=1.300, train_acc_epoch=0.475]\n","Epoch 37:  67% 8/12 [00:01<00:00,  4.82it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.400, train_loss_epoch=1.300, train_acc_epoch=0.475]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.96it/s]\u001b[A\n","Epoch 37:  83% 10/12 [00:01<00:00,  5.06it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.400, train_loss_epoch=1.300, train_acc_epoch=0.475]\n","Epoch 37: 100% 12/12 [00:02<00:00,  5.26it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.040, train_loss_epoch=1.320, train_acc_epoch=0.415]\n","Epoch 38:  67% 8/12 [00:01<00:00,  4.84it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.320, train_loss_epoch=1.320, train_acc_epoch=0.415]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.93it/s]\u001b[A\n","Epoch 38:  83% 10/12 [00:01<00:00,  5.09it/s, loss=1.27, v_num=0, val_loss=1.520, val_acc_epoch=0.293, train_loss_step=1.320, train_loss_epoch=1.320, train_acc_epoch=0.415]\n","Epoch 38: 100% 12/12 [00:02<00:00,  5.34it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.220, train_loss_epoch=1.270, train_acc_epoch=0.480]\n","Epoch 39:  67% 8/12 [00:01<00:00,  4.32it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.270, train_acc_epoch=0.480]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 39:  83% 10/12 [00:02<00:00,  4.65it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.270, train_acc_epoch=0.480]\n","Epoch 39: 100% 12/12 [00:02<00:00,  5.14it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.270, train_acc_epoch=0.480]\n","Epoch 39: 100% 12/12 [00:02<00:00,  4.91it/s, loss=1.27, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.120, train_loss_epoch=1.290, train_acc_epoch=0.430]\n","Epoch 40:  67% 8/12 [00:01<00:00,  4.86it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.210, train_loss_epoch=1.290, train_acc_epoch=0.430]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.40it/s]\u001b[A\n","Epoch 40:  83% 10/12 [00:02<00:00,  4.98it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.210, train_loss_epoch=1.290, train_acc_epoch=0.430]\n","Validating:  60% 3/5 [00:00<00:00,  6.62it/s]\u001b[A\n","Epoch 40: 100% 12/12 [00:02<00:00,  5.30it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.307, train_loss_step=1.210, train_loss_epoch=1.290, train_acc_epoch=0.430]\n","Epoch 40: 100% 12/12 [00:02<00:00,  4.92it/s, loss=1.25, v_num=0, val_loss=1.540, val_acc_epoch=0.333, train_loss_step=1.030, train_loss_epoch=1.260, train_acc_epoch=0.405]\n","Epoch 41:  67% 8/12 [00:02<00:01,  3.09it/s, loss=1.25, v_num=0, val_loss=1.540, val_acc_epoch=0.333, train_loss_step=1.190, train_loss_epoch=1.260, train_acc_epoch=0.405]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.46it/s]\u001b[A\n","Epoch 41:  83% 10/12 [00:03<00:00,  3.32it/s, loss=1.25, v_num=0, val_loss=1.540, val_acc_epoch=0.333, train_loss_step=1.190, train_loss_epoch=1.260, train_acc_epoch=0.405]\n","Validating:  60% 3/5 [00:00<00:00,  5.71it/s]\u001b[A\n","Epoch 41: 100% 12/12 [00:03<00:00,  3.65it/s, loss=1.25, v_num=0, val_loss=1.540, val_acc_epoch=0.333, train_loss_step=1.190, train_loss_epoch=1.260, train_acc_epoch=0.405]\n","Epoch 41: 100% 12/12 [00:03<00:00,  3.47it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.230, train_loss_epoch=1.280, train_acc_epoch=0.470]\n","Epoch 42:  67% 8/12 [00:02<00:01,  3.53it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.120, train_loss_epoch=1.280, train_acc_epoch=0.470]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 42:  83% 10/12 [00:02<00:00,  3.89it/s, loss=1.25, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.120, train_loss_epoch=1.280, train_acc_epoch=0.470]\n","Validating:  60% 3/5 [00:00<00:00,  8.19it/s]\u001b[A\n","Epoch 42: 100% 12/12 [00:02<00:00,  4.16it/s, loss=1.25, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.410, train_loss_epoch=1.230, train_acc_epoch=0.475]\n","Epoch 43:  67% 8/12 [00:01<00:00,  5.19it/s, loss=1.26, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.220, train_loss_epoch=1.230, train_acc_epoch=0.475]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 43:  83% 10/12 [00:01<00:00,  5.53it/s, loss=1.26, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.220, train_loss_epoch=1.230, train_acc_epoch=0.475]\n","Epoch 43: 100% 12/12 [00:01<00:00,  6.06it/s, loss=1.26, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.220, train_loss_epoch=1.230, train_acc_epoch=0.475]\n","Epoch 43: 100% 12/12 [00:02<00:00,  5.69it/s, loss=1.26, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.230, train_loss_epoch=1.250, train_acc_epoch=0.505]\n","Epoch 44:  67% 8/12 [00:01<00:00,  4.67it/s, loss=1.24, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.040, train_loss_epoch=1.250, train_acc_epoch=0.505]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 44:  83% 10/12 [00:01<00:00,  5.02it/s, loss=1.24, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.040, train_loss_epoch=1.250, train_acc_epoch=0.505]\n","Epoch 44: 100% 12/12 [00:02<00:00,  5.62it/s, loss=1.24, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.040, train_loss_epoch=1.250, train_acc_epoch=0.505]\n","Epoch 44: 100% 12/12 [00:02<00:00,  5.36it/s, loss=1.24, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.310, train_loss_epoch=1.230, train_acc_epoch=0.490]\n","Epoch 45:  67% 8/12 [00:01<00:00,  4.61it/s, loss=1.22, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.150, train_loss_epoch=1.230, train_acc_epoch=0.490]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 45:  83% 10/12 [00:01<00:00,  5.02it/s, loss=1.22, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.150, train_loss_epoch=1.230, train_acc_epoch=0.490]\n","Epoch 45: 100% 12/12 [00:02<00:00,  5.60it/s, loss=1.22, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.150, train_loss_epoch=1.230, train_acc_epoch=0.490]\n","Epoch 45: 100% 12/12 [00:02<00:00,  5.35it/s, loss=1.22, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.110, train_loss_epoch=1.210, train_acc_epoch=0.525]\n","Epoch 46:  67% 8/12 [00:01<00:00,  4.37it/s, loss=1.23, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.140, train_loss_epoch=1.210, train_acc_epoch=0.525]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.03it/s]\u001b[A\n","Epoch 46:  83% 10/12 [00:02<00:00,  4.53it/s, loss=1.23, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.140, train_loss_epoch=1.210, train_acc_epoch=0.525]\n","Validating:  60% 3/5 [00:00<00:00,  6.58it/s]\u001b[A\n","Epoch 46: 100% 12/12 [00:02<00:00,  4.65it/s, loss=1.23, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.410, train_loss_epoch=1.260, train_acc_epoch=0.445]\n","Epoch 47:  67% 8/12 [00:02<00:01,  3.27it/s, loss=1.21, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.240, train_loss_epoch=1.260, train_acc_epoch=0.445]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.49it/s]\u001b[A\n","Epoch 47:  83% 10/12 [00:02<00:00,  3.49it/s, loss=1.21, v_num=0, val_loss=1.520, val_acc_epoch=0.320, train_loss_step=1.240, train_loss_epoch=1.260, train_acc_epoch=0.445]\n","Validating:  60% 3/5 [00:00<00:00,  5.76it/s]\u001b[A\n","Epoch 47: 100% 12/12 [00:03<00:00,  3.62it/s, loss=1.21, v_num=0, val_loss=1.500, val_acc_epoch=0.320, train_loss_step=1.040, train_loss_epoch=1.200, train_acc_epoch=0.540]\n","Epoch 48:  67% 8/12 [00:02<00:01,  3.37it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.320, train_loss_step=1.070, train_loss_epoch=1.200, train_acc_epoch=0.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 48:  83% 10/12 [00:02<00:00,  3.75it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.320, train_loss_step=1.070, train_loss_epoch=1.200, train_acc_epoch=0.540]\n","Epoch 48: 100% 12/12 [00:02<00:00,  4.24it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.320, train_loss_step=1.070, train_loss_epoch=1.200, train_acc_epoch=0.540]\n","Epoch 48: 100% 12/12 [00:02<00:00,  4.10it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=0.966, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Epoch 49:  67% 8/12 [00:01<00:00,  4.57it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.75it/s]\u001b[A\n","Epoch 49:  83% 10/12 [00:02<00:00,  4.84it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Epoch 49: 100% 12/12 [00:02<00:00,  5.09it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.210, train_loss_epoch=1.160, train_acc_epoch=0.560]\n","Epoch 50:  67% 8/12 [00:01<00:00,  4.24it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.160, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.97it/s]\u001b[A\n","Epoch 50:  83% 10/12 [00:02<00:00,  4.55it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.160, train_acc_epoch=0.560]\n","Epoch 50: 100% 12/12 [00:02<00:00,  4.84it/s, loss=1.16, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.340, train_loss_epoch=1.130, train_acc_epoch=0.535]\n","Epoch 51:  67% 8/12 [00:01<00:00,  4.54it/s, loss=1.17, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.150, train_loss_epoch=1.130, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.86it/s]\u001b[A\n","Epoch 51:  83% 10/12 [00:02<00:00,  4.81it/s, loss=1.17, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.150, train_loss_epoch=1.130, train_acc_epoch=0.535]\n","Epoch 51: 100% 12/12 [00:02<00:00,  5.15it/s, loss=1.17, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.290, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Epoch 52:  67% 8/12 [00:01<00:00,  4.54it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.27it/s]\u001b[A\n","Epoch 52:  83% 10/12 [00:02<00:00,  4.52it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Validating:  60% 3/5 [00:00<00:00,  6.08it/s]\u001b[A\n","Epoch 52: 100% 12/12 [00:02<00:00,  4.92it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.170, train_acc_epoch=0.515]\n","Epoch 52: 100% 12/12 [00:02<00:00,  4.57it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.340, train_loss_step=1.440, train_loss_epoch=1.180, train_acc_epoch=0.515]\n","Epoch 53:  67% 8/12 [00:02<00:01,  3.10it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.340, train_loss_step=1.130, train_loss_epoch=1.180, train_acc_epoch=0.515]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.28it/s]\u001b[A\n","Epoch 53:  83% 10/12 [00:03<00:00,  3.31it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.340, train_loss_step=1.130, train_loss_epoch=1.180, train_acc_epoch=0.515]\n","Validating:  60% 3/5 [00:00<00:00,  6.06it/s]\u001b[A\n","Epoch 53: 100% 12/12 [00:03<00:00,  3.67it/s, loss=1.19, v_num=0, val_loss=1.500, val_acc_epoch=0.340, train_loss_step=1.130, train_loss_epoch=1.180, train_acc_epoch=0.515]\n","Epoch 53: 100% 12/12 [00:03<00:00,  3.46it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.160, train_loss_epoch=1.190, train_acc_epoch=0.500]\n","Epoch 54:  67% 8/12 [00:02<00:01,  3.85it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.110, train_loss_epoch=1.190, train_acc_epoch=0.500]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.55it/s]\u001b[A\n","Epoch 54:  83% 10/12 [00:02<00:00,  4.14it/s, loss=1.19, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.110, train_loss_epoch=1.190, train_acc_epoch=0.500]\n","Epoch 54: 100% 12/12 [00:02<00:00,  4.42it/s, loss=1.19, v_num=0, val_loss=1.550, val_acc_epoch=0.327, train_loss_step=1.160, train_loss_epoch=1.180, train_acc_epoch=0.535]\n","Epoch 55:  67% 8/12 [00:01<00:00,  4.90it/s, loss=1.17, v_num=0, val_loss=1.550, val_acc_epoch=0.327, train_loss_step=1.230, train_loss_epoch=1.180, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 55:  83% 10/12 [00:01<00:00,  5.25it/s, loss=1.17, v_num=0, val_loss=1.550, val_acc_epoch=0.327, train_loss_step=1.230, train_loss_epoch=1.180, train_acc_epoch=0.535]\n","Epoch 55: 100% 12/12 [00:02<00:00,  5.85it/s, loss=1.17, v_num=0, val_loss=1.550, val_acc_epoch=0.327, train_loss_step=1.230, train_loss_epoch=1.180, train_acc_epoch=0.535]\n","Validating: 100% 5/5 [00:00<00:00, 11.28it/s]\u001b[AEpoch 00056: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 55: 100% 12/12 [00:02<00:00,  5.58it/s, loss=1.17, v_num=0, val_loss=1.510, val_acc_epoch=0.327, train_loss_step=1.190, train_loss_epoch=1.150, train_acc_epoch=0.545]\n","Epoch 56:  67% 8/12 [00:01<00:00,  4.48it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.327, train_loss_step=1.040, train_loss_epoch=1.150, train_acc_epoch=0.545]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.86it/s]\u001b[A\n","Epoch 56:  83% 10/12 [00:02<00:00,  4.76it/s, loss=1.16, v_num=0, val_loss=1.510, val_acc_epoch=0.327, train_loss_step=1.040, train_loss_epoch=1.150, train_acc_epoch=0.545]\n","Epoch 56: 100% 12/12 [00:02<00:00,  4.94it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.170, train_loss_epoch=1.150, train_acc_epoch=0.555]\n","Epoch 57:  67% 8/12 [00:01<00:00,  4.86it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.150, train_acc_epoch=0.555]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  6.04it/s]\u001b[A\n","Epoch 57:  83% 10/12 [00:01<00:00,  5.22it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.150, train_acc_epoch=0.555]\n","Epoch 57: 100% 12/12 [00:02<00:00,  5.57it/s, loss=1.16, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.100, train_loss_epoch=1.150, train_acc_epoch=0.560]\n","Epoch 58:  67% 8/12 [00:01<00:00,  4.57it/s, loss=1.13, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.110, train_loss_epoch=1.150, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.47it/s]\u001b[A\n","Epoch 58:  83% 10/12 [00:02<00:00,  4.58it/s, loss=1.13, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.110, train_loss_epoch=1.150, train_acc_epoch=0.560]\n","Validating:  60% 3/5 [00:00<00:00,  6.04it/s]\u001b[A\n","Epoch 58: 100% 12/12 [00:02<00:00,  4.93it/s, loss=1.13, v_num=0, val_loss=1.500, val_acc_epoch=0.313, train_loss_step=1.110, train_loss_epoch=1.150, train_acc_epoch=0.560]\n","Epoch 58: 100% 12/12 [00:02<00:00,  4.59it/s, loss=1.13, v_num=0, val_loss=1.520, val_acc_epoch=0.307, train_loss_step=1.110, train_loss_epoch=1.120, train_acc_epoch=0.565]\n","Epoch 59:  67% 8/12 [00:02<00:01,  3.10it/s, loss=1.13, v_num=0, val_loss=1.520, val_acc_epoch=0.307, train_loss_step=0.883, train_loss_epoch=1.120, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.45it/s]\u001b[A\n","Epoch 59:  83% 10/12 [00:03<00:00,  3.32it/s, loss=1.13, v_num=0, val_loss=1.520, val_acc_epoch=0.307, train_loss_step=0.883, train_loss_epoch=1.120, train_acc_epoch=0.565]\n","Validating:  60% 3/5 [00:00<00:00,  5.93it/s]\u001b[A\n","Epoch 59: 100% 12/12 [00:03<00:00,  3.68it/s, loss=1.13, v_num=0, val_loss=1.520, val_acc_epoch=0.307, train_loss_step=0.883, train_loss_epoch=1.120, train_acc_epoch=0.565]\n","Epoch 59: 100% 12/12 [00:03<00:00,  3.48it/s, loss=1.13, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.230, train_loss_epoch=1.140, train_acc_epoch=0.570]\n","Epoch 60:  67% 8/12 [00:02<00:01,  3.74it/s, loss=1.14, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.220, train_loss_epoch=1.140, train_acc_epoch=0.570]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 60:  83% 10/12 [00:02<00:00,  4.15it/s, loss=1.14, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.220, train_loss_epoch=1.140, train_acc_epoch=0.570]\n","Epoch 60: 100% 12/12 [00:02<00:00,  4.68it/s, loss=1.14, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.220, train_loss_epoch=1.140, train_acc_epoch=0.570]\n","Epoch 60: 100% 12/12 [00:02<00:00,  4.47it/s, loss=1.14, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=0.977, train_loss_epoch=1.160, train_acc_epoch=0.545]\n","Epoch 61:  67% 8/12 [00:01<00:00,  4.32it/s, loss=1.14, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.160, train_acc_epoch=0.545]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.80it/s]\u001b[A\n","Epoch 61:  83% 10/12 [00:02<00:00,  4.59it/s, loss=1.14, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.140, train_loss_epoch=1.160, train_acc_epoch=0.545]\n","Epoch 61: 100% 12/12 [00:02<00:00,  4.84it/s, loss=1.14, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.120, train_loss_epoch=1.130, train_acc_epoch=0.545]\n","Epoch 62:  67% 8/12 [00:01<00:00,  4.91it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.010, train_loss_epoch=1.130, train_acc_epoch=0.545]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 62:  83% 10/12 [00:01<00:00,  5.27it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.010, train_loss_epoch=1.130, train_acc_epoch=0.545]\n","Epoch 62: 100% 12/12 [00:02<00:00,  5.84it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.010, train_loss_epoch=1.130, train_acc_epoch=0.545]\n","Epoch 62: 100% 12/12 [00:02<00:00,  5.53it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=1.020, train_loss_epoch=1.110, train_acc_epoch=0.560]\n","Epoch 63:  67% 8/12 [00:01<00:00,  5.00it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=0.978, train_loss_epoch=1.110, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.98it/s]\u001b[A\n","Epoch 63:  83% 10/12 [00:01<00:00,  5.18it/s, loss=1.12, v_num=0, val_loss=1.510, val_acc_epoch=0.320, train_loss_step=0.978, train_loss_epoch=1.110, train_acc_epoch=0.560]\n","Epoch 63: 100% 12/12 [00:02<00:00,  5.32it/s, loss=1.12, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.210, train_loss_epoch=1.130, train_acc_epoch=0.525]\n","Epoch 64:  67% 8/12 [00:01<00:00,  5.02it/s, loss=1.1, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.100, train_loss_epoch=1.130, train_acc_epoch=0.525]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.35it/s]\u001b[A\n","Epoch 64:  83% 10/12 [00:01<00:00,  5.21it/s, loss=1.1, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.100, train_loss_epoch=1.130, train_acc_epoch=0.525]\n","Validating:  60% 3/5 [00:00<00:00,  7.49it/s]\u001b[A\n","Epoch 64: 100% 12/12 [00:02<00:00,  5.63it/s, loss=1.1, v_num=0, val_loss=1.520, val_acc_epoch=0.313, train_loss_step=1.100, train_loss_epoch=1.130, train_acc_epoch=0.525]\n","Epoch 64: 100% 12/12 [00:02<00:00,  5.19it/s, loss=1.1, v_num=0, val_loss=1.520, val_acc_epoch=0.327, train_loss_step=0.700, train_loss_epoch=1.100, train_acc_epoch=0.565]\n","Epoch 65:  67% 8/12 [00:02<00:01,  3.17it/s, loss=1.11, v_num=0, val_loss=1.520, val_acc_epoch=0.327, train_loss_step=1.060, train_loss_epoch=1.100, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.52it/s]\u001b[A\n","Epoch 65:  83% 10/12 [00:02<00:00,  3.40it/s, loss=1.11, v_num=0, val_loss=1.520, val_acc_epoch=0.327, train_loss_step=1.060, train_loss_epoch=1.100, train_acc_epoch=0.565]\n","Validating:  60% 3/5 [00:00<00:00,  6.59it/s]\u001b[A\n","Epoch 65: 100% 12/12 [00:03<00:00,  3.63it/s, loss=1.11, v_num=0, val_loss=1.520, val_acc_epoch=0.333, train_loss_step=1.010, train_loss_epoch=1.150, train_acc_epoch=0.540]\n","Epoch 66:  67% 8/12 [00:02<00:01,  3.69it/s, loss=1.09, v_num=0, val_loss=1.520, val_acc_epoch=0.333, train_loss_step=1.070, train_loss_epoch=1.150, train_acc_epoch=0.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 66:  83% 10/12 [00:02<00:00,  4.11it/s, loss=1.09, v_num=0, val_loss=1.520, val_acc_epoch=0.333, train_loss_step=1.070, train_loss_epoch=1.150, train_acc_epoch=0.540]\n","Epoch 66: 100% 12/12 [00:02<00:00,  4.63it/s, loss=1.09, v_num=0, val_loss=1.520, val_acc_epoch=0.333, train_loss_step=1.070, train_loss_epoch=1.150, train_acc_epoch=0.540]\n","Epoch 66: 100% 12/12 [00:02<00:00,  4.43it/s, loss=1.09, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.050, train_loss_epoch=1.100, train_acc_epoch=0.535]\n","Epoch 67:  67% 8/12 [00:01<00:00,  4.27it/s, loss=1.13, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.160, train_loss_epoch=1.100, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.76it/s]\u001b[A\n","Epoch 67:  83% 10/12 [00:02<00:00,  4.58it/s, loss=1.13, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.160, train_loss_epoch=1.100, train_acc_epoch=0.535]\n","Epoch 67: 100% 12/12 [00:02<00:00,  5.09it/s, loss=1.13, v_num=0, val_loss=1.530, val_acc_epoch=0.327, train_loss_step=1.160, train_loss_epoch=1.100, train_acc_epoch=0.535]Epoch 00068: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 67: 100% 12/12 [00:02<00:00,  4.84it/s, loss=1.13, v_num=0, val_loss=1.540, val_acc_epoch=0.327, train_loss_step=1.590, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Epoch 68:  67% 8/12 [00:01<00:00,  4.14it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.327, train_loss_step=0.902, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.95it/s]\u001b[A\n","Epoch 68:  83% 10/12 [00:02<00:00,  4.46it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.327, train_loss_step=0.902, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Epoch 68: 100% 12/12 [00:02<00:00,  4.67it/s, loss=1.11, v_num=0, val_loss=1.530, val_acc_epoch=0.333, train_loss_step=0.978, train_loss_epoch=1.070, train_acc_epoch=0.570]\n","Epoch 69:  67% 8/12 [00:01<00:00,  4.33it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.333, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.570]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.66it/s]\u001b[A\n","Epoch 69:  83% 10/12 [00:02<00:00,  4.59it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.333, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.570]\n","Epoch 69: 100% 12/12 [00:02<00:00,  4.86it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.320, train_loss_step=0.919, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Epoch 70:  67% 8/12 [00:01<00:00,  4.17it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.320, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.11it/s]\u001b[A\n","Epoch 70:  83% 10/12 [00:02<00:00,  4.40it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.320, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Validating:  60% 3/5 [00:00<00:00,  7.18it/s]\u001b[A\n","Epoch 70: 100% 12/12 [00:02<00:00,  4.54it/s, loss=1.1, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.310, train_loss_epoch=1.140, train_acc_epoch=0.530]\n","Epoch 71:  67% 8/12 [00:02<00:01,  3.19it/s, loss=1.11, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.140, train_acc_epoch=0.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.31it/s]\u001b[A\n","Epoch 71:  83% 10/12 [00:02<00:00,  3.38it/s, loss=1.11, v_num=0, val_loss=1.530, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.140, train_acc_epoch=0.530]\n","Validating:  60% 3/5 [00:00<00:00,  5.90it/s]\u001b[A\n","Epoch 71: 100% 12/12 [00:03<00:00,  3.58it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.858, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Epoch 72:  67% 8/12 [00:02<00:01,  3.87it/s, loss=1.12, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.030, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.89it/s]\u001b[A\n","Epoch 72:  83% 10/12 [00:02<00:00,  4.21it/s, loss=1.12, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.030, train_loss_epoch=1.120, train_acc_epoch=0.535]\n","Epoch 72: 100% 12/12 [00:02<00:00,  4.51it/s, loss=1.12, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.970, train_loss_epoch=1.140, train_acc_epoch=0.520]\n","Epoch 73:  67% 8/12 [00:01<00:00,  4.28it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.140, train_loss_epoch=1.140, train_acc_epoch=0.520]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.97it/s]\u001b[A\n","Epoch 73:  83% 10/12 [00:02<00:00,  4.59it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.140, train_loss_epoch=1.140, train_acc_epoch=0.520]\n","Epoch 73: 100% 12/12 [00:02<00:00,  4.94it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.661, train_loss_epoch=1.090, train_acc_epoch=0.540]\n","Epoch 74:  67% 8/12 [00:01<00:00,  5.00it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.160, train_loss_epoch=1.090, train_acc_epoch=0.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 74:  83% 10/12 [00:01<00:00,  5.36it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.160, train_loss_epoch=1.090, train_acc_epoch=0.540]\n","Epoch 74: 100% 12/12 [00:02<00:00,  5.95it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.160, train_loss_epoch=1.090, train_acc_epoch=0.540]\n","Epoch 74: 100% 12/12 [00:02<00:00,  5.66it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.110, train_loss_epoch=1.110, train_acc_epoch=0.540]\n","Epoch 75:  67% 8/12 [00:01<00:00,  5.02it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.110, train_acc_epoch=0.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 75:  83% 10/12 [00:01<00:00,  5.38it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.110, train_acc_epoch=0.540]\n","Epoch 75: 100% 12/12 [00:02<00:00,  5.97it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.160, train_loss_epoch=1.110, train_acc_epoch=0.540]\n","Epoch 75: 100% 12/12 [00:02<00:00,  5.70it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.882, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Epoch 76:  67% 8/12 [00:01<00:00,  5.23it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.080, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.40it/s]\u001b[A\n","Epoch 76:  83% 10/12 [00:01<00:00,  5.43it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.080, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Validating:  60% 3/5 [00:00<00:00,  7.78it/s]\u001b[A\n","Epoch 76: 100% 12/12 [00:02<00:00,  5.48it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.815, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Epoch 77:  67% 8/12 [00:02<00:01,  3.15it/s, loss=1.05, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.985, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.94it/s]\u001b[A\n","Epoch 77:  83% 10/12 [00:02<00:00,  3.34it/s, loss=1.05, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.985, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Validating:  60% 3/5 [00:00<00:00,  6.10it/s]\u001b[A\n","Epoch 77: 100% 12/12 [00:03<00:00,  3.58it/s, loss=1.05, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.985, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 78:  67% 8/12 [00:02<00:01,  3.18it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.92it/s]\u001b[A\n","Epoch 78:  83% 10/12 [00:02<00:00,  3.34it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating:  60% 3/5 [00:00<00:00,  5.49it/s]\u001b[A\n","Epoch 78: 100% 12/12 [00:03<00:00,  3.68it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.060, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating: 100% 5/5 [00:00<00:00,  7.04it/s]\u001b[AEpoch 00079: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 78: 100% 12/12 [00:03<00:00,  3.49it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.969, train_loss_epoch=1.080, train_acc_epoch=0.550]\n","Epoch 79:  67% 8/12 [00:02<00:01,  2.83it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.992, train_loss_epoch=1.080, train_acc_epoch=0.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  2.90it/s]\u001b[A\n","Epoch 79:  83% 10/12 [00:03<00:00,  3.00it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.992, train_loss_epoch=1.080, train_acc_epoch=0.550]\n","Validating:  60% 3/5 [00:00<00:00,  5.30it/s]\u001b[A\n","Epoch 79: 100% 12/12 [00:03<00:00,  3.34it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.992, train_loss_epoch=1.080, train_acc_epoch=0.550]\n","Epoch 79: 100% 12/12 [00:03<00:00,  3.16it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.883, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 80:  67% 8/12 [00:01<00:00,  4.72it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.905, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 80:  83% 10/12 [00:02<00:00,  4.94it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.905, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 80: 100% 12/12 [00:02<00:00,  5.49it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.905, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 80: 100% 12/12 [00:02<00:00,  5.19it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.140, train_loss_epoch=1.120, train_acc_epoch=0.575]\n","Epoch 81:  67% 8/12 [00:01<00:00,  4.74it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.250, train_loss_epoch=1.120, train_acc_epoch=0.575]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 81:  83% 10/12 [00:01<00:00,  5.07it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.250, train_loss_epoch=1.120, train_acc_epoch=0.575]\n","Epoch 81: 100% 12/12 [00:02<00:00,  5.58it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.250, train_loss_epoch=1.120, train_acc_epoch=0.575]\n","Epoch 81: 100% 12/12 [00:02<00:00,  5.29it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.250, train_loss_epoch=1.130, train_acc_epoch=0.520]\n","Epoch 82:  67% 8/12 [00:01<00:00,  4.46it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.130, train_acc_epoch=0.520]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 82:  83% 10/12 [00:02<00:00,  4.84it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.130, train_acc_epoch=0.520]\n","Epoch 82: 100% 12/12 [00:02<00:00,  5.42it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.060, train_loss_epoch=1.130, train_acc_epoch=0.520]\n","Epoch 82: 100% 12/12 [00:02<00:00,  5.15it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.080, train_loss_epoch=1.080, train_acc_epoch=0.565]\n","Epoch 83:  67% 8/12 [00:01<00:00,  5.01it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.190, train_loss_epoch=1.080, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.66it/s]\u001b[A\n","Epoch 83:  83% 10/12 [00:01<00:00,  5.22it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.190, train_loss_epoch=1.080, train_acc_epoch=0.565]\n","Epoch 83: 100% 12/12 [00:02<00:00,  5.66it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.190, train_loss_epoch=1.080, train_acc_epoch=0.565]\n","Epoch 83: 100% 12/12 [00:02<00:00,  5.35it/s, loss=1.11, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.989, train_loss_epoch=1.090, train_acc_epoch=0.595]\n","Epoch 84:  67% 8/12 [00:02<00:01,  3.15it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.120, train_loss_epoch=1.090, train_acc_epoch=0.595]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.22it/s]\u001b[A\n","Epoch 84:  83% 10/12 [00:02<00:00,  3.46it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.120, train_loss_epoch=1.090, train_acc_epoch=0.595]\n","Validating:  60% 3/5 [00:00<00:00,  6.93it/s]\u001b[A\n","Epoch 84: 100% 12/12 [00:03<00:00,  3.69it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=0.888, train_loss_epoch=1.070, train_acc_epoch=0.550]\n","Epoch 85:  67% 8/12 [00:02<00:01,  3.28it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 85:  83% 10/12 [00:02<00:00,  3.58it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.550]\n","Epoch 85: 100% 12/12 [00:02<00:00,  4.02it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.090, train_loss_epoch=1.070, train_acc_epoch=0.550]\n","Epoch 85: 100% 12/12 [00:03<00:00,  3.88it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.170, train_loss_epoch=1.090, train_acc_epoch=0.570]\n","Epoch 86:  67% 8/12 [00:01<00:00,  4.34it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.090, train_acc_epoch=0.570]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 86:  83% 10/12 [00:02<00:00,  4.68it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.090, train_acc_epoch=0.570]\n","Epoch 86: 100% 12/12 [00:02<00:00,  5.23it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.090, train_acc_epoch=0.570]\n","Epoch 86: 100% 12/12 [00:02<00:00,  5.02it/s, loss=1.07, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.030, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Epoch 87:  67% 8/12 [00:01<00:00,  4.36it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.030, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  5.01it/s]\u001b[A\n","Epoch 87:  83% 10/12 [00:02<00:00,  4.68it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.030, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Epoch 87: 100% 12/12 [00:02<00:00,  5.17it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.030, train_loss_epoch=1.080, train_acc_epoch=0.555]\n","Epoch 87: 100% 12/12 [00:02<00:00,  4.90it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.951, train_loss_epoch=1.090, train_acc_epoch=0.565]\n","Epoch 88:  67% 8/12 [00:01<00:00,  4.79it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.090, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 88:  83% 10/12 [00:01<00:00,  5.03it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.090, train_acc_epoch=0.565]\n","Epoch 88: 100% 12/12 [00:02<00:00,  5.49it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.090, train_loss_epoch=1.090, train_acc_epoch=0.565]\n","Epoch 88: 100% 12/12 [00:02<00:00,  5.20it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.000, train_loss_epoch=1.100, train_acc_epoch=0.585]\n","Epoch 89:  67% 8/12 [00:01<00:00,  4.19it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.080, train_loss_epoch=1.100, train_acc_epoch=0.585]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.57it/s]\u001b[A\n","Epoch 89:  83% 10/12 [00:02<00:00,  4.48it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.080, train_loss_epoch=1.100, train_acc_epoch=0.585]\n","Epoch 89: 100% 12/12 [00:02<00:00,  4.96it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.080, train_loss_epoch=1.100, train_acc_epoch=0.585]\n","Validating: 100% 5/5 [00:00<00:00,  8.82it/s]\u001b[AEpoch 00090: reducing learning rate of group 0 to 6.2500e-06.\n","Epoch 89: 100% 12/12 [00:02<00:00,  4.62it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.100, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 90:  67% 8/12 [00:02<00:01,  3.08it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.100, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.08it/s]\u001b[A\n","Epoch 90:  83% 10/12 [00:03<00:00,  3.26it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.100, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Validating:  60% 3/5 [00:00<00:00,  5.55it/s]\u001b[A\n","Epoch 90: 100% 12/12 [00:03<00:00,  3.60it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.100, train_loss_epoch=1.080, train_acc_epoch=0.575]\n","Epoch 90: 100% 12/12 [00:03<00:00,  3.40it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.170, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 91:  67% 8/12 [00:02<00:01,  3.19it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.040, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.95it/s]\u001b[A\n","Epoch 91:  83% 10/12 [00:02<00:00,  3.54it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.307, train_loss_step=1.040, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 91: 100% 12/12 [00:03<00:00,  3.88it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.010, train_loss_epoch=1.070, train_acc_epoch=0.565]\n","Epoch 92:  67% 8/12 [00:01<00:00,  4.62it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.070, train_loss_epoch=1.070, train_acc_epoch=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 92:  83% 10/12 [00:02<00:00,  4.93it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.070, train_loss_epoch=1.070, train_acc_epoch=0.565]\n","Epoch 92: 100% 12/12 [00:02<00:00,  5.48it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.070, train_loss_epoch=1.070, train_acc_epoch=0.565]\n","Epoch 92: 100% 12/12 [00:02<00:00,  5.25it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.969, train_loss_epoch=1.130, train_acc_epoch=0.500]\n","Epoch 93:  67% 8/12 [00:01<00:00,  5.20it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.090, train_loss_epoch=1.130, train_acc_epoch=0.500]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  6.10it/s]\u001b[A\n","Epoch 93:  83% 10/12 [00:01<00:00,  5.52it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.090, train_loss_epoch=1.130, train_acc_epoch=0.500]\n","Epoch 93: 100% 12/12 [00:02<00:00,  5.62it/s, loss=1.09, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.946, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Epoch 94:  67% 8/12 [00:01<00:00,  4.78it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.938, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.44it/s]\u001b[A\n","Epoch 94:  83% 10/12 [00:02<00:00,  4.97it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.938, train_loss_epoch=1.080, train_acc_epoch=0.560]\n","Epoch 94: 100% 12/12 [00:02<00:00,  5.16it/s, loss=1.08, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=1.130, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Epoch 95:  67% 8/12 [00:01<00:00,  4.17it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.942, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.89it/s]\u001b[A\n","Epoch 95:  83% 10/12 [00:02<00:00,  4.49it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.942, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Validating:  60% 3/5 [00:00<00:00,  7.45it/s]\u001b[A\n","Epoch 95: 100% 12/12 [00:02<00:00,  4.86it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.942, train_loss_epoch=1.060, train_acc_epoch=0.585]\n","Epoch 95: 100% 12/12 [00:02<00:00,  4.60it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.956, train_loss_epoch=1.030, train_acc_epoch=0.625]\n","Epoch 96:  67% 8/12 [00:02<00:01,  3.25it/s, loss=1.03, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.902, train_loss_epoch=1.030, train_acc_epoch=0.625]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:01,  3.50it/s]\u001b[A\n","Epoch 96:  83% 10/12 [00:02<00:00,  3.47it/s, loss=1.03, v_num=0, val_loss=1.540, val_acc_epoch=0.313, train_loss_step=0.902, train_loss_epoch=1.030, train_acc_epoch=0.625]\n","Validating:  60% 3/5 [00:00<00:00,  6.44it/s]\u001b[A\n","Epoch 96: 100% 12/12 [00:03<00:00,  3.67it/s, loss=1.03, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.140, train_loss_epoch=1.030, train_acc_epoch=0.610]\n","Epoch 97:  67% 8/12 [00:02<00:01,  3.38it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.030, train_acc_epoch=0.610]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.88it/s]\u001b[A\n","Epoch 97:  83% 10/12 [00:02<00:00,  3.73it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.050, train_loss_epoch=1.030, train_acc_epoch=0.610]\n","Epoch 97: 100% 12/12 [00:03<00:00,  3.99it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.230, train_loss_epoch=1.070, train_acc_epoch=0.610]\n","Epoch 98:  67% 8/12 [00:01<00:00,  4.84it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.060, train_loss_epoch=1.070, train_acc_epoch=0.610]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Validating:  20% 1/5 [00:00<00:00,  4.86it/s]\u001b[A\n","Epoch 98:  83% 10/12 [00:01<00:00,  5.08it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=1.060, train_loss_epoch=1.070, train_acc_epoch=0.610]\n","Epoch 98: 100% 12/12 [00:02<00:00,  5.29it/s, loss=1.06, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.858, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 99:  67% 8/12 [00:01<00:00,  4.88it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.923, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n","Epoch 99:  83% 10/12 [00:01<00:00,  5.27it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.923, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 99: 100% 12/12 [00:02<00:00,  5.84it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.923, train_loss_epoch=1.040, train_acc_epoch=0.610]\n","Epoch 99: 100% 12/12 [00:02<00:00,  5.56it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.796, train_loss_epoch=1.050, train_acc_epoch=0.620]\n","Epoch 99: 100% 12/12 [00:02<00:00,  5.55it/s, loss=1.04, v_num=0, val_loss=1.540, val_acc_epoch=0.320, train_loss_step=0.796, train_loss_epoch=1.050, train_acc_epoch=0.620]\n","Testing: 100% 7/7 [00:00<00:00, 11.12it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test_acc_best_epoch': 0.42500001192092896, 'test_loss': 1.3911406993865967}\n","--------------------------------------------------------------------------------\n","Path to best model found during training: \n","/content/outputs/2023-06-11/20-35-11/lightning_logs/version_0/checkpoints/56-398.ckpt\n"]}]},{"cell_type":"markdown","source":["## Supervised learning only"],"metadata":{"id":"NyPOPTmDOePz"}},{"cell_type":"code","source":["# bare SUP\n","\n","!python /content/saint/main.py experiment=supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_sup"],"metadata":{"id":"dEoRCf56GlGD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686513770596,"user_tz":-420,"elapsed":2508,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"0e6d4763-99ab-4da9-e88d-25b9f4f15ece"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/saint/main.py\", line 4, in <module>\n","    from pytorch_lightning import seed_everything\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/__init__.py\", line 20, in <module>\n","    from pytorch_lightning import metrics  # noqa: E402\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/metrics/__init__.py\", line 15, in <module>\n","    from pytorch_lightning.metrics.classification import (  # noqa: F401\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/metrics/classification/__init__.py\", line 14, in <module>\n","    from pytorch_lightning.metrics.classification.accuracy import Accuracy  # noqa: F401\n","  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/metrics/classification/accuracy.py\", line 16, in <module>\n","    from torchmetrics import Accuracy as _Accuracy\n","  File \"/usr/local/lib/python3.10/dist-packages/torchmetrics/__init__.py\", line 14, in <module>\n","    from torchmetrics.average import AverageMeter  # noqa: F401 E402\n","  File \"/usr/local/lib/python3.10/dist-packages/torchmetrics/average.py\", line 16, in <module>\n","    import torch\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1465, in <module>\n","    from . import _meta_registrations\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 7, in <module>\n","    from torch._decomp import _add_op_to_registry, global_decomposition_table, meta_table\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 169, in <module>\n","    import torch._decomp.decompositions\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/decompositions.py\", line 10, in <module>\n","    import torch._prims as prims\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\", line 33, in <module>\n","    from torch._subclasses.fake_tensor import FakeTensor, FakeTensorMode\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/__init__.py\", line 3, in <module>\n","    from torch._subclasses.fake_tensor import (\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 13, in <module>\n","    from torch._guards import Source\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_guards.py\", line 14, in <module>\n","    import sympy  # type: ignore[import]\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\", line 196, in <module>\n","    from .geometry import (Point, Point2D, Point3D, Line, Ray, Segment, Line2D,\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/geometry/__init__.py\", line 22, in <module>\n","    from sympy.geometry.curve import Curve\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 937, in _find_spec\n","  File \"<frozen importlib._bootstrap>\", line 893, in __enter__\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"gvrjFwH2JpA-"}},{"cell_type":"code","source":["pretrained_checkpoint = \"/content/outputs/2023-06-11/20-03-11/lightning_logs/version_0/checkpoints/56-398.ckpt\""],"metadata":{"id":"pbNGb90_qzMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python /content/saint/predict.py experiment=predict \\\n","  experiment.model=saint \\\n","  data=bank_sup \\\n","  data.data_folder=/content/saint/data \\\n","  experiment.pretrained_checkpoint={pretrained_checkpoint} \\\n","  experiment.pred_sav_path=/content/predict.csv"],"metadata":{"id":"2BkcD_bOHJlX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686514897120,"user_tz":-420,"elapsed":10723,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"82f1be31-e3c8-4d4d-93ae-063477d8e21e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 20:21:32.260458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-11 20:21:33.540769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/predict.py:15: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","{'seed': 1234, 'transformer': {'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'dropout_ff': 0.1, 'embed_dim': 32, 'd_ff': 32, 'cls_token_idx': 0}, 'augmentation': {'prob_cutmix': 0.3, 'alpha': 0.2, 'lambda_pt': 10}, 'optimizer': {'temperature': 0.7, 'proj_head_dim': 128, 'beta_1': 0.9, 'beta_2': 0.99, 'lr': 0.0001, 'weight_decay': 0.01, 'optim': 'adamw', 'metric': 'auroc'}, 'preproc': {'data_folder': None, 'train_split': 0.65, 'validation_split': 0.15, 'test_split': 0.2, 'num_supervised_train_data': None}, 'callback': {'monitor': 'val_loss', 'mode': 'min', 'auto_insert_metric_name': False}, 'trainer': {'max_epochs': 100, 'deterministic': True, 'default_root_dir': None}, 'dataloader': {'shuffle_val': False, 'train_bs': 32, 'val_bs': 32, 'test_bs': 16, 'num_workers': 2, 'pin_memory': False}, 'metric': '${optimizer.metric}', 'print_config': False, 'experiment': {'model': 'saint', 'task': 'classification', 'pretrained_checkpoint': '/content/outputs/2023-06-11/20-03-11/lightning_logs/version_0/checkpoints/56-398.ckpt', 'num_output': 1, 'pred_sav_path': '/content/predict.csv', 'save_prediction': True, 'id_col': 'index', 'target_col': 'target'}, 'data': {'data_folder': '/content/saint/data', 'data_paths': {'train_csv_path': '${data.data_folder}/train.csv', 'train_y_csv_path': '${data.data_folder}/train_y.csv', 'val_csv_path': '${data.data_folder}/val.csv', 'val_y_csv_path': '${data.data_folder}/val_y.csv', 'test_csv_path': '${data.data_folder}/test.csv', 'test_y_csv_path': '${data.data_folder}/test_y.csv'}, 'data_stats': {'no_cat': 1, 'no_num': 49, 'cats': [1]}}}\n","/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)\n","Prediction finished,  csv saved at /content/predict.csv\n"]}]},{"cell_type":"code","source":["pred = pd.read_csv(\"/content/predict.csv\")\n","pred['target'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkms-sdgJFYA","executionInfo":{"status":"ok","timestamp":1686515329523,"user_tz":-420,"elapsed":1064,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"e5726bc0-423d-45da-d0f0-3dd93a68dac3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    160\n","1     40\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":[],"metadata":{"id":"V9pdLifpJ0ug"},"execution_count":null,"outputs":[]}]}