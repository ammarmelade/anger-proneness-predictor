{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Pspfu6nqi5IPQ5E_0xSDhcH9PLYjyy9i","timestamp":1686545832569}],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Preliminaries"],"metadata":{"id":"6ZiHKXJ0JQ2y"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"],"metadata":{"id":"irMRi0gI9vH_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686537383641,"user_tz":-420,"elapsed":27357,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"0d4d974e-27dc-46d9-8822-413dd840dc42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wF_EZuxx8-UB","executionInfo":{"status":"ok","timestamp":1686537397592,"user_tz":-420,"elapsed":2683,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"a8e6053a-a8b3-46bc-f922-c672bc25ca4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'saint'...\n","remote: Enumerating objects: 664, done.\u001b[K\n","remote: Total 664 (delta 0), reused 0 (delta 0), pack-reused 664\u001b[K\n","Receiving objects: 100% (664/664), 17.00 MiB | 25.60 MiB/s, done.\n","Resolving deltas: 100% (364/364), done.\n"]}],"source":["!git clone https://github.com/ogunlao/saint.git"]},{"cell_type":"code","source":["!pip install pytorch-lightning==1.3.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZ4VpFXm7GDp","executionInfo":{"status":"ok","timestamp":1686537418194,"user_tz":-420,"elapsed":18824,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"f55fcbe6-90fb-4377-88f6-1187c530a5da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-lightning==1.3.2\n","  Downloading pytorch_lightning-1.3.2-py3-none-any.whl (805 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.7/805.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (1.22.4)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2.0.1+cu118)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (0.18.3)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (4.65.0)\n","Collecting PyYAML<=5.4.1,>=5.1 (from pytorch-lightning==1.3.2)\n","  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2023.4.0)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (2.12.2)\n","Collecting torchmetrics>=0.2.0 (from pytorch-lightning==1.3.2)\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyDeprecate==0.3.0 (from pytorch-lightning==1.3.2)\n","  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.3.2) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2.27.1)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.54.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.4.3)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch-lightning==1.3.2) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->pytorch-lightning==1.3.2) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->pytorch-lightning==1.3.2) (16.0.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.2) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->pytorch-lightning==1.3.2) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.2) (3.2.2)\n","Building wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=34809655ea94a2bb85d250b43eec1049348b63167722ee4827a0fdf5009bf6ec\n","  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n","Successfully built PyYAML\n","Installing collected packages: PyYAML, pyDeprecate, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","Successfully installed PyYAML-5.4.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 pyDeprecate-0.3.0 pytorch-lightning-1.3.2 torchmetrics-0.11.4 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["# !pip3 install -r \"/content/saint/requirements.txt\""],"metadata":{"id":"0ECBnzW59KNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scikit-learn==0.24.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czKgW7eW7lTi","executionInfo":{"status":"ok","timestamp":1686537681458,"user_tz":-420,"elapsed":263268,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"2737ac1e-0ab8-4ff7-9c5d-71e42a53d142"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-learn==0.24.2\n","  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.22.4)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.10.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.2) (3.1.0)\n","Building wheels for collected packages: scikit-learn\n","  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0mTraceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 417, in run\n","    _, build_failures = build(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 320, in build\n","    wheel_file = _build_one(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 194, in _build_one\n","    wheel_path = _build_one_inside_env(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/wheel_builder.py\", line 234, in _build_one_inside_env\n","    wheel_path = build_wheel_pep517(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/build/wheel.py\", line 30, in build_wheel_pep517\n","    wheel_name = backend.build_wheel(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 665, in build_wheel\n","    return super().build_wheel(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 209, in build_wheel\n","    return self._call_hook('build_wheel', {\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 311, in _call_hook\n","    self._subprocess_runner(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/subprocess.py\", line 252, in runner\n","    call_subprocess(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/subprocess.py\", line 166, in call_subprocess\n","    line: str = proc.stdout.readline()\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n","    return run(options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 207, in exc_logging_wrapper\n","    logger.debug(\"Exception information:\", exc_info=True)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n","    self._log(DEBUG, msg, args, **kwargs)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n","    self.handle(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n","    self.callHandlers(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n","    hdlr.handle(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n","    self.emit(record)\n","  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n","    logging.FileHandler.emit(self, record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n","    StreamHandler.emit(self, record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n","    msg = self.format(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n","    return fmt.format(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n","    formatted = super().format(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n","    record.exc_text = self.formatException(record.exc_info)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n","    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n","  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n","    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n","  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n","    self.stack = StackSummary.extract(\n","  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n","    f.line\n","  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n","    self._line = linecache.getline(self.filename, self.lineno)\n","  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n","    lines = getlines(filename, module_globals)\n","  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n","    return updatecache(filename, module_globals)\n","  File \"/usr/lib/python3.10/linecache.py\", line 136, in updatecache\n","    with tokenize.open(fullname) as fp:\n","  File \"/usr/lib/python3.10/tokenize.py\", line 396, in open\n","    encoding, lines = detect_encoding(buffer.readline)\n","  File \"/usr/lib/python3.10/tokenize.py\", line 365, in detect_encoding\n","    first = read_or_stop()\n","  File \"/usr/lib/python3.10/tokenize.py\", line 323, in read_or_stop\n","    return readline()\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":["!pip install scipy==1.5.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdZC8Xhx7rEc","executionInfo":{"status":"ok","timestamp":1686537811881,"user_tz":-420,"elapsed":130436,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"5fe2bf5c-a766-466c-8e2f-3e7e203a5a22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scipy==1.5.4\n","  Downloading scipy-1.5.4.tar.gz (25.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/25.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"code","source":["!pip install tensorboard==2.4.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"QzrvzYGU7q_C","executionInfo":{"status":"ok","timestamp":1686537823418,"user_tz":-420,"elapsed":11569,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"dd041205-966f-42f3-8d48-202db4563431"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboard==2.4.1\n","  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.54.0)\n","Collecting google-auth<2,>=1.6.3 (from tensorboard==2.4.1)\n","  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.4.1)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (3.4.3)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.22.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (2.27.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (67.7.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.16.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.4.1) (0.40.0)\n","Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard==2.4.1)\n","  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.1) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.1) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.1) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard==2.4.1) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.4.1) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.1) (3.2.2)\n","Installing collected packages: cachetools, google-auth, google-auth-oauthlib, tensorboard\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.3.0\n","    Uninstalling cachetools-5.3.0:\n","      Successfully uninstalled cachetools-5.3.0\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 2.17.3\n","    Uninstalling google-auth-2.17.3:\n","      Successfully uninstalled google-auth-2.17.3\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.2\n","    Uninstalling tensorboard-2.12.2:\n","      Successfully uninstalled tensorboard-2.12.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n","google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 1.35.0 which is incompatible.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cachetools-4.2.4 google-auth-1.35.0 google-auth-oauthlib-0.4.6 tensorboard-2.4.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install tensorboard-plugin-wit==1.8.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wNz-mi0s7q0l","executionInfo":{"status":"ok","timestamp":1686537828998,"user_tz":-420,"elapsed":5585,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"563f66c6-1b46-4a02-de7f-6e94b10cd54a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboard-plugin-wit==1.8.0\n","  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.2/781.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard-plugin-wit\n","  Attempting uninstall: tensorboard-plugin-wit\n","    Found existing installation: tensorboard-plugin-wit 1.8.1\n","    Uninstalling tensorboard-plugin-wit-1.8.1:\n","      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tensorboard-plugin-wit-1.8.0\n"]}]},{"cell_type":"code","source":["!pip install torch==1.8.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sk55Z5-d75xp","executionInfo":{"status":"ok","timestamp":1686537830048,"user_tz":-420,"elapsed":1054,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"7562890c-a27b-4ed4-b9e4-c1d9014383a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.1\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install torchmetrics==0.3.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"832RK6OB78_Y","executionInfo":{"status":"ok","timestamp":1686537835246,"user_tz":-420,"elapsed":5201,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"11297661-a39c-4663-c114-104cb0fffcec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics==0.3.2\n","  Downloading torchmetrics-0.3.2-py3-none-any.whl (274 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.3.2) (2.0.1+cu118)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.3.2) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->torchmetrics==0.3.2) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.1->torchmetrics==0.3.2) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.1->torchmetrics==0.3.2) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.1->torchmetrics==0.3.2) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.1->torchmetrics==0.3.2) (1.3.0)\n","Installing collected packages: torchmetrics\n","  Attempting uninstall: torchmetrics\n","    Found existing installation: torchmetrics 0.11.4\n","    Uninstalling torchmetrics-0.11.4:\n","      Successfully uninstalled torchmetrics-0.11.4\n","Successfully installed torchmetrics-0.3.2\n"]}]},{"cell_type":"code","source":["!pip install torchtext==0.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cBUbpbK7_ji","executionInfo":{"status":"ok","timestamp":1686537840967,"user_tz":-420,"elapsed":5726,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"f505e7c3-29f3-4ce8-c1e5-e26aaf7113bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.6.0\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.6.0)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.15.2\n","    Uninstalling torchtext-0.15.2:\n","      Successfully uninstalled torchtext-0.15.2\n","Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"]}]},{"cell_type":"code","source":["!pip install torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIYK5adF7_et","executionInfo":{"status":"ok","timestamp":1686537845412,"user_tz":-420,"elapsed":4450,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"a9933aa4-e841-4ab1-ff1e-499b1172e303"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install hydra-core"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"S6QGqYdU8Frx","executionInfo":{"status":"ok","timestamp":1686537851633,"user_tz":-420,"elapsed":6229,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"71b305b2-3532-45d7-9f93-83b30f591abe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hydra-core\n","  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting omegaconf<2.4,>=2.2 (from hydra-core)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (23.1)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (5.4.1)\n","Building wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=2a4f77f346d67066c8b6cfca84eac340cff7638a452a580e88fe374c7d93b936\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n","Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install ruamel_yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZpR1cNo8IRD","executionInfo":{"status":"ok","timestamp":1686537856974,"user_tz":-420,"elapsed":5349,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"a2225911-c121-4b7e-a969-e800f09f4d50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ruamel_yaml\n","  Downloading ruamel.yaml-0.17.31-py3-none-any.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel_yaml)\n","  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel_yaml\n","Successfully installed ruamel.yaml.clib-0.2.7 ruamel_yaml-0.17.31\n"]}]},{"cell_type":"code","source":["# !pip install torch --upgrade torch\n","# !pip install torch --upgrade pytorch-lightning"],"metadata":{"id":"RMjvVdfYITxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !nvcc --version"],"metadata":{"id":"WEdpJXk_I_lg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","\n","print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9VEnYWP1KH_Z","executionInfo":{"status":"ok","timestamp":1686537860460,"user_tz":-420,"elapsed":3493,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"e1a2aafa-6457-42d0-c6fe-4e7cc7e5c3b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"markdown","source":["\n","# Setting up parameters"],"metadata":{"id":"Q5gQs6SaILhx"}},{"cell_type":"code","source":["from ruamel.yaml import YAML "],"metadata":{"id":"VnOHG2rCImIq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**IMPORTANT : EDIT FILE-FILE INI DULU SEBELUM RUN**\n","\n","Cara editnya : double-click file yg mau di edit di \"Files\" (tab kiri colab) ato klik link-link dibawah, terus ctrl-s buat save.\n","\n","```1. /content/saint/configs/config.yaml, line 46```\n","\n","```\n","trainer:\n","  max_epochs: 100 # default is 100\n","  # gpus: 0\n","  accelerator: auto\n","  deterministic: true\n","  default_root_dir: null\n","  # resume_from_checkpoint: null\n","```\n","\n","```2. /content/saint/configs/data/bank_ssl.yaml, line 10```\n","```\n","data_stats:\n","  no_cat: 1\n","  no_num: 49\n","  cats: [1]\n","```\n","\n","```3. /content/saint/configs/data/bank_sup.yaml, line 10```\n","\n","```\n","data_stats:\n","  no_cat: 1\n","  no_num: 49\n","  cats: [1]\n","```\n","\n","```4. /content/saint/configs/experiment/supervised.yaml```\n","\n","```\n","experiment: supervised\n","task: classification # {classification, regression}\n","model: saint\n","num_output: 5 # no of output neurons: 1 for binary classification num of classes in target for multiclass}\n","freeze_encoder: false # freeze transformer layer\n","pretrained_checkpoint: null #checkpoints/lightning_logs/version_7/checkpoints/epoch=0-step=1.ckpt\n","```"],"metadata":{"id":"fdYj59RZzVWT"}},{"cell_type":"code","source":["config_path = 'saint/configs/config.yaml'\n","\n","yaml = YAML(typ='safe')\n","with open(config_path) as f:\n","  args = yaml.load(f)\n","\n","print(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3yBXKWlIOjL","executionInfo":{"status":"ok","timestamp":1686538728822,"user_tz":-420,"elapsed":728,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"7a05e6f7-365e-4de3-fdca-99a2239fd5b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'defaults': ['_self_', {'experiment': 'supervised'}, {'data': 'bank_sup'}], 'seed': 1234, 'transformer': {'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'dropout_ff': 0.1, 'embed_dim': 32, 'd_ff': 32, 'cls_token_idx': 0}, 'augmentation': {'prob_cutmix': 0.3, 'alpha': 0.2, 'lambda_pt': 10}, 'optimizer': {'temperature': 0.7, 'proj_head_dim': 128, 'beta_1': 0.9, 'beta_2': 0.99, 'lr': 0.0001, 'weight_decay': 0.01, 'optim': 'adamw', 'metric': 'auroc'}, 'preproc': {'data_folder': None, 'train_split': 0.65, 'validation_split': 0.15, 'test_split': 0.2, 'num_supervised_train_data': None}, 'callback': {'monitor': 'val_loss', 'mode': 'min', 'auto_insert_metric_name': False}, 'trainer': {'max_epochs': 100, 'gpus': 1, 'accelerator': 'auto', 'deterministic': True, 'default_root_dir': None}, 'dataloader': {'shuffle_val': False, 'train_bs': 32, 'val_bs': 32, 'test_bs': 16, 'num_workers': 2, 'pin_memory': False}, 'metric': '${optimizer.metric}', 'print_config': False}\n"]}]},{"cell_type":"markdown","source":["# Data preprocessing"],"metadata":{"id":"mIFvOAGlIueV"}},{"cell_type":"code","source":["data_folder = \"/content/saint/data\"\n","\n","os.mkdir(\"/content/saint/data\")"],"metadata":{"id":"b2yZLsEkI1Ex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset = pd.read_csv(\"/content/drive/MyDrive/SEM4/Research Method/RM Kel 19 Experiment/data-final.csv\", sep='\\t')\n","dataset = pd.read_csv(\"/content/drive/MyDrive/RESEARCH METHOD 19/XGBoost/RM Kel 19 Experiment/data-final.csv\", sep='\\t')\n","\n","\n","print(dataset.shape)\n","dataset.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"0NCa-gD39lta","executionInfo":{"status":"ok","timestamp":1686538413559,"user_tz":-420,"elapsed":13405,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"5231b1c6-6955-40d1-c06f-947af4168667"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1015341, 110)\n"]},{"output_type":"execute_result","data":{"text/plain":["   EXT1  EXT2  EXT3  EXT4  EXT5  EXT6  EXT7  EXT8  EXT9  EXT10  ...  \\\n","0   4.0   1.0   5.0   2.0   5.0   1.0   5.0   2.0   4.0    1.0  ...   \n","1   3.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0   1.0    5.0  ...   \n","2   2.0   3.0   4.0   4.0   3.0   2.0   1.0   3.0   2.0    5.0  ...   \n","3   2.0   2.0   2.0   3.0   4.0   2.0   2.0   4.0   1.0    4.0  ...   \n","4   3.0   3.0   3.0   3.0   5.0   3.0   3.0   5.0   3.0    4.0  ...   \n","\n","              dateload  screenw  screenh  introelapse  testelapse  endelapse  \\\n","0  2016-03-03 02:01:01    768.0   1024.0          9.0       234.0          6   \n","1  2016-03-03 02:01:20   1360.0    768.0         12.0       179.0         11   \n","2  2016-03-03 02:01:56   1366.0    768.0          3.0       186.0          7   \n","3  2016-03-03 02:02:02   1920.0   1200.0        186.0       219.0          7   \n","4  2016-03-03 02:02:57   1366.0    768.0          8.0       315.0         17   \n","\n","   IPC  country  lat_appx_lots_of_err  long_appx_lots_of_err  \n","0    1       GB               51.5448                 0.1991  \n","1    1       MY                3.1698                101.706  \n","2    1       GB               54.9119                -1.3833  \n","3    1       GB                 51.75                  -1.25  \n","4    2       KE                   1.0                   38.0  \n","\n","[5 rows x 110 columns]"],"text/html":["\n","  <div id=\"df-4e567478-3f41-4ee7-aad5-7a1f1fe64e6f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EXT1</th>\n","      <th>EXT2</th>\n","      <th>EXT3</th>\n","      <th>EXT4</th>\n","      <th>EXT5</th>\n","      <th>EXT6</th>\n","      <th>EXT7</th>\n","      <th>EXT8</th>\n","      <th>EXT9</th>\n","      <th>EXT10</th>\n","      <th>...</th>\n","      <th>dateload</th>\n","      <th>screenw</th>\n","      <th>screenh</th>\n","      <th>introelapse</th>\n","      <th>testelapse</th>\n","      <th>endelapse</th>\n","      <th>IPC</th>\n","      <th>country</th>\n","      <th>lat_appx_lots_of_err</th>\n","      <th>long_appx_lots_of_err</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:01</td>\n","      <td>768.0</td>\n","      <td>1024.0</td>\n","      <td>9.0</td>\n","      <td>234.0</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>51.5448</td>\n","      <td>0.1991</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:20</td>\n","      <td>1360.0</td>\n","      <td>768.0</td>\n","      <td>12.0</td>\n","      <td>179.0</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>MY</td>\n","      <td>3.1698</td>\n","      <td>101.706</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:01:56</td>\n","      <td>1366.0</td>\n","      <td>768.0</td>\n","      <td>3.0</td>\n","      <td>186.0</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>54.9119</td>\n","      <td>-1.3833</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:02:02</td>\n","      <td>1920.0</td>\n","      <td>1200.0</td>\n","      <td>186.0</td>\n","      <td>219.0</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>GB</td>\n","      <td>51.75</td>\n","      <td>-1.25</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>2016-03-03 02:02:57</td>\n","      <td>1366.0</td>\n","      <td>768.0</td>\n","      <td>8.0</td>\n","      <td>315.0</td>\n","      <td>17</td>\n","      <td>2</td>\n","      <td>KE</td>\n","      <td>1.0</td>\n","      <td>38.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 110 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e567478-3f41-4ee7-aad5-7a1f1fe64e6f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4e567478-3f41-4ee7-aad5-7a1f1fe64e6f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4e567478-3f41-4ee7-aad5-7a1f1fe64e6f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["data = dataset.drop(list(dataset)[50:], axis=1)\n","\n","print(data.shape)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"kKU71FbL-CH1","executionInfo":{"status":"ok","timestamp":1686538413560,"user_tz":-420,"elapsed":12,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"83a42e32-a605-4de0-fe82-66ba71c744b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1015341, 50)\n"]},{"output_type":"execute_result","data":{"text/plain":["   EXT1  EXT2  EXT3  EXT4  EXT5  EXT6  EXT7  EXT8  EXT9  EXT10  ...  OPN1  \\\n","0   4.0   1.0   5.0   2.0   5.0   1.0   5.0   2.0   4.0    1.0  ...   5.0   \n","1   3.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0   1.0    5.0  ...   1.0   \n","2   2.0   3.0   4.0   4.0   3.0   2.0   1.0   3.0   2.0    5.0  ...   5.0   \n","3   2.0   2.0   2.0   3.0   4.0   2.0   2.0   4.0   1.0    4.0  ...   4.0   \n","4   3.0   3.0   3.0   3.0   5.0   3.0   3.0   5.0   3.0    4.0  ...   5.0   \n","\n","   OPN2  OPN3  OPN4  OPN5  OPN6  OPN7  OPN8  OPN9  OPN10  \n","0   1.0   4.0   1.0   4.0   1.0   5.0   3.0   4.0    5.0  \n","1   2.0   4.0   2.0   3.0   1.0   4.0   2.0   5.0    3.0  \n","2   1.0   2.0   1.0   4.0   2.0   5.0   3.0   4.0    4.0  \n","3   2.0   5.0   2.0   3.0   1.0   4.0   4.0   3.0    3.0  \n","4   1.0   5.0   1.0   5.0   1.0   5.0   3.0   5.0    5.0  \n","\n","[5 rows x 50 columns]"],"text/html":["\n","  <div id=\"df-036d5c70-67d0-4814-86f0-0b2757157e30\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EXT1</th>\n","      <th>EXT2</th>\n","      <th>EXT3</th>\n","      <th>EXT4</th>\n","      <th>EXT5</th>\n","      <th>EXT6</th>\n","      <th>EXT7</th>\n","      <th>EXT8</th>\n","      <th>EXT9</th>\n","      <th>EXT10</th>\n","      <th>...</th>\n","      <th>OPN1</th>\n","      <th>OPN2</th>\n","      <th>OPN3</th>\n","      <th>OPN4</th>\n","      <th>OPN5</th>\n","      <th>OPN6</th>\n","      <th>OPN7</th>\n","      <th>OPN8</th>\n","      <th>OPN9</th>\n","      <th>OPN10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 50 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-036d5c70-67d0-4814-86f0-0b2757157e30')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-036d5c70-67d0-4814-86f0-0b2757157e30 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-036d5c70-67d0-4814-86f0-0b2757157e30');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["for i in data.columns:\n","  data = data[(data[i].notna()) & (data[i] != 0)]\n","\n","print(data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z75-zCjK-IiC","executionInfo":{"status":"ok","timestamp":1686538423343,"user_tz":-420,"elapsed":9790,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"d92c7f27-856c-4f87-dce2-d754eba857ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(874434, 50)\n"]}]},{"cell_type":"code","source":["data = data.astype(int)"],"metadata":{"id":"v6-PfEfr9vjd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['EST9'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fK-gKGep-GKA","executionInfo":{"status":"ok","timestamp":1686538423891,"user_tz":-420,"elapsed":4,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"07c8f387-3f43-4a19-a68f-ba315864dde7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4    247851\n","2    199050\n","3    182001\n","5    133152\n","1    112380\n","Name: EST9, dtype: int64"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["data = data[:10000]"],"metadata":{"id":"rOAEEVuG1oC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = data.drop(columns=['EST9'])\n","y = data['EST9']"],"metadata":{"id":"f9Arorto-KIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = y - 1"],"metadata":{"id":"QN7RPmy9S8X9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from saint.src.dataset import generate_splits, preprocess"],"metadata":{"id":"XYfjIiOeJe91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_supervised_train_data = 2000\n","\n","sup_train_indices, val_indices, test_indices, ssl_train_indices = generate_splits(len(x), num_supervised_train_data, args['preproc']['validation_split'], args['preproc']['test_split'], args['seed'],)"],"metadata":{"id":"sBdu-PJLJjt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_proc, y_proc, no_num, no_cat, cats  = preprocess(x, y, args['transformer']['cls_token_idx'])"],"metadata":{"id":"1ldx0xi8J9mT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('no of numerical columns: ', no_num)\n","print('no of categorical columns: ', no_cat)\n","\n","print('list of categories in each categorical column: ', cats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99TtxxqOKEMK","executionInfo":{"status":"ok","timestamp":1686538432234,"user_tz":-420,"elapsed":5,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"6a0c5c3c-5618-44b4-c6fa-09b2c41ab775"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["no of numerical columns:  49\n","no of categorical columns:  1\n","list of categories in each categorical column:  [1]\n"]}]},{"cell_type":"code","source":["train_df, train_y   = x_proc.iloc[sup_train_indices], y_proc.iloc[sup_train_indices]\n","val_df, val_y       = x_proc.iloc[val_indices], y_proc.iloc[val_indices]\n","test_df, test_y     = x_proc.iloc[test_indices], y_proc.iloc[test_indices]"],"metadata":{"id":"zO_CGazRKIlS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ssl, train_ssl_y = None, None\n","\n","if num_supervised_train_data != 'all':\n","    train_ssl, train_ssl_y = x_proc.iloc[ssl_train_indices], y_proc.iloc[ssl_train_indices]"],"metadata":{"id":"xdM8JYynKciy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df.to_csv('/content/saint/data/train.csv' , index=False)\n","train_y.to_csv('/content/saint/data/train_y.csv' , index=False)\n","val_df.to_csv('/content/saint/data/val.csv' , index=False)\n","val_y.to_csv('/content/saint/data/val_y.csv' , index=False)\n","test_df.to_csv('/content/saint/data/test.csv' , index=False)\n","test_y.to_csv('/content/saint/data/test_y.csv' , index=False)\n","\n","if train_ssl is not None:\n","   train_ssl.to_csv('/content/saint/data/train_ssl.csv' , index=False)\n","\n","if train_ssl_y is not None:\n","  train_ssl_y.to_csv('/content/saint/data/train_ssl_y.csv' , index=False)"],"metadata":{"id":"xn-T7swhKpmA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SAINT training"],"metadata":{"id":"XK3XHgVCGfOm"}},{"cell_type":"code","source":["num_gpus = 1"],"metadata":{"id":"mqJKcN-ZNr7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"HYDRA_FULL_ERROR\"] = \"1\""],"metadata":{"id":"uMwBMOpT51E3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Self-supervised learning + supervised learning"],"metadata":{"id":"rZyNJ8mpOm6A"}},{"cell_type":"code","source":["# Self-Supervised Learning\n","\n","!python /content/saint/main.py experiment=self-supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_ssl"],"metadata":{"id":"tVpqTt4RLUV7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686540887220,"user_tz":-420,"elapsed":2128526,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"fe6083af-70e8-40a0-9a80-c16e370816fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-12 02:59:24.260512: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-12 02:59:25.239756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/main.py:11: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Global seed set to 1234\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name                | Type            | Params\n","--------------------------------------------------------\n","0 | transformer         | Encoder         | 65.0 K\n","1 | embedding           | Embedding       | 3.2 K \n","2 | contrastive_loss_fn | ContrastiveLoss | 409 K \n","3 | denoising_loss_fn   | DenoisingLoss   | 1.6 K \n","--------------------------------------------------------\n","479 K     Trainable params\n","0         Non-trainable params\n","479 K     Total params\n","1.918     Total estimated model params size (MB)\n","Global seed set to 1234\n","Epoch 0:  85% 160/188 [00:18<00:03,  8.48it/s, loss=598, v_num=0, val_loss_epoch=687.0, train_loss_step=531.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  96% 180/188 [00:19<00:00,  9.12it/s, loss=598, v_num=0, val_loss_epoch=687.0, train_loss_step=531.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.58it/s]\u001b[A\n","Epoch 0: 100% 188/188 [00:20<00:00,  9.01it/s, loss=593, v_num=0, val_loss_epoch=598.0, train_loss_step=552.0, train_loss_epoch=606.0, val_loss_step=601.0]\n","Epoch 1:  85% 160/188 [00:17<00:03,  8.99it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=608.0, train_loss_epoch=606.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 1:  96% 180/188 [00:18<00:00,  9.63it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=608.0, train_loss_epoch=606.0, val_loss_step=601.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.27it/s]\u001b[A\n","Epoch 1: 100% 188/188 [00:19<00:00,  9.49it/s, loss=595, v_num=0, val_loss_epoch=598.0, train_loss_step=541.0, train_loss_epoch=601.0, val_loss_step=601.0]\n","Epoch 2:  85% 160/188 [00:19<00:03,  8.13it/s, loss=590, v_num=0, val_loss_epoch=598.0, train_loss_step=619.0, train_loss_epoch=601.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 2:  96% 180/188 [00:20<00:00,  8.62it/s, loss=590, v_num=0, val_loss_epoch=598.0, train_loss_step=619.0, train_loss_epoch=601.0, val_loss_step=601.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.92it/s]\u001b[A\n","Epoch 2: 100% 188/188 [00:22<00:00,  8.37it/s, loss=590, v_num=0, val_loss_epoch=599.0, train_loss_step=567.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 3:  85% 160/188 [00:18<00:03,  8.75it/s, loss=599, v_num=0, val_loss_epoch=599.0, train_loss_step=634.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 3:  96% 180/188 [00:19<00:00,  9.39it/s, loss=599, v_num=0, val_loss_epoch=599.0, train_loss_step=634.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.45it/s]\u001b[A\n","Epoch 3: 100% 188/188 [00:20<00:00,  9.27it/s, loss=594, v_num=0, val_loss_epoch=598.0, train_loss_step=502.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 4:  85% 160/188 [00:17<00:03,  8.94it/s, loss=612, v_num=0, val_loss_epoch=598.0, train_loss_step=581.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 4:  96% 180/188 [00:18<00:00,  9.58it/s, loss=612, v_num=0, val_loss_epoch=598.0, train_loss_step=581.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.49it/s]\u001b[A\n","Epoch 4: 100% 188/188 [00:20<00:00,  9.36it/s, loss=607, v_num=0, val_loss_epoch=598.0, train_loss_step=571.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 5:  85% 160/188 [00:18<00:03,  8.45it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=626.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 5:  96% 180/188 [00:19<00:00,  9.07it/s, loss=600, v_num=0, val_loss_epoch=598.0, train_loss_step=626.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.76it/s]\u001b[A\n","Epoch 5: 100% 188/188 [00:20<00:00,  8.97it/s, loss=596, v_num=0, val_loss_epoch=598.0, train_loss_step=507.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 6:  85% 160/188 [00:17<00:03,  9.11it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=537.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 6:  96% 180/188 [00:18<00:00,  9.76it/s, loss=601, v_num=0, val_loss_epoch=598.0, train_loss_step=537.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.42it/s]\u001b[A\n","Epoch 6: 100% 188/188 [00:19<00:00,  9.62it/s, loss=597, v_num=0, val_loss_epoch=598.0, train_loss_step=476.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 7:  85% 160/188 [00:18<00:03,  8.68it/s, loss=583, v_num=0, val_loss_epoch=598.0, train_loss_step=600.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 7:  96% 180/188 [00:19<00:00,  9.17it/s, loss=583, v_num=0, val_loss_epoch=598.0, train_loss_step=600.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  85% 40/47 [00:02<00:00, 15.72it/s]\u001b[A\n","Epoch 7: 100% 188/188 [00:21<00:00,  8.84it/s, loss=579, v_num=0, val_loss_epoch=598.0, train_loss_step=505.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Epoch 8:  85% 160/188 [00:17<00:03,  9.08it/s, loss=606, v_num=0, val_loss_epoch=598.0, train_loss_step=660.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 8:  96% 180/188 [00:18<00:00,  9.72it/s, loss=606, v_num=0, val_loss_epoch=598.0, train_loss_step=660.0, train_loss_epoch=601.0, val_loss_step=602.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.03it/s]\u001b[A\n","Epoch 8: 100% 188/188 [00:19<00:00,  9.58it/s, loss=606, v_num=0, val_loss_epoch=598.0, train_loss_step=605.0, train_loss_epoch=601.0, val_loss_step=601.0]\n","Epoch 9:  85% 160/188 [00:17<00:03,  9.04it/s, loss=614, v_num=0, val_loss_epoch=598.0, train_loss_step=609.0, train_loss_epoch=601.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 9:  96% 180/188 [00:18<00:00,  9.68it/s, loss=614, v_num=0, val_loss_epoch=598.0, train_loss_step=609.0, train_loss_epoch=601.0, val_loss_step=601.0]\n","Validating:  85% 40/47 [00:01<00:00, 21.05it/s]\u001b[A\n","Epoch 9: 100% 188/188 [00:20<00:00,  9.38it/s, loss=609, v_num=0, val_loss_epoch=598.0, train_loss_step=607.0, train_loss_epoch=600.0, val_loss_step=602.0]\n","Epoch 10:  85% 160/188 [00:18<00:03,  8.48it/s, loss=593, v_num=0, val_loss_epoch=598.0, train_loss_step=618.0, train_loss_epoch=600.0, val_loss_step=602.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 10:  96% 180/188 [00:19<00:00,  9.12it/s, loss=593, v_num=0, val_loss_epoch=598.0, train_loss_step=618.0, train_loss_epoch=600.0, val_loss_step=602.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.08it/s]\u001b[A\n","Epoch 10: 100% 188/188 [00:20<00:00,  9.01it/s, loss=591, v_num=0, val_loss_epoch=597.0, train_loss_step=569.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Epoch 11:  85% 160/188 [00:17<00:03,  9.07it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=591.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 11:  96% 180/188 [00:18<00:00,  9.71it/s, loss=597, v_num=0, val_loss_epoch=597.0, train_loss_step=591.0, train_loss_epoch=600.0, val_loss_step=601.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.89it/s]\u001b[A\n","Epoch 11: 100% 188/188 [00:19<00:00,  9.56it/s, loss=595, v_num=0, val_loss_epoch=595.0, train_loss_step=524.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Epoch 12:  85% 160/188 [00:20<00:03,  7.92it/s, loss=586, v_num=0, val_loss_epoch=595.0, train_loss_step=524.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 12:  96% 180/188 [00:21<00:00,  8.40it/s, loss=586, v_num=0, val_loss_epoch=595.0, train_loss_step=524.0, train_loss_epoch=599.0, val_loss_step=601.0]\n","Validating:  85% 40/47 [00:02<00:00, 16.92it/s]\u001b[A\n","Epoch 12: 100% 188/188 [00:23<00:00,  8.12it/s, loss=586, v_num=0, val_loss_epoch=594.0, train_loss_step=596.0, train_loss_epoch=598.0, val_loss_step=600.0]\n","Epoch 13:  85% 160/188 [00:17<00:03,  8.97it/s, loss=596, v_num=0, val_loss_epoch=594.0, train_loss_step=626.0, train_loss_epoch=598.0, val_loss_step=600.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 13:  96% 180/188 [00:18<00:00,  9.61it/s, loss=596, v_num=0, val_loss_epoch=594.0, train_loss_step=626.0, train_loss_epoch=598.0, val_loss_step=600.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.47it/s]\u001b[A\n","Epoch 13: 100% 188/188 [00:19<00:00,  9.48it/s, loss=595, v_num=0, val_loss_epoch=593.0, train_loss_step=566.0, train_loss_epoch=597.0, val_loss_step=598.0]\n","Epoch 14:  85% 160/188 [00:17<00:03,  9.05it/s, loss=594, v_num=0, val_loss_epoch=593.0, train_loss_step=597.0, train_loss_epoch=597.0, val_loss_step=598.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 14:  96% 180/188 [00:18<00:00,  9.70it/s, loss=594, v_num=0, val_loss_epoch=593.0, train_loss_step=597.0, train_loss_epoch=597.0, val_loss_step=598.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.01it/s]\u001b[A\n","Epoch 14: 100% 188/188 [00:19<00:00,  9.42it/s, loss=594, v_num=0, val_loss_epoch=592.0, train_loss_step=586.0, train_loss_epoch=597.0, val_loss_step=596.0]\n","Epoch 15:  85% 160/188 [00:18<00:03,  8.53it/s, loss=592, v_num=0, val_loss_epoch=592.0, train_loss_step=592.0, train_loss_epoch=597.0, val_loss_step=596.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 15:  96% 180/188 [00:19<00:00,  9.17it/s, loss=592, v_num=0, val_loss_epoch=592.0, train_loss_step=592.0, train_loss_epoch=597.0, val_loss_step=596.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.11it/s]\u001b[A\n","Epoch 15: 100% 188/188 [00:20<00:00,  9.05it/s, loss=590, v_num=0, val_loss_epoch=590.0, train_loss_step=536.0, train_loss_epoch=596.0, val_loss_step=601.0]\n","Epoch 16:  85% 160/188 [00:18<00:03,  8.87it/s, loss=588, v_num=0, val_loss_epoch=590.0, train_loss_step=560.0, train_loss_epoch=596.0, val_loss_step=601.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 16:  96% 180/188 [00:18<00:00,  9.52it/s, loss=588, v_num=0, val_loss_epoch=590.0, train_loss_step=560.0, train_loss_epoch=596.0, val_loss_step=601.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.63it/s]\u001b[A\n","Epoch 16: 100% 188/188 [00:20<00:00,  9.39it/s, loss=587, v_num=0, val_loss_epoch=590.0, train_loss_step=556.0, train_loss_epoch=595.0, val_loss_step=596.0]\n","Epoch 17:  85% 160/188 [00:18<00:03,  8.67it/s, loss=601, v_num=0, val_loss_epoch=590.0, train_loss_step=607.0, train_loss_epoch=595.0, val_loss_step=596.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 17:  96% 180/188 [00:19<00:00,  9.07it/s, loss=601, v_num=0, val_loss_epoch=590.0, train_loss_step=607.0, train_loss_epoch=595.0, val_loss_step=596.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.06it/s]\u001b[A\n","Epoch 17: 100% 188/188 [00:21<00:00,  8.87it/s, loss=601, v_num=0, val_loss_epoch=587.0, train_loss_step=632.0, train_loss_epoch=594.0, val_loss_step=597.0]\n","Epoch 18:  85% 160/188 [00:17<00:03,  9.14it/s, loss=593, v_num=0, val_loss_epoch=587.0, train_loss_step=666.0, train_loss_epoch=594.0, val_loss_step=597.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 18:  96% 180/188 [00:18<00:00,  9.78it/s, loss=593, v_num=0, val_loss_epoch=587.0, train_loss_step=666.0, train_loss_epoch=594.0, val_loss_step=597.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.82it/s]\u001b[A\n","Epoch 18: 100% 188/188 [00:19<00:00,  9.61it/s, loss=591, v_num=0, val_loss_epoch=588.0, train_loss_step=541.0, train_loss_epoch=593.0, val_loss_step=597.0]\n","Epoch 19:  85% 160/188 [00:17<00:03,  9.06it/s, loss=602, v_num=0, val_loss_epoch=588.0, train_loss_step=606.0, train_loss_epoch=593.0, val_loss_step=597.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 19:  96% 180/188 [00:18<00:00,  9.69it/s, loss=602, v_num=0, val_loss_epoch=588.0, train_loss_step=606.0, train_loss_epoch=593.0, val_loss_step=597.0]\n","Validating:  85% 40/47 [00:01<00:00, 20.09it/s]\u001b[A\n","Epoch 19: 100% 188/188 [00:20<00:00,  9.36it/s, loss=602, v_num=0, val_loss_epoch=586.0, train_loss_step=559.0, train_loss_epoch=592.0, val_loss_step=590.0]\n","Epoch 20:  85% 160/188 [00:18<00:03,  8.55it/s, loss=582, v_num=0, val_loss_epoch=586.0, train_loss_step=557.0, train_loss_epoch=592.0, val_loss_step=590.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 20:  96% 180/188 [00:19<00:00,  9.17it/s, loss=582, v_num=0, val_loss_epoch=586.0, train_loss_step=557.0, train_loss_epoch=592.0, val_loss_step=590.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.16it/s]\u001b[A\n","Epoch 20: 100% 188/188 [00:20<00:00,  9.07it/s, loss=579, v_num=0, val_loss_epoch=584.0, train_loss_step=555.0, train_loss_epoch=591.0, val_loss_step=585.0]\n","Epoch 21:  85% 160/188 [00:19<00:03,  8.32it/s, loss=594, v_num=0, val_loss_epoch=584.0, train_loss_step=514.0, train_loss_epoch=591.0, val_loss_step=585.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 21:  96% 180/188 [00:20<00:00,  8.95it/s, loss=594, v_num=0, val_loss_epoch=584.0, train_loss_step=514.0, train_loss_epoch=591.0, val_loss_step=585.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.91it/s]\u001b[A\n","Epoch 21: 100% 188/188 [00:21<00:00,  8.76it/s, loss=596, v_num=0, val_loss_epoch=583.0, train_loss_step=683.0, train_loss_epoch=590.0, val_loss_step=594.0]\n","Epoch 22:  85% 160/188 [00:18<00:03,  8.48it/s, loss=579, v_num=0, val_loss_epoch=583.0, train_loss_step=542.0, train_loss_epoch=590.0, val_loss_step=594.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 22:  96% 180/188 [00:19<00:00,  9.11it/s, loss=579, v_num=0, val_loss_epoch=583.0, train_loss_step=542.0, train_loss_epoch=590.0, val_loss_step=594.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.43it/s]\u001b[A\n","Epoch 22: 100% 188/188 [00:20<00:00,  9.02it/s, loss=579, v_num=0, val_loss_epoch=583.0, train_loss_step=548.0, train_loss_epoch=589.0, val_loss_step=593.0]\n","Epoch 23:  85% 160/188 [00:17<00:03,  9.03it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=557.0, train_loss_epoch=589.0, val_loss_step=593.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 23:  96% 180/188 [00:18<00:00,  9.67it/s, loss=585, v_num=0, val_loss_epoch=583.0, train_loss_step=557.0, train_loss_epoch=589.0, val_loss_step=593.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.82it/s]\u001b[A\n","Epoch 23: 100% 188/188 [00:19<00:00,  9.53it/s, loss=579, v_num=0, val_loss_epoch=582.0, train_loss_step=480.0, train_loss_epoch=588.0, val_loss_step=590.0]\n","Epoch 24:  85% 160/188 [00:18<00:03,  8.60it/s, loss=583, v_num=0, val_loss_epoch=582.0, train_loss_step=579.0, train_loss_epoch=588.0, val_loss_step=590.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 24:  96% 180/188 [00:19<00:00,  9.05it/s, loss=583, v_num=0, val_loss_epoch=582.0, train_loss_step=579.0, train_loss_epoch=588.0, val_loss_step=590.0]\n","Validating:  85% 40/47 [00:02<00:00, 16.80it/s]\u001b[A\n","Epoch 24: 100% 188/188 [00:21<00:00,  8.81it/s, loss=579, v_num=0, val_loss_epoch=581.0, train_loss_step=581.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Epoch 25:  85% 160/188 [00:17<00:03,  9.06it/s, loss=578, v_num=0, val_loss_epoch=581.0, train_loss_step=535.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 25:  96% 180/188 [00:18<00:00,  9.70it/s, loss=578, v_num=0, val_loss_epoch=581.0, train_loss_step=535.0, train_loss_epoch=588.0, val_loss_step=591.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.29it/s]\u001b[A\n","Epoch 25: 100% 188/188 [00:19<00:00,  9.57it/s, loss=575, v_num=0, val_loss_epoch=581.0, train_loss_step=550.0, train_loss_epoch=587.0, val_loss_step=586.0]\n","Epoch 26:  85% 160/188 [00:17<00:03,  9.09it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=573.0, train_loss_epoch=587.0, val_loss_step=586.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 26:  96% 180/188 [00:18<00:00,  9.73it/s, loss=583, v_num=0, val_loss_epoch=581.0, train_loss_step=573.0, train_loss_epoch=587.0, val_loss_step=586.0]\n","Validating:  85% 40/47 [00:01<00:00, 20.41it/s]\u001b[A\n","Epoch 26: 100% 188/188 [00:20<00:00,  9.40it/s, loss=578, v_num=0, val_loss_epoch=579.0, train_loss_step=487.0, train_loss_epoch=587.0, val_loss_step=585.0]\n","Epoch 27:  85% 160/188 [00:18<00:03,  8.61it/s, loss=587, v_num=0, val_loss_epoch=579.0, train_loss_step=584.0, train_loss_epoch=587.0, val_loss_step=585.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 27:  96% 180/188 [00:19<00:00,  9.26it/s, loss=587, v_num=0, val_loss_epoch=579.0, train_loss_step=584.0, train_loss_epoch=587.0, val_loss_step=585.0]\n","Validating:  85% 40/47 [00:01<00:00, 25.01it/s]\u001b[A\n","Epoch 27: 100% 188/188 [00:20<00:00,  9.16it/s, loss=588, v_num=0, val_loss_epoch=578.0, train_loss_step=557.0, train_loss_epoch=585.0, val_loss_step=588.0]\n","Epoch 28:  85% 160/188 [00:17<00:03,  9.09it/s, loss=604, v_num=0, val_loss_epoch=578.0, train_loss_step=553.0, train_loss_epoch=585.0, val_loss_step=588.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 28:  96% 180/188 [00:18<00:00,  9.74it/s, loss=604, v_num=0, val_loss_epoch=578.0, train_loss_step=553.0, train_loss_epoch=585.0, val_loss_step=588.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.46it/s]\u001b[A\n","Epoch 28: 100% 188/188 [00:19<00:00,  9.60it/s, loss=597, v_num=0, val_loss_epoch=578.0, train_loss_step=505.0, train_loss_epoch=586.0, val_loss_step=590.0]\n","Epoch 29:  85% 160/188 [00:18<00:03,  8.53it/s, loss=583, v_num=0, val_loss_epoch=578.0, train_loss_step=572.0, train_loss_epoch=586.0, val_loss_step=590.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 29:  96% 180/188 [00:20<00:00,  8.95it/s, loss=583, v_num=0, val_loss_epoch=578.0, train_loss_step=572.0, train_loss_epoch=586.0, val_loss_step=590.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.47it/s]\u001b[A\n","Epoch 29: 100% 188/188 [00:21<00:00,  8.77it/s, loss=576, v_num=0, val_loss_epoch=576.0, train_loss_step=497.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Epoch 30:  85% 160/188 [00:17<00:03,  9.12it/s, loss=584, v_num=0, val_loss_epoch=576.0, train_loss_step=581.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 30:  96% 180/188 [00:18<00:00,  9.77it/s, loss=584, v_num=0, val_loss_epoch=576.0, train_loss_step=581.0, train_loss_epoch=583.0, val_loss_step=583.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.56it/s]\u001b[A\n","Epoch 30: 100% 188/188 [00:19<00:00,  9.63it/s, loss=577, v_num=0, val_loss_epoch=576.0, train_loss_step=507.0, train_loss_epoch=584.0, val_loss_step=588.0]\n","Epoch 31:  85% 160/188 [00:19<00:03,  8.29it/s, loss=589, v_num=0, val_loss_epoch=576.0, train_loss_step=573.0, train_loss_epoch=584.0, val_loss_step=588.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 31:  96% 180/188 [00:20<00:00,  8.78it/s, loss=589, v_num=0, val_loss_epoch=576.0, train_loss_step=573.0, train_loss_epoch=584.0, val_loss_step=588.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.89it/s]\u001b[A\n","Epoch 31: 100% 188/188 [00:22<00:00,  8.50it/s, loss=585, v_num=0, val_loss_epoch=574.0, train_loss_step=482.0, train_loss_epoch=582.0, val_loss_step=585.0]\n","Epoch 32:  85% 160/188 [00:17<00:03,  8.94it/s, loss=580, v_num=0, val_loss_epoch=574.0, train_loss_step=624.0, train_loss_epoch=582.0, val_loss_step=585.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 32:  96% 180/188 [00:18<00:00,  9.59it/s, loss=580, v_num=0, val_loss_epoch=574.0, train_loss_step=624.0, train_loss_epoch=582.0, val_loss_step=585.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.42it/s]\u001b[A\n","Epoch 32: 100% 188/188 [00:19<00:00,  9.45it/s, loss=575, v_num=0, val_loss_epoch=575.0, train_loss_step=495.0, train_loss_epoch=582.0, val_loss_step=579.0]\n","Epoch 33:  85% 160/188 [00:18<00:03,  8.87it/s, loss=578, v_num=0, val_loss_epoch=575.0, train_loss_step=580.0, train_loss_epoch=582.0, val_loss_step=579.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 33:  96% 180/188 [00:18<00:00,  9.49it/s, loss=578, v_num=0, val_loss_epoch=575.0, train_loss_step=580.0, train_loss_epoch=582.0, val_loss_step=579.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.57it/s]\u001b[A\n","Epoch 33: 100% 188/188 [00:20<00:00,  9.26it/s, loss=569, v_num=0, val_loss_epoch=573.0, train_loss_step=460.0, train_loss_epoch=581.0, val_loss_step=576.0]\n","Epoch 34:  85% 160/188 [00:18<00:03,  8.55it/s, loss=574, v_num=0, val_loss_epoch=573.0, train_loss_step=510.0, train_loss_epoch=581.0, val_loss_step=576.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 34:  96% 180/188 [00:19<00:00,  9.18it/s, loss=574, v_num=0, val_loss_epoch=573.0, train_loss_step=510.0, train_loss_epoch=581.0, val_loss_step=576.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.00it/s]\u001b[A\n","Epoch 34: 100% 188/188 [00:20<00:00,  9.06it/s, loss=572, v_num=0, val_loss_epoch=572.0, train_loss_step=519.0, train_loss_epoch=580.0, val_loss_step=578.0]\n","Epoch 35:  85% 160/188 [00:17<00:03,  9.06it/s, loss=582, v_num=0, val_loss_epoch=572.0, train_loss_step=575.0, train_loss_epoch=580.0, val_loss_step=578.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 35:  96% 180/188 [00:18<00:00,  9.71it/s, loss=582, v_num=0, val_loss_epoch=572.0, train_loss_step=575.0, train_loss_epoch=580.0, val_loss_step=578.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.27it/s]\u001b[A\n","Epoch 35: 100% 188/188 [00:19<00:00,  9.57it/s, loss=577, v_num=0, val_loss_epoch=570.0, train_loss_step=468.0, train_loss_epoch=579.0, val_loss_step=577.0]\n","Epoch 36:  85% 160/188 [00:18<00:03,  8.79it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=590.0, train_loss_epoch=579.0, val_loss_step=577.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 36:  96% 180/188 [00:19<00:00,  9.27it/s, loss=584, v_num=0, val_loss_epoch=570.0, train_loss_step=590.0, train_loss_epoch=579.0, val_loss_step=577.0]\n","Validating:  85% 40/47 [00:02<00:00, 15.91it/s]\u001b[A\n","Epoch 36: 100% 188/188 [00:21<00:00,  8.93it/s, loss=582, v_num=0, val_loss_epoch=567.0, train_loss_step=556.0, train_loss_epoch=578.0, val_loss_step=568.0]\n","Epoch 37:  85% 160/188 [00:17<00:03,  9.06it/s, loss=580, v_num=0, val_loss_epoch=567.0, train_loss_step=615.0, train_loss_epoch=578.0, val_loss_step=568.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 37:  96% 180/188 [00:18<00:00,  9.70it/s, loss=580, v_num=0, val_loss_epoch=567.0, train_loss_step=615.0, train_loss_epoch=578.0, val_loss_step=568.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.88it/s]\u001b[A\n","Epoch 37: 100% 188/188 [00:19<00:00,  9.55it/s, loss=576, v_num=0, val_loss_epoch=564.0, train_loss_step=484.0, train_loss_epoch=575.0, val_loss_step=573.0]\n","Epoch 38:  85% 160/188 [00:17<00:03,  8.89it/s, loss=589, v_num=0, val_loss_epoch=564.0, train_loss_step=560.0, train_loss_epoch=575.0, val_loss_step=573.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 38:  96% 180/188 [00:19<00:00,  9.47it/s, loss=589, v_num=0, val_loss_epoch=564.0, train_loss_step=560.0, train_loss_epoch=575.0, val_loss_step=573.0]\n","Validating:  85% 40/47 [00:02<00:00, 19.29it/s]\u001b[A\n","Epoch 38: 100% 188/188 [00:20<00:00,  9.16it/s, loss=583, v_num=0, val_loss_epoch=564.0, train_loss_step=462.0, train_loss_epoch=575.0, val_loss_step=564.0]\n","Epoch 39:  85% 160/188 [00:18<00:03,  8.59it/s, loss=572, v_num=0, val_loss_epoch=564.0, train_loss_step=567.0, train_loss_epoch=575.0, val_loss_step=564.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 39:  96% 180/188 [00:19<00:00,  9.23it/s, loss=572, v_num=0, val_loss_epoch=564.0, train_loss_step=567.0, train_loss_epoch=575.0, val_loss_step=564.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.43it/s]\u001b[A\n","Epoch 39: 100% 188/188 [00:20<00:00,  9.12it/s, loss=566, v_num=0, val_loss_epoch=563.0, train_loss_step=479.0, train_loss_epoch=572.0, val_loss_step=569.0]\n","Epoch 40:  85% 160/188 [00:17<00:03,  9.02it/s, loss=565, v_num=0, val_loss_epoch=563.0, train_loss_step=625.0, train_loss_epoch=572.0, val_loss_step=569.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 40:  96% 180/188 [00:18<00:00,  9.66it/s, loss=565, v_num=0, val_loss_epoch=563.0, train_loss_step=625.0, train_loss_epoch=572.0, val_loss_step=569.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.57it/s]\u001b[A\n","Epoch 40: 100% 188/188 [00:19<00:00,  9.50it/s, loss=562, v_num=0, val_loss_epoch=561.0, train_loss_step=491.0, train_loss_epoch=571.0, val_loss_step=569.0]\n","Epoch 41:  85% 160/188 [00:21<00:03,  7.41it/s, loss=568, v_num=0, val_loss_epoch=561.0, train_loss_step=614.0, train_loss_epoch=571.0, val_loss_step=569.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 41:  96% 180/188 [00:23<00:01,  7.80it/s, loss=568, v_num=0, val_loss_epoch=561.0, train_loss_step=614.0, train_loss_epoch=571.0, val_loss_step=569.0]\n","Validating:  85% 40/47 [00:02<00:00, 18.12it/s]\u001b[A\n","Epoch 41: 100% 188/188 [00:24<00:00,  7.76it/s, loss=562, v_num=0, val_loss_epoch=558.0, train_loss_step=459.0, train_loss_epoch=571.0, val_loss_step=568.0]\n","Epoch 42:  85% 160/188 [00:18<00:03,  8.49it/s, loss=558, v_num=0, val_loss_epoch=558.0, train_loss_step=618.0, train_loss_epoch=571.0, val_loss_step=568.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 42:  96% 180/188 [00:19<00:00,  9.10it/s, loss=558, v_num=0, val_loss_epoch=558.0, train_loss_step=618.0, train_loss_epoch=571.0, val_loss_step=568.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.03it/s]\u001b[A\n","Epoch 42: 100% 188/188 [00:20<00:00,  8.96it/s, loss=557, v_num=0, val_loss_epoch=560.0, train_loss_step=545.0, train_loss_epoch=570.0, val_loss_step=564.0]\n","Epoch 43:  85% 160/188 [00:18<00:03,  8.76it/s, loss=574, v_num=0, val_loss_epoch=560.0, train_loss_step=603.0, train_loss_epoch=570.0, val_loss_step=564.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 43:  96% 180/188 [00:19<00:00,  9.25it/s, loss=574, v_num=0, val_loss_epoch=560.0, train_loss_step=603.0, train_loss_epoch=570.0, val_loss_step=564.0]\n","Validating:  85% 40/47 [00:02<00:00, 16.34it/s]\u001b[A\n","Epoch 43: 100% 188/188 [00:21<00:00,  8.87it/s, loss=573, v_num=0, val_loss_epoch=557.0, train_loss_step=468.0, train_loss_epoch=569.0, val_loss_step=555.0]\n","Epoch 44:  85% 160/188 [00:17<00:03,  8.99it/s, loss=562, v_num=0, val_loss_epoch=557.0, train_loss_step=586.0, train_loss_epoch=569.0, val_loss_step=555.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 44:  96% 180/188 [00:18<00:00,  9.62it/s, loss=562, v_num=0, val_loss_epoch=557.0, train_loss_step=586.0, train_loss_epoch=569.0, val_loss_step=555.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.78it/s]\u001b[A\n","Epoch 44: 100% 188/188 [00:19<00:00,  9.48it/s, loss=558, v_num=0, val_loss_epoch=554.0, train_loss_step=528.0, train_loss_epoch=567.0, val_loss_step=554.0]\n","Epoch 45:  85% 160/188 [00:17<00:03,  8.91it/s, loss=557, v_num=0, val_loss_epoch=554.0, train_loss_step=525.0, train_loss_epoch=567.0, val_loss_step=554.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 45:  96% 180/188 [00:18<00:00,  9.52it/s, loss=557, v_num=0, val_loss_epoch=554.0, train_loss_step=525.0, train_loss_epoch=567.0, val_loss_step=554.0]\n","Validating:  85% 40/47 [00:02<00:00, 19.43it/s]\u001b[A\n","Epoch 45: 100% 188/188 [00:20<00:00,  9.17it/s, loss=557, v_num=0, val_loss_epoch=556.0, train_loss_step=519.0, train_loss_epoch=566.0, val_loss_step=573.0]\n","Epoch 46:  85% 160/188 [00:18<00:03,  8.46it/s, loss=558, v_num=0, val_loss_epoch=556.0, train_loss_step=573.0, train_loss_epoch=566.0, val_loss_step=573.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 46:  96% 180/188 [00:20<00:00,  8.96it/s, loss=558, v_num=0, val_loss_epoch=556.0, train_loss_step=573.0, train_loss_epoch=566.0, val_loss_step=573.0]\n","Validating:  85% 40/47 [00:02<00:00, 20.21it/s]\u001b[A\n","Epoch 46: 100% 188/188 [00:21<00:00,  8.84it/s, loss=558, v_num=0, val_loss_epoch=555.0, train_loss_step=525.0, train_loss_epoch=565.0, val_loss_step=576.0]\n","Epoch 47:  85% 160/188 [00:18<00:03,  8.75it/s, loss=554, v_num=0, val_loss_epoch=555.0, train_loss_step=520.0, train_loss_epoch=565.0, val_loss_step=576.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 47:  96% 180/188 [00:19<00:00,  9.38it/s, loss=554, v_num=0, val_loss_epoch=555.0, train_loss_step=520.0, train_loss_epoch=565.0, val_loss_step=576.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.19it/s]\u001b[A\n","Epoch 47: 100% 188/188 [00:20<00:00,  9.19it/s, loss=554, v_num=0, val_loss_epoch=553.0, train_loss_step=538.0, train_loss_epoch=565.0, val_loss_step=565.0]\n","Epoch 48:  85% 160/188 [00:19<00:03,  8.13it/s, loss=561, v_num=0, val_loss_epoch=553.0, train_loss_step=525.0, train_loss_epoch=565.0, val_loss_step=565.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 48:  96% 180/188 [00:20<00:00,  8.76it/s, loss=561, v_num=0, val_loss_epoch=553.0, train_loss_step=525.0, train_loss_epoch=565.0, val_loss_step=565.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.72it/s]\u001b[A\n","Epoch 48: 100% 188/188 [00:21<00:00,  8.66it/s, loss=554, v_num=0, val_loss_epoch=552.0, train_loss_step=526.0, train_loss_epoch=563.0, val_loss_step=560.0]\n","Epoch 49:  85% 160/188 [00:18<00:03,  8.86it/s, loss=568, v_num=0, val_loss_epoch=552.0, train_loss_step=634.0, train_loss_epoch=563.0, val_loss_step=560.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 49:  96% 180/188 [00:19<00:00,  9.47it/s, loss=568, v_num=0, val_loss_epoch=552.0, train_loss_step=634.0, train_loss_epoch=563.0, val_loss_step=560.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.79it/s]\u001b[A\n","Epoch 49: 100% 188/188 [00:20<00:00,  9.32it/s, loss=566, v_num=0, val_loss_epoch=550.0, train_loss_step=475.0, train_loss_epoch=562.0, val_loss_step=568.0]\n","Epoch 50:  85% 160/188 [00:21<00:03,  7.38it/s, loss=565, v_num=0, val_loss_epoch=550.0, train_loss_step=562.0, train_loss_epoch=562.0, val_loss_step=568.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 50:  96% 180/188 [00:22<00:01,  7.92it/s, loss=565, v_num=0, val_loss_epoch=550.0, train_loss_step=562.0, train_loss_epoch=562.0, val_loss_step=568.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.14it/s]\u001b[A\n","Epoch 50: 100% 188/188 [00:23<00:00,  7.88it/s, loss=564, v_num=0, val_loss_epoch=549.0, train_loss_step=543.0, train_loss_epoch=560.0, val_loss_step=559.0]\n","Epoch 51:  85% 160/188 [00:18<00:03,  8.44it/s, loss=539, v_num=0, val_loss_epoch=549.0, train_loss_step=603.0, train_loss_epoch=560.0, val_loss_step=559.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 51:  96% 180/188 [00:19<00:00,  9.06it/s, loss=539, v_num=0, val_loss_epoch=549.0, train_loss_step=603.0, train_loss_epoch=560.0, val_loss_step=559.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.86it/s]\u001b[A\n","Epoch 51: 100% 188/188 [00:20<00:00,  8.96it/s, loss=535, v_num=0, val_loss_epoch=548.0, train_loss_step=505.0, train_loss_epoch=560.0, val_loss_step=563.0]\n","Epoch 52:  85% 160/188 [00:19<00:03,  8.01it/s, loss=555, v_num=0, val_loss_epoch=548.0, train_loss_step=567.0, train_loss_epoch=560.0, val_loss_step=563.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 52:  96% 180/188 [00:21<00:00,  8.48it/s, loss=555, v_num=0, val_loss_epoch=548.0, train_loss_step=567.0, train_loss_epoch=560.0, val_loss_step=563.0]\n","Validating:  85% 40/47 [00:02<00:00, 20.39it/s]\u001b[A\n","Epoch 52: 100% 188/188 [00:22<00:00,  8.40it/s, loss=555, v_num=0, val_loss_epoch=546.0, train_loss_step=495.0, train_loss_epoch=558.0, val_loss_step=561.0]\n","Epoch 53:  85% 160/188 [00:18<00:03,  8.52it/s, loss=532, v_num=0, val_loss_epoch=546.0, train_loss_step=487.0, train_loss_epoch=558.0, val_loss_step=561.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 53:  96% 180/188 [00:19<00:00,  9.11it/s, loss=532, v_num=0, val_loss_epoch=546.0, train_loss_step=487.0, train_loss_epoch=558.0, val_loss_step=561.0]\n","Validating:  85% 40/47 [00:01<00:00, 21.77it/s]\u001b[A\n","Epoch 53: 100% 188/188 [00:20<00:00,  8.95it/s, loss=532, v_num=0, val_loss_epoch=545.0, train_loss_step=555.0, train_loss_epoch=557.0, val_loss_step=553.0]\n","Epoch 54:  85% 160/188 [00:19<00:03,  8.10it/s, loss=555, v_num=0, val_loss_epoch=545.0, train_loss_step=579.0, train_loss_epoch=557.0, val_loss_step=553.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 54:  96% 180/188 [00:21<00:00,  8.52it/s, loss=555, v_num=0, val_loss_epoch=545.0, train_loss_step=579.0, train_loss_epoch=557.0, val_loss_step=553.0]\n","Validating:  85% 40/47 [00:02<00:00, 19.24it/s]\u001b[A\n","Epoch 54: 100% 188/188 [00:22<00:00,  8.44it/s, loss=552, v_num=0, val_loss_epoch=545.0, train_loss_step=507.0, train_loss_epoch=557.0, val_loss_step=553.0]\n","Epoch 55:  85% 160/188 [00:19<00:03,  8.15it/s, loss=548, v_num=0, val_loss_epoch=545.0, train_loss_step=540.0, train_loss_epoch=557.0, val_loss_step=553.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 55:  96% 180/188 [00:20<00:00,  8.75it/s, loss=548, v_num=0, val_loss_epoch=545.0, train_loss_step=540.0, train_loss_epoch=557.0, val_loss_step=553.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.92it/s]\u001b[A\n","Epoch 55: 100% 188/188 [00:21<00:00,  8.65it/s, loss=541, v_num=0, val_loss_epoch=544.0, train_loss_step=448.0, train_loss_epoch=556.0, val_loss_step=569.0]\n","Epoch 56:  85% 160/188 [00:20<00:03,  7.63it/s, loss=562, v_num=0, val_loss_epoch=544.0, train_loss_step=583.0, train_loss_epoch=556.0, val_loss_step=569.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 56:  96% 180/188 [00:21<00:00,  8.22it/s, loss=562, v_num=0, val_loss_epoch=544.0, train_loss_step=583.0, train_loss_epoch=556.0, val_loss_step=569.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.42it/s]\u001b[A\n","Epoch 56: 100% 188/188 [00:23<00:00,  8.16it/s, loss=553, v_num=0, val_loss_epoch=542.0, train_loss_step=421.0, train_loss_epoch=555.0, val_loss_step=562.0]\n","Epoch 57:  85% 160/188 [00:18<00:03,  8.76it/s, loss=553, v_num=0, val_loss_epoch=542.0, train_loss_step=566.0, train_loss_epoch=555.0, val_loss_step=562.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 57:  96% 180/188 [00:19<00:00,  9.38it/s, loss=553, v_num=0, val_loss_epoch=542.0, train_loss_step=566.0, train_loss_epoch=555.0, val_loss_step=562.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.44it/s]\u001b[A\n","Epoch 57: 100% 188/188 [00:20<00:00,  9.24it/s, loss=548, v_num=0, val_loss_epoch=544.0, train_loss_step=517.0, train_loss_epoch=554.0, val_loss_step=545.0]\n","Epoch 58:  85% 160/188 [00:20<00:03,  7.99it/s, loss=563, v_num=0, val_loss_epoch=544.0, train_loss_step=575.0, train_loss_epoch=554.0, val_loss_step=545.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 58:  96% 180/188 [00:21<00:00,  8.42it/s, loss=563, v_num=0, val_loss_epoch=544.0, train_loss_step=575.0, train_loss_epoch=554.0, val_loss_step=545.0]\n","Validating:  85% 40/47 [00:02<00:00, 18.37it/s]\u001b[A\n","Epoch 58: 100% 188/188 [00:22<00:00,  8.30it/s, loss=550, v_num=0, val_loss_epoch=541.0, train_loss_step=383.0, train_loss_epoch=553.0, val_loss_step=557.0]\n","Epoch 59:  85% 160/188 [00:20<00:03,  7.89it/s, loss=555, v_num=0, val_loss_epoch=541.0, train_loss_step=530.0, train_loss_epoch=553.0, val_loss_step=557.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 59:  96% 180/188 [00:21<00:00,  8.48it/s, loss=555, v_num=0, val_loss_epoch=541.0, train_loss_step=530.0, train_loss_epoch=553.0, val_loss_step=557.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.79it/s]\u001b[A\n","Epoch 59: 100% 188/188 [00:22<00:00,  8.40it/s, loss=556, v_num=0, val_loss_epoch=543.0, train_loss_step=551.0, train_loss_epoch=553.0, val_loss_step=548.0]\n","Epoch 60:  85% 160/188 [00:20<00:03,  7.70it/s, loss=542, v_num=0, val_loss_epoch=543.0, train_loss_step=558.0, train_loss_epoch=553.0, val_loss_step=548.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 60:  96% 180/188 [00:21<00:00,  8.27it/s, loss=542, v_num=0, val_loss_epoch=543.0, train_loss_step=558.0, train_loss_epoch=553.0, val_loss_step=548.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.32it/s]\u001b[A\n","Epoch 60: 100% 188/188 [00:22<00:00,  8.19it/s, loss=544, v_num=0, val_loss_epoch=543.0, train_loss_step=560.0, train_loss_epoch=553.0, val_loss_step=554.0]\n","Epoch 61:  85% 160/188 [00:18<00:03,  8.43it/s, loss=555, v_num=0, val_loss_epoch=543.0, train_loss_step=596.0, train_loss_epoch=553.0, val_loss_step=554.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 61:  96% 180/188 [00:19<00:00,  9.02it/s, loss=555, v_num=0, val_loss_epoch=543.0, train_loss_step=596.0, train_loss_epoch=553.0, val_loss_step=554.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.03it/s]\u001b[A\n","Epoch 61: 100% 188/188 [00:21<00:00,  8.88it/s, loss=555, v_num=0, val_loss_epoch=543.0, train_loss_step=533.0, train_loss_epoch=552.0, val_loss_step=557.0]\n","Epoch 62:  85% 160/188 [00:20<00:03,  7.79it/s, loss=556, v_num=0, val_loss_epoch=543.0, train_loss_step=607.0, train_loss_epoch=552.0, val_loss_step=557.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 62:  96% 180/188 [00:21<00:00,  8.38it/s, loss=556, v_num=0, val_loss_epoch=543.0, train_loss_step=607.0, train_loss_epoch=552.0, val_loss_step=557.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.37it/s]\u001b[A\n","Epoch 62: 100% 188/188 [00:22<00:00,  8.28it/s, loss=550, v_num=0, val_loss_epoch=540.0, train_loss_step=474.0, train_loss_epoch=551.0, val_loss_step=545.0]\n","Epoch 63:  85% 160/188 [00:19<00:03,  8.05it/s, loss=559, v_num=0, val_loss_epoch=540.0, train_loss_step=533.0, train_loss_epoch=551.0, val_loss_step=545.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 63:  96% 180/188 [00:20<00:00,  8.62it/s, loss=559, v_num=0, val_loss_epoch=540.0, train_loss_step=533.0, train_loss_epoch=551.0, val_loss_step=545.0]\n","Validating:  85% 40/47 [00:02<00:00, 19.79it/s]\u001b[A\n","Epoch 63: 100% 188/188 [00:22<00:00,  8.38it/s, loss=556, v_num=0, val_loss_epoch=539.0, train_loss_step=462.0, train_loss_epoch=551.0, val_loss_step=542.0]\n","Epoch 64:  85% 160/188 [00:20<00:03,  7.84it/s, loss=554, v_num=0, val_loss_epoch=539.0, train_loss_step=638.0, train_loss_epoch=551.0, val_loss_step=542.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 64:  96% 180/188 [00:21<00:00,  8.41it/s, loss=554, v_num=0, val_loss_epoch=539.0, train_loss_step=638.0, train_loss_epoch=551.0, val_loss_step=542.0]\n","Validating:  85% 40/47 [00:01<00:00, 21.42it/s]\u001b[A\n","Epoch 64: 100% 188/188 [00:22<00:00,  8.30it/s, loss=554, v_num=0, val_loss_epoch=541.0, train_loss_step=536.0, train_loss_epoch=551.0, val_loss_step=566.0]\n","Epoch 65:  85% 160/188 [00:19<00:03,  8.34it/s, loss=545, v_num=0, val_loss_epoch=541.0, train_loss_step=554.0, train_loss_epoch=551.0, val_loss_step=566.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 65:  96% 180/188 [00:20<00:00,  8.87it/s, loss=545, v_num=0, val_loss_epoch=541.0, train_loss_step=554.0, train_loss_epoch=551.0, val_loss_step=566.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.68it/s]\u001b[A\n","Epoch 65: 100% 188/188 [00:21<00:00,  8.57it/s, loss=541, v_num=0, val_loss_epoch=540.0, train_loss_step=465.0, train_loss_epoch=551.0, val_loss_step=554.0]\n","Epoch 66:  85% 160/188 [00:20<00:03,  7.99it/s, loss=557, v_num=0, val_loss_epoch=540.0, train_loss_step=512.0, train_loss_epoch=551.0, val_loss_step=554.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 66:  96% 180/188 [00:20<00:00,  8.58it/s, loss=557, v_num=0, val_loss_epoch=540.0, train_loss_step=512.0, train_loss_epoch=551.0, val_loss_step=554.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.20it/s]\u001b[A\n","Epoch 66: 100% 188/188 [00:22<00:00,  8.47it/s, loss=550, v_num=0, val_loss_epoch=541.0, train_loss_step=463.0, train_loss_epoch=549.0, val_loss_step=562.0]\n","Epoch 67:  85% 160/188 [00:19<00:03,  8.05it/s, loss=550, v_num=0, val_loss_epoch=541.0, train_loss_step=539.0, train_loss_epoch=549.0, val_loss_step=562.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 67:  96% 180/188 [00:21<00:00,  8.49it/s, loss=550, v_num=0, val_loss_epoch=541.0, train_loss_step=539.0, train_loss_epoch=549.0, val_loss_step=562.0]\n","Validating:  85% 40/47 [00:02<00:00, 16.32it/s]\u001b[A\n","Epoch 67: 100% 188/188 [00:22<00:00,  8.21it/s, loss=550, v_num=0, val_loss_epoch=539.0, train_loss_step=517.0, train_loss_epoch=549.0, val_loss_step=542.0]\n","Epoch 68:  85% 160/188 [00:20<00:03,  7.93it/s, loss=540, v_num=0, val_loss_epoch=539.0, train_loss_step=510.0, train_loss_epoch=549.0, val_loss_step=542.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 68:  96% 180/188 [00:21<00:00,  8.34it/s, loss=540, v_num=0, val_loss_epoch=539.0, train_loss_step=510.0, train_loss_epoch=549.0, val_loss_step=542.0]\n","Validating:  85% 40/47 [00:02<00:00, 18.64it/s]\u001b[A\n","Epoch 68: 100% 188/188 [00:22<00:00,  8.25it/s, loss=534, v_num=0, val_loss_epoch=537.0, train_loss_step=507.0, train_loss_epoch=548.0, val_loss_step=552.0]\n","Epoch 69:  85% 160/188 [00:20<00:03,  7.96it/s, loss=537, v_num=0, val_loss_epoch=537.0, train_loss_step=594.0, train_loss_epoch=548.0, val_loss_step=552.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 69:  96% 180/188 [00:21<00:00,  8.33it/s, loss=537, v_num=0, val_loss_epoch=537.0, train_loss_step=594.0, train_loss_epoch=548.0, val_loss_step=552.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.23it/s]\u001b[A\n","Epoch 69: 100% 188/188 [00:22<00:00,  8.22it/s, loss=534, v_num=0, val_loss_epoch=536.0, train_loss_step=501.0, train_loss_epoch=546.0, val_loss_step=549.0]\n","Epoch 70:  85% 160/188 [00:18<00:03,  8.55it/s, loss=565, v_num=0, val_loss_epoch=536.0, train_loss_step=561.0, train_loss_epoch=546.0, val_loss_step=549.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 70:  96% 180/188 [00:19<00:00,  9.18it/s, loss=565, v_num=0, val_loss_epoch=536.0, train_loss_step=561.0, train_loss_epoch=546.0, val_loss_step=549.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.57it/s]\u001b[A\n","Epoch 70: 100% 188/188 [00:20<00:00,  9.05it/s, loss=562, v_num=0, val_loss_epoch=533.0, train_loss_step=456.0, train_loss_epoch=546.0, val_loss_step=552.0]\n","Epoch 71:  85% 160/188 [00:18<00:03,  8.53it/s, loss=536, v_num=0, val_loss_epoch=533.0, train_loss_step=531.0, train_loss_epoch=546.0, val_loss_step=552.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 71:  96% 180/188 [00:19<00:00,  9.02it/s, loss=536, v_num=0, val_loss_epoch=533.0, train_loss_step=531.0, train_loss_epoch=546.0, val_loss_step=552.0]\n","Validating:  85% 40/47 [00:02<00:00, 16.05it/s]\u001b[A\n","Epoch 71: 100% 188/188 [00:21<00:00,  8.63it/s, loss=537, v_num=0, val_loss_epoch=533.0, train_loss_step=557.0, train_loss_epoch=545.0, val_loss_step=553.0]\n","Epoch 72:  85% 160/188 [00:18<00:03,  8.66it/s, loss=556, v_num=0, val_loss_epoch=533.0, train_loss_step=545.0, train_loss_epoch=545.0, val_loss_step=553.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 72:  96% 180/188 [00:19<00:00,  9.27it/s, loss=556, v_num=0, val_loss_epoch=533.0, train_loss_step=545.0, train_loss_epoch=545.0, val_loss_step=553.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.14it/s]\u001b[A\n","Epoch 72: 100% 188/188 [00:20<00:00,  9.14it/s, loss=552, v_num=0, val_loss_epoch=534.0, train_loss_step=444.0, train_loss_epoch=545.0, val_loss_step=548.0]\n","Epoch 73:  85% 160/188 [00:18<00:03,  8.73it/s, loss=545, v_num=0, val_loss_epoch=534.0, train_loss_step=565.0, train_loss_epoch=545.0, val_loss_step=548.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 73:  96% 180/188 [00:19<00:00,  9.19it/s, loss=545, v_num=0, val_loss_epoch=534.0, train_loss_step=565.0, train_loss_epoch=545.0, val_loss_step=548.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.06it/s]\u001b[A\n","Epoch 73: 100% 188/188 [00:21<00:00,  8.88it/s, loss=542, v_num=0, val_loss_epoch=531.0, train_loss_step=474.0, train_loss_epoch=543.0, val_loss_step=549.0]\n","Epoch 74:  85% 160/188 [00:19<00:03,  8.24it/s, loss=533, v_num=0, val_loss_epoch=531.0, train_loss_step=508.0, train_loss_epoch=543.0, val_loss_step=549.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 74:  96% 180/188 [00:20<00:00,  8.85it/s, loss=533, v_num=0, val_loss_epoch=531.0, train_loss_step=508.0, train_loss_epoch=543.0, val_loss_step=549.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.38it/s]\u001b[A\n","Epoch 74: 100% 188/188 [00:21<00:00,  8.75it/s, loss=533, v_num=0, val_loss_epoch=534.0, train_loss_step=517.0, train_loss_epoch=542.0, val_loss_step=570.0]\n","Epoch 75:  85% 160/188 [00:18<00:03,  8.56it/s, loss=532, v_num=0, val_loss_epoch=534.0, train_loss_step=569.0, train_loss_epoch=542.0, val_loss_step=570.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 75:  96% 180/188 [00:19<00:00,  9.07it/s, loss=532, v_num=0, val_loss_epoch=534.0, train_loss_step=569.0, train_loss_epoch=542.0, val_loss_step=570.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.98it/s]\u001b[A\n","Epoch 75: 100% 188/188 [00:21<00:00,  8.78it/s, loss=532, v_num=0, val_loss_epoch=533.0, train_loss_step=485.0, train_loss_epoch=542.0, val_loss_step=548.0]\n","Epoch 76:  85% 160/188 [00:19<00:03,  8.35it/s, loss=539, v_num=0, val_loss_epoch=533.0, train_loss_step=550.0, train_loss_epoch=542.0, val_loss_step=548.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 76:  96% 180/188 [00:20<00:00,  8.97it/s, loss=539, v_num=0, val_loss_epoch=533.0, train_loss_step=550.0, train_loss_epoch=542.0, val_loss_step=548.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.39it/s]\u001b[A\n","Epoch 76: 100% 188/188 [00:21<00:00,  8.86it/s, loss=536, v_num=0, val_loss_epoch=531.0, train_loss_step=469.0, train_loss_epoch=542.0, val_loss_step=543.0]\n","Epoch 77:  85% 160/188 [00:18<00:03,  8.66it/s, loss=540, v_num=0, val_loss_epoch=531.0, train_loss_step=545.0, train_loss_epoch=542.0, val_loss_step=543.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 77:  96% 180/188 [00:19<00:00,  9.28it/s, loss=540, v_num=0, val_loss_epoch=531.0, train_loss_step=545.0, train_loss_epoch=542.0, val_loss_step=543.0]\n","Validating:  85% 40/47 [00:01<00:00, 21.61it/s]\u001b[A\n","Epoch 77: 100% 188/188 [00:20<00:00,  9.04it/s, loss=542, v_num=0, val_loss_epoch=532.0, train_loss_step=518.0, train_loss_epoch=542.0, val_loss_step=535.0]\n","Epoch 78:  85% 160/188 [00:21<00:03,  7.49it/s, loss=540, v_num=0, val_loss_epoch=532.0, train_loss_step=561.0, train_loss_epoch=542.0, val_loss_step=535.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 78:  96% 180/188 [00:22<00:00,  8.09it/s, loss=540, v_num=0, val_loss_epoch=532.0, train_loss_step=561.0, train_loss_epoch=542.0, val_loss_step=535.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.80it/s]\u001b[A\n","Epoch 78: 100% 188/188 [00:23<00:00,  8.04it/s, loss=537, v_num=0, val_loss_epoch=531.0, train_loss_step=482.0, train_loss_epoch=541.0, val_loss_step=538.0]\n","Epoch 79:  85% 160/188 [00:18<00:03,  8.78it/s, loss=536, v_num=0, val_loss_epoch=531.0, train_loss_step=574.0, train_loss_epoch=541.0, val_loss_step=538.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 79:  96% 180/188 [00:19<00:00,  9.37it/s, loss=536, v_num=0, val_loss_epoch=531.0, train_loss_step=574.0, train_loss_epoch=541.0, val_loss_step=538.0]\n","Validating:  85% 40/47 [00:02<00:00, 18.79it/s]\u001b[A\n","Epoch 79: 100% 188/188 [00:20<00:00,  9.03it/s, loss=535, v_num=0, val_loss_epoch=531.0, train_loss_step=512.0, train_loss_epoch=540.0, val_loss_step=539.0]\n","Epoch 80:  85% 160/188 [00:19<00:03,  8.27it/s, loss=545, v_num=0, val_loss_epoch=531.0, train_loss_step=570.0, train_loss_epoch=540.0, val_loss_step=539.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 80:  96% 180/188 [00:20<00:00,  8.87it/s, loss=545, v_num=0, val_loss_epoch=531.0, train_loss_step=570.0, train_loss_epoch=540.0, val_loss_step=539.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.27it/s]\u001b[A\n","Epoch 80: 100% 188/188 [00:21<00:00,  8.77it/s, loss=540, v_num=0, val_loss_epoch=531.0, train_loss_step=491.0, train_loss_epoch=540.0, val_loss_step=524.0]\n","Epoch 81:  85% 160/188 [00:18<00:03,  8.67it/s, loss=527, v_num=0, val_loss_epoch=531.0, train_loss_step=529.0, train_loss_epoch=540.0, val_loss_step=524.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 81:  96% 180/188 [00:19<00:00,  9.29it/s, loss=527, v_num=0, val_loss_epoch=531.0, train_loss_step=529.0, train_loss_epoch=540.0, val_loss_step=524.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.63it/s]\u001b[A\n","Epoch 81: 100% 188/188 [00:20<00:00,  9.14it/s, loss=521, v_num=0, val_loss_epoch=527.0, train_loss_step=479.0, train_loss_epoch=538.0, val_loss_step=532.0]\n","Epoch 82:  85% 160/188 [00:19<00:03,  8.07it/s, loss=529, v_num=0, val_loss_epoch=527.0, train_loss_step=558.0, train_loss_epoch=538.0, val_loss_step=532.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 82:  96% 180/188 [00:20<00:00,  8.67it/s, loss=529, v_num=0, val_loss_epoch=527.0, train_loss_step=558.0, train_loss_epoch=538.0, val_loss_step=532.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.97it/s]\u001b[A\n","Epoch 82: 100% 188/188 [00:21<00:00,  8.58it/s, loss=530, v_num=0, val_loss_epoch=522.0, train_loss_step=526.0, train_loss_epoch=535.0, val_loss_step=531.0]\n","Epoch 83:  85% 160/188 [00:18<00:03,  8.49it/s, loss=545, v_num=0, val_loss_epoch=522.0, train_loss_step=532.0, train_loss_epoch=535.0, val_loss_step=531.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 83:  96% 180/188 [00:19<00:00,  9.11it/s, loss=545, v_num=0, val_loss_epoch=522.0, train_loss_step=532.0, train_loss_epoch=535.0, val_loss_step=531.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.58it/s]\u001b[A\n","Epoch 83: 100% 188/188 [00:20<00:00,  9.00it/s, loss=542, v_num=0, val_loss_epoch=524.0, train_loss_step=501.0, train_loss_epoch=533.0, val_loss_step=547.0]\n","Epoch 84:  85% 160/188 [00:20<00:03,  7.95it/s, loss=528, v_num=0, val_loss_epoch=524.0, train_loss_step=526.0, train_loss_epoch=533.0, val_loss_step=547.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 84:  96% 180/188 [00:21<00:00,  8.53it/s, loss=528, v_num=0, val_loss_epoch=524.0, train_loss_step=526.0, train_loss_epoch=533.0, val_loss_step=547.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.40it/s]\u001b[A\n","Epoch 84: 100% 188/188 [00:22<00:00,  8.44it/s, loss=527, v_num=0, val_loss_epoch=523.0, train_loss_step=524.0, train_loss_epoch=531.0, val_loss_step=528.0]\n","Epoch 85:  85% 160/188 [00:18<00:03,  8.52it/s, loss=537, v_num=0, val_loss_epoch=523.0, train_loss_step=609.0, train_loss_epoch=531.0, val_loss_step=528.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 85:  96% 180/188 [00:19<00:00,  9.13it/s, loss=537, v_num=0, val_loss_epoch=523.0, train_loss_step=609.0, train_loss_epoch=531.0, val_loss_step=528.0]\n","Epoch 85: 100% 188/188 [00:20<00:00,  9.01it/s, loss=532, v_num=0, val_loss_epoch=520.0, train_loss_step=488.0, train_loss_epoch=531.0, val_loss_step=535.0]\n","Epoch 86:  85% 160/188 [00:19<00:03,  8.09it/s, loss=525, v_num=0, val_loss_epoch=520.0, train_loss_step=451.0, train_loss_epoch=531.0, val_loss_step=535.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 86:  96% 180/188 [00:20<00:00,  8.71it/s, loss=525, v_num=0, val_loss_epoch=520.0, train_loss_step=451.0, train_loss_epoch=531.0, val_loss_step=535.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.03it/s]\u001b[A\n","Epoch 86: 100% 188/188 [00:21<00:00,  8.63it/s, loss=520, v_num=0, val_loss_epoch=519.0, train_loss_step=467.0, train_loss_epoch=529.0, val_loss_step=536.0]\n","Epoch 87:  85% 160/188 [00:19<00:03,  8.41it/s, loss=523, v_num=0, val_loss_epoch=519.0, train_loss_step=487.0, train_loss_epoch=529.0, val_loss_step=536.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 87:  96% 180/188 [00:20<00:00,  8.85it/s, loss=523, v_num=0, val_loss_epoch=519.0, train_loss_step=487.0, train_loss_epoch=529.0, val_loss_step=536.0]\n","Validating:  85% 40/47 [00:02<00:00, 16.16it/s]\u001b[A\n","Epoch 87: 100% 188/188 [00:21<00:00,  8.61it/s, loss=523, v_num=0, val_loss_epoch=516.0, train_loss_step=501.0, train_loss_epoch=529.0, val_loss_step=531.0]\n","Epoch 88:  85% 160/188 [00:19<00:03,  8.14it/s, loss=529, v_num=0, val_loss_epoch=516.0, train_loss_step=546.0, train_loss_epoch=529.0, val_loss_step=531.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 88:  96% 180/188 [00:20<00:00,  8.76it/s, loss=529, v_num=0, val_loss_epoch=516.0, train_loss_step=546.0, train_loss_epoch=529.0, val_loss_step=531.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.54it/s]\u001b[A\n","Epoch 88: 100% 188/188 [00:21<00:00,  8.66it/s, loss=526, v_num=0, val_loss_epoch=519.0, train_loss_step=436.0, train_loss_epoch=528.0, val_loss_step=545.0]\n","Epoch 89:  85% 160/188 [00:18<00:03,  8.74it/s, loss=523, v_num=0, val_loss_epoch=519.0, train_loss_step=536.0, train_loss_epoch=528.0, val_loss_step=545.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 89:  96% 180/188 [00:19<00:00,  9.34it/s, loss=523, v_num=0, val_loss_epoch=519.0, train_loss_step=536.0, train_loss_epoch=528.0, val_loss_step=545.0]\n","Validating:  85% 40/47 [00:01<00:00, 22.92it/s]\u001b[A\n","Epoch 89: 100% 188/188 [00:20<00:00,  9.21it/s, loss=516, v_num=0, val_loss_epoch=518.0, train_loss_step=426.0, train_loss_epoch=528.0, val_loss_step=531.0]\n","Epoch 90:  85% 160/188 [00:18<00:03,  8.53it/s, loss=530, v_num=0, val_loss_epoch=518.0, train_loss_step=511.0, train_loss_epoch=528.0, val_loss_step=531.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 90:  96% 180/188 [00:20<00:00,  8.94it/s, loss=530, v_num=0, val_loss_epoch=518.0, train_loss_step=511.0, train_loss_epoch=528.0, val_loss_step=531.0]\n","Validating:  85% 40/47 [00:02<00:00, 16.95it/s]\u001b[A\n","Epoch 90: 100% 188/188 [00:21<00:00,  8.74it/s, loss=533, v_num=0, val_loss_epoch=519.0, train_loss_step=597.0, train_loss_epoch=528.0, val_loss_step=540.0]\n","Epoch 91:  85% 160/188 [00:18<00:03,  8.84it/s, loss=518, v_num=0, val_loss_epoch=519.0, train_loss_step=464.0, train_loss_epoch=528.0, val_loss_step=540.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 91:  96% 180/188 [00:18<00:00,  9.48it/s, loss=518, v_num=0, val_loss_epoch=519.0, train_loss_step=464.0, train_loss_epoch=528.0, val_loss_step=540.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.12it/s]\u001b[A\n","Validating: 100% 47/47 [00:01<00:00, 24.55it/s]\u001b[AEpoch 00092: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 91: 100% 188/188 [00:20<00:00,  9.35it/s, loss=520, v_num=0, val_loss_epoch=516.0, train_loss_step=531.0, train_loss_epoch=528.0, val_loss_step=551.0]\n","Epoch 92:  85% 160/188 [00:18<00:03,  8.46it/s, loss=527, v_num=0, val_loss_epoch=516.0, train_loss_step=531.0, train_loss_epoch=528.0, val_loss_step=551.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 92:  96% 180/188 [00:20<00:00,  8.94it/s, loss=527, v_num=0, val_loss_epoch=516.0, train_loss_step=531.0, train_loss_epoch=528.0, val_loss_step=551.0]\n","Validating:  85% 40/47 [00:02<00:00, 17.38it/s]\u001b[A\n","Epoch 92: 100% 188/188 [00:21<00:00,  8.62it/s, loss=524, v_num=0, val_loss_epoch=516.0, train_loss_step=473.0, train_loss_epoch=525.0, val_loss_step=522.0]\n","Epoch 93:  85% 160/188 [00:18<00:03,  8.82it/s, loss=525, v_num=0, val_loss_epoch=516.0, train_loss_step=531.0, train_loss_epoch=525.0, val_loss_step=522.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 93:  96% 180/188 [00:19<00:00,  9.46it/s, loss=525, v_num=0, val_loss_epoch=516.0, train_loss_step=531.0, train_loss_epoch=525.0, val_loss_step=522.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.05it/s]\u001b[A\n","Epoch 93: 100% 188/188 [00:20<00:00,  9.33it/s, loss=525, v_num=0, val_loss_epoch=516.0, train_loss_step=454.0, train_loss_epoch=525.0, val_loss_step=524.0]\n","Epoch 94:  85% 160/188 [00:17<00:03,  8.91it/s, loss=522, v_num=0, val_loss_epoch=516.0, train_loss_step=453.0, train_loss_epoch=525.0, val_loss_step=524.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 94:  96% 180/188 [00:18<00:00,  9.54it/s, loss=522, v_num=0, val_loss_epoch=516.0, train_loss_step=453.0, train_loss_epoch=525.0, val_loss_step=524.0]\n","Validating:  85% 40/47 [00:01<00:00, 21.38it/s]\u001b[A\n","Epoch 94: 100% 188/188 [00:20<00:00,  9.26it/s, loss=521, v_num=0, val_loss_epoch=513.0, train_loss_step=534.0, train_loss_epoch=525.0, val_loss_step=522.0]\n","Epoch 95:  85% 160/188 [00:19<00:03,  8.32it/s, loss=522, v_num=0, val_loss_epoch=513.0, train_loss_step=518.0, train_loss_epoch=525.0, val_loss_step=522.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 95:  96% 180/188 [00:20<00:00,  8.93it/s, loss=522, v_num=0, val_loss_epoch=513.0, train_loss_step=518.0, train_loss_epoch=525.0, val_loss_step=522.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.39it/s]\u001b[A\n","Epoch 95: 100% 188/188 [00:21<00:00,  8.82it/s, loss=516, v_num=0, val_loss_epoch=516.0, train_loss_step=431.0, train_loss_epoch=526.0, val_loss_step=529.0]\n","Epoch 96:  85% 160/188 [00:18<00:03,  8.87it/s, loss=529, v_num=0, val_loss_epoch=516.0, train_loss_step=527.0, train_loss_epoch=526.0, val_loss_step=529.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 96:  96% 180/188 [00:18<00:00,  9.50it/s, loss=529, v_num=0, val_loss_epoch=516.0, train_loss_step=527.0, train_loss_epoch=526.0, val_loss_step=529.0]\n","Validating:  85% 40/47 [00:01<00:00, 19.76it/s]\u001b[A\n","Epoch 96: 100% 188/188 [00:20<00:00,  9.22it/s, loss=529, v_num=0, val_loss_epoch=515.0, train_loss_step=472.0, train_loss_epoch=526.0, val_loss_step=524.0]\n","Epoch 97:  85% 160/188 [00:21<00:03,  7.54it/s, loss=525, v_num=0, val_loss_epoch=515.0, train_loss_step=499.0, train_loss_epoch=526.0, val_loss_step=524.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 97:  96% 180/188 [00:22<00:00,  8.14it/s, loss=525, v_num=0, val_loss_epoch=515.0, train_loss_step=499.0, train_loss_epoch=526.0, val_loss_step=524.0]\n","Validating:  85% 40/47 [00:01<00:00, 24.01it/s]\u001b[A\n","Epoch 97: 100% 188/188 [00:23<00:00,  8.09it/s, loss=521, v_num=0, val_loss_epoch=515.0, train_loss_step=397.0, train_loss_epoch=525.0, val_loss_step=515.0]\n","Epoch 98:  85% 160/188 [00:17<00:03,  8.93it/s, loss=523, v_num=0, val_loss_epoch=515.0, train_loss_step=552.0, train_loss_epoch=525.0, val_loss_step=515.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 98:  96% 180/188 [00:18<00:00,  9.56it/s, loss=523, v_num=0, val_loss_epoch=515.0, train_loss_step=552.0, train_loss_epoch=525.0, val_loss_step=515.0]\n","Validating:  85% 40/47 [00:01<00:00, 23.95it/s]\u001b[A\n","Epoch 98: 100% 188/188 [00:19<00:00,  9.42it/s, loss=522, v_num=0, val_loss_epoch=516.0, train_loss_step=509.0, train_loss_epoch=524.0, val_loss_step=521.0]\n","Epoch 99:  85% 160/188 [00:18<00:03,  8.66it/s, loss=519, v_num=0, val_loss_epoch=516.0, train_loss_step=513.0, train_loss_epoch=524.0, val_loss_step=521.0]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 99:  96% 180/188 [00:19<00:00,  9.13it/s, loss=519, v_num=0, val_loss_epoch=516.0, train_loss_step=513.0, train_loss_epoch=524.0, val_loss_step=521.0]\n","Validating:  85% 40/47 [00:02<00:00, 15.26it/s]\u001b[A\n","Epoch 99: 100% 188/188 [00:21<00:00,  8.78it/s, loss=521, v_num=0, val_loss_epoch=513.0, train_loss_step=567.0, train_loss_epoch=524.0, val_loss_step=524.0]\n","Epoch 99: 100% 188/188 [00:21<00:00,  8.77it/s, loss=521, v_num=0, val_loss_epoch=513.0, train_loss_step=567.0, train_loss_epoch=524.0, val_loss_step=524.0]\n","Path to best model found during training: \n","/content/outputs/2023-06-12/02-59-27/lightning_logs/version_0/checkpoints/97-13817.ckpt\n"]}]},{"cell_type":"code","source":["# copy path file dari baris terakhir output cell di atas.\n","\n","best_ssl_model_ckpt = \"/content/outputs/2023-06-12/02-59-27/lightning_logs/version_0/checkpoints/97-13817.ckpt\""],"metadata":{"id":"nAskwksGLXVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**WARNING : Edit dulu**\n","\n","```1. /content/saint/utils/utils.py, line 42```\n","```\n","def auroc(self):\n","  return AUROC(num_classes=self.num_classes, task='multiclass')\n","```\n","```2. /content/saint/src/trainer.py, line 81```\n","```\n","def on_training_epoch_end(self, training_step_outputs):\n","```\n","```4. /content/saint/src/trainer.py, line 107```\n","```\n","def on_test_epoch_end(self):\n","```\n","```5. /content/saint/src/trainer.py, line 16```\n","```\n","super().__init__()\n","self.validation_step_outputs = []\n","self.transformer = transformer\n","```\n","```6. /content/saint/src/trainer.py, line 88```\n","```\n","def validation_step(self, batch, batch_idx):\n","    val_loss = self._shared_step(batch, self.valid_metric)\n","\n","    # log the outputs!\n","    self.log(f'val_loss', val_loss, on_step=False, \n","              on_epoch=True, prog_bar=True, logger=True)\n","    \n","    self.log(f'val_{self.metric}_epoch', self.valid_metric.compute(), prog_bar=True,)\n","    \n","    self.validation_step_outputs.append(val_loss)\n","    return val_loss\n","```\n","```7. /content/saint/src/trainer.py, line 98```\n","```\n","def on_validation_epoch_end(self):\n","    self.log(f'val_{self.metric}_epoch', self.valid_metric.compute(), prog_bar=True,)\n","\n","    # reset after each epoch\n","    self.valid_metric.reset()\n","    \n","    epoch_average = torch.stack(self.validation_step_outputs).mean()\n","    self.log(\"validation_epoch_average\", epoch_average)\n","    self.validation_step_outputs.clear()  # free memory\n","```\n","```Reference : https://github.com/Lightning-AI/lightning/discussions/17182```\n","\n","```8. /content/saint/src/train.py, line 42```\n","```\n","trainer.fit(model, dataloaders['train_loader'], dataloaders['validation_loader'])\n","```"],"metadata":{"id":"pl-QqhdQBdvr"}},{"cell_type":"code","source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"I9ogLvZUJHr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SUP from SSL\n","\n","!python /content/saint/main.py experiment=supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_sup \\\n","  experiment.pretrained_checkpoint={best_ssl_model_ckpt}"],"metadata":{"id":"kyF5SYkKLY2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686541899991,"user_tz":-420,"elapsed":751012,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"67a35993-4c6a-4317-db0c-c22399965706"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-12 03:39:14.589526: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-12 03:39:15.600510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/main.py:11: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Global seed set to 1234\n","/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)\n","Initializing supervised task using pretrained model:\n","/content/outputs/2023-06-12/02-59-27/lightning_logs/version_0/checkpoints/97-13817.ckpt\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name         | Type             | Params\n","--------------------------------------------------\n","0 | transformer  | Encoder          | 65.0 K\n","1 | embedding    | Embedding        | 3.2 K \n","2 | fc           | Linear           | 165   \n","3 | criterion    | CrossEntropyLoss | 0     \n","4 | train_metric | AUROC            | 0     \n","--------------------------------------------------\n","68.3 K    Trainable params\n","0         Non-trainable params\n","68.3 K    Total params\n","0.273     Total estimated model params size (MB)\n","Global seed set to 1234\n","Epoch 0:  73% 80/110 [00:06<00:02, 12.89it/s, loss=1.49, v_num=0, val_loss=1.610, val_auroc_epoch=0.494, train_loss_step=1.410]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  91% 100/110 [00:06<00:00, 14.66it/s, loss=1.49, v_num=0, val_loss=1.610, val_auroc_epoch=0.494, train_loss_step=1.410]\n","Validating:  85% 40/47 [00:01<00:00, 35.83it/s]\u001b[A\n","Epoch 0: 100% 110/110 [00:07<00:00, 14.49it/s, loss=1.46, v_num=0, val_loss=1.560, val_auroc_epoch=0.595, train_loss_step=1.290, train_loss_epoch=1.570, train_auroc_epoch=0.541]\n","Epoch 1:  73% 80/110 [00:05<00:01, 15.22it/s, loss=1.47, v_num=0, val_loss=1.560, val_auroc_epoch=0.595, train_loss_step=1.410, train_loss_epoch=1.570, train_auroc_epoch=0.541]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 1:  91% 100/110 [00:05<00:00, 17.03it/s, loss=1.47, v_num=0, val_loss=1.560, val_auroc_epoch=0.595, train_loss_step=1.410, train_loss_epoch=1.570, train_auroc_epoch=0.541]\n","Validating:  85% 40/47 [00:01<00:00, 33.08it/s]\u001b[A\n","Epoch 1: 100% 110/110 [00:06<00:00, 16.24it/s, loss=1.45, v_num=0, val_loss=1.530, val_auroc_epoch=0.620, train_loss_step=1.340, train_loss_epoch=1.550, train_auroc_epoch=0.552]\n","Epoch 2:  73% 80/110 [00:06<00:02, 12.70it/s, loss=1.46, v_num=0, val_loss=1.530, val_auroc_epoch=0.620, train_loss_step=1.430, train_loss_epoch=1.550, train_auroc_epoch=0.552]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 2:  91% 100/110 [00:06<00:00, 14.45it/s, loss=1.46, v_num=0, val_loss=1.530, val_auroc_epoch=0.620, train_loss_step=1.430, train_loss_epoch=1.550, train_auroc_epoch=0.552]\n","Validating:  85% 40/47 [00:01<00:00, 35.27it/s]\u001b[A\n","Epoch 2: 100% 110/110 [00:07<00:00, 14.30it/s, loss=1.43, v_num=0, val_loss=1.490, val_auroc_epoch=0.639, train_loss_step=1.210, train_loss_epoch=1.510, train_auroc_epoch=0.593]\n","Epoch 3:  73% 80/110 [00:05<00:02, 14.99it/s, loss=1.41, v_num=0, val_loss=1.490, val_auroc_epoch=0.639, train_loss_step=1.270, train_loss_epoch=1.510, train_auroc_epoch=0.593]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 3:  91% 100/110 [00:05<00:00, 16.78it/s, loss=1.41, v_num=0, val_loss=1.490, val_auroc_epoch=0.639, train_loss_step=1.270, train_loss_epoch=1.510, train_auroc_epoch=0.593]\n","Validating:  85% 40/47 [00:01<00:00, 31.36it/s]\u001b[A\n","Epoch 3: 100% 110/110 [00:06<00:00, 15.82it/s, loss=1.38, v_num=0, val_loss=1.450, val_auroc_epoch=0.680, train_loss_step=1.280, train_loss_epoch=1.480, train_auroc_epoch=0.626]\n","Epoch 4:  73% 80/110 [00:06<00:02, 12.96it/s, loss=1.39, v_num=0, val_loss=1.450, val_auroc_epoch=0.680, train_loss_step=1.260, train_loss_epoch=1.480, train_auroc_epoch=0.626]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 4:  91% 100/110 [00:06<00:00, 14.72it/s, loss=1.39, v_num=0, val_loss=1.450, val_auroc_epoch=0.680, train_loss_step=1.260, train_loss_epoch=1.480, train_auroc_epoch=0.626]\n","Validating:  85% 40/47 [00:01<00:00, 34.90it/s]\u001b[A\n","Epoch 4: 100% 110/110 [00:07<00:00, 14.52it/s, loss=1.38, v_num=0, val_loss=1.460, val_auroc_epoch=0.704, train_loss_step=1.150, train_loss_epoch=1.450, train_auroc_epoch=0.648]\n","Epoch 5:  73% 80/110 [00:05<00:01, 15.06it/s, loss=1.36, v_num=0, val_loss=1.460, val_auroc_epoch=0.704, train_loss_step=1.350, train_loss_epoch=1.450, train_auroc_epoch=0.648]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 5:  91% 100/110 [00:05<00:00, 16.70it/s, loss=1.36, v_num=0, val_loss=1.460, val_auroc_epoch=0.704, train_loss_step=1.350, train_loss_epoch=1.450, train_auroc_epoch=0.648]\n","Validating:  85% 40/47 [00:01<00:00, 30.05it/s]\u001b[A\n","Epoch 5: 100% 110/110 [00:06<00:00, 15.75it/s, loss=1.33, v_num=0, val_loss=1.410, val_auroc_epoch=0.718, train_loss_step=1.120, train_loss_epoch=1.440, train_auroc_epoch=0.677]\n","Epoch 6:  73% 80/110 [00:06<00:02, 13.04it/s, loss=1.36, v_num=0, val_loss=1.410, val_auroc_epoch=0.718, train_loss_step=1.290, train_loss_epoch=1.440, train_auroc_epoch=0.677]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 6:  91% 100/110 [00:06<00:00, 14.76it/s, loss=1.36, v_num=0, val_loss=1.410, val_auroc_epoch=0.718, train_loss_step=1.290, train_loss_epoch=1.440, train_auroc_epoch=0.677]\n","Validating:  85% 40/47 [00:01<00:00, 34.99it/s]\u001b[A\n","Epoch 6: 100% 110/110 [00:07<00:00, 14.59it/s, loss=1.33, v_num=0, val_loss=1.400, val_auroc_epoch=0.723, train_loss_step=1.150, train_loss_epoch=1.400, train_auroc_epoch=0.698]\n","Epoch 7:  73% 80/110 [00:05<00:01, 15.25it/s, loss=1.37, v_num=0, val_loss=1.400, val_auroc_epoch=0.723, train_loss_step=1.340, train_loss_epoch=1.400, train_auroc_epoch=0.698]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 7:  91% 100/110 [00:05<00:00, 16.75it/s, loss=1.37, v_num=0, val_loss=1.400, val_auroc_epoch=0.723, train_loss_step=1.340, train_loss_epoch=1.400, train_auroc_epoch=0.698]\n","Validating:  85% 40/47 [00:01<00:00, 29.29it/s]\u001b[A\n","Epoch 7: 100% 110/110 [00:06<00:00, 15.83it/s, loss=1.33, v_num=0, val_loss=1.390, val_auroc_epoch=0.728, train_loss_step=1.020, train_loss_epoch=1.400, train_auroc_epoch=0.699]\n","Epoch 8:  73% 80/110 [00:06<00:02, 13.18it/s, loss=1.3, v_num=0, val_loss=1.390, val_auroc_epoch=0.728, train_loss_step=1.080, train_loss_epoch=1.400, train_auroc_epoch=0.699]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 8:  91% 100/110 [00:06<00:00, 14.94it/s, loss=1.3, v_num=0, val_loss=1.390, val_auroc_epoch=0.728, train_loss_step=1.080, train_loss_epoch=1.400, train_auroc_epoch=0.699]\n","Validating:  85% 40/47 [00:01<00:00, 35.65it/s]\u001b[A\n","Epoch 8: 100% 110/110 [00:07<00:00, 14.76it/s, loss=1.29, v_num=0, val_loss=1.370, val_auroc_epoch=0.729, train_loss_step=1.140, train_loss_epoch=1.380, train_auroc_epoch=0.711]\n","Epoch 9:  73% 80/110 [00:05<00:01, 15.18it/s, loss=1.34, v_num=0, val_loss=1.370, val_auroc_epoch=0.729, train_loss_step=1.160, train_loss_epoch=1.380, train_auroc_epoch=0.711]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 9:  91% 100/110 [00:06<00:00, 16.65it/s, loss=1.34, v_num=0, val_loss=1.370, val_auroc_epoch=0.729, train_loss_step=1.160, train_loss_epoch=1.380, train_auroc_epoch=0.711]\n","Validating:  85% 40/47 [00:01<00:00, 29.37it/s]\u001b[A\n","Epoch 9: 100% 110/110 [00:06<00:00, 15.73it/s, loss=1.32, v_num=0, val_loss=1.350, val_auroc_epoch=0.735, train_loss_step=1.110, train_loss_epoch=1.360, train_auroc_epoch=0.720]\n","Epoch 10:  73% 80/110 [00:06<00:02, 13.19it/s, loss=1.29, v_num=0, val_loss=1.350, val_auroc_epoch=0.735, train_loss_step=1.320, train_loss_epoch=1.360, train_auroc_epoch=0.720]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 10:  91% 100/110 [00:06<00:00, 14.97it/s, loss=1.29, v_num=0, val_loss=1.350, val_auroc_epoch=0.735, train_loss_step=1.320, train_loss_epoch=1.360, train_auroc_epoch=0.720]\n","Validating:  85% 40/47 [00:01<00:00, 35.78it/s]\u001b[A\n","Epoch 10: 100% 110/110 [00:07<00:00, 14.78it/s, loss=1.28, v_num=0, val_loss=1.340, val_auroc_epoch=0.740, train_loss_step=1.380, train_loss_epoch=1.350, train_auroc_epoch=0.732]\n","Epoch 11:  73% 80/110 [00:05<00:02, 14.15it/s, loss=1.32, v_num=0, val_loss=1.340, val_auroc_epoch=0.740, train_loss_step=1.220, train_loss_epoch=1.350, train_auroc_epoch=0.732]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 11:  91% 100/110 [00:06<00:00, 15.48it/s, loss=1.32, v_num=0, val_loss=1.340, val_auroc_epoch=0.740, train_loss_step=1.220, train_loss_epoch=1.350, train_auroc_epoch=0.732]\n","Validating:  85% 40/47 [00:01<00:00, 27.30it/s]\u001b[A\n","Epoch 11: 100% 110/110 [00:07<00:00, 14.73it/s, loss=1.27, v_num=0, val_loss=1.340, val_auroc_epoch=0.740, train_loss_step=1.050, train_loss_epoch=1.340, train_auroc_epoch=0.738]\n","Epoch 12:  73% 80/110 [00:07<00:02, 10.82it/s, loss=1.33, v_num=0, val_loss=1.340, val_auroc_epoch=0.740, train_loss_step=1.230, train_loss_epoch=1.340, train_auroc_epoch=0.738]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 12:  91% 100/110 [00:07<00:00, 12.50it/s, loss=1.33, v_num=0, val_loss=1.340, val_auroc_epoch=0.740, train_loss_step=1.230, train_loss_epoch=1.340, train_auroc_epoch=0.738]\n","Validating:  85% 40/47 [00:01<00:00, 35.36it/s]\u001b[A\n","Epoch 12: 100% 110/110 [00:08<00:00, 12.53it/s, loss=1.28, v_num=0, val_loss=1.340, val_auroc_epoch=0.748, train_loss_step=1.030, train_loss_epoch=1.330, train_auroc_epoch=0.739]\n","Epoch 13:  73% 80/110 [00:05<00:02, 14.84it/s, loss=1.26, v_num=0, val_loss=1.340, val_auroc_epoch=0.748, train_loss_step=1.170, train_loss_epoch=1.330, train_auroc_epoch=0.739]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 13:  91% 100/110 [00:06<00:00, 16.41it/s, loss=1.26, v_num=0, val_loss=1.340, val_auroc_epoch=0.748, train_loss_step=1.170, train_loss_epoch=1.330, train_auroc_epoch=0.739]\n","Validating:  85% 40/47 [00:01<00:00, 29.66it/s]\u001b[A\n","Epoch 13: 100% 110/110 [00:07<00:00, 15.55it/s, loss=1.25, v_num=0, val_loss=1.320, val_auroc_epoch=0.748, train_loss_step=1.250, train_loss_epoch=1.320, train_auroc_epoch=0.744]\n","Epoch 14:  73% 80/110 [00:06<00:02, 13.12it/s, loss=1.28, v_num=0, val_loss=1.320, val_auroc_epoch=0.748, train_loss_step=1.120, train_loss_epoch=1.320, train_auroc_epoch=0.744]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 14:  91% 100/110 [00:06<00:00, 14.87it/s, loss=1.28, v_num=0, val_loss=1.320, val_auroc_epoch=0.748, train_loss_step=1.120, train_loss_epoch=1.320, train_auroc_epoch=0.744]\n","Validating:  85% 40/47 [00:01<00:00, 35.43it/s]\u001b[A\n","Epoch 14: 100% 110/110 [00:07<00:00, 14.69it/s, loss=1.25, v_num=0, val_loss=1.320, val_auroc_epoch=0.756, train_loss_step=0.982, train_loss_epoch=1.320, train_auroc_epoch=0.745]\n","Epoch 15:  73% 80/110 [00:05<00:01, 15.26it/s, loss=1.27, v_num=0, val_loss=1.320, val_auroc_epoch=0.756, train_loss_step=1.230, train_loss_epoch=1.320, train_auroc_epoch=0.745]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 15:  91% 100/110 [00:06<00:00, 16.65it/s, loss=1.27, v_num=0, val_loss=1.320, val_auroc_epoch=0.756, train_loss_step=1.230, train_loss_epoch=1.320, train_auroc_epoch=0.745]\n","Validating:  85% 40/47 [00:01<00:00, 29.06it/s]\u001b[A\n","Epoch 15: 100% 110/110 [00:06<00:00, 15.83it/s, loss=1.24, v_num=0, val_loss=1.310, val_auroc_epoch=0.757, train_loss_step=1.120, train_loss_epoch=1.300, train_auroc_epoch=0.759]\n","Epoch 16:  73% 80/110 [00:06<00:02, 13.18it/s, loss=1.26, v_num=0, val_loss=1.310, val_auroc_epoch=0.757, train_loss_step=1.240, train_loss_epoch=1.300, train_auroc_epoch=0.759]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 16:  91% 100/110 [00:06<00:00, 14.94it/s, loss=1.26, v_num=0, val_loss=1.310, val_auroc_epoch=0.757, train_loss_step=1.240, train_loss_epoch=1.300, train_auroc_epoch=0.759]\n","Validating:  85% 40/47 [00:01<00:00, 35.44it/s]\u001b[A\n","Epoch 16: 100% 110/110 [00:07<00:00, 14.74it/s, loss=1.23, v_num=0, val_loss=1.310, val_auroc_epoch=0.760, train_loss_step=1.200, train_loss_epoch=1.290, train_auroc_epoch=0.763]\n","Epoch 17:  73% 80/110 [00:05<00:01, 15.11it/s, loss=1.25, v_num=0, val_loss=1.310, val_auroc_epoch=0.760, train_loss_step=1.050, train_loss_epoch=1.290, train_auroc_epoch=0.763]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 17:  91% 100/110 [00:06<00:00, 16.47it/s, loss=1.25, v_num=0, val_loss=1.310, val_auroc_epoch=0.760, train_loss_step=1.050, train_loss_epoch=1.290, train_auroc_epoch=0.763]\n","Validating:  85% 40/47 [00:01<00:00, 28.52it/s]\u001b[A\n","Epoch 17: 100% 110/110 [00:07<00:00, 15.60it/s, loss=1.2, v_num=0, val_loss=1.320, val_auroc_epoch=0.758, train_loss_step=1.050, train_loss_epoch=1.270, train_auroc_epoch=0.769] \n","Epoch 18:  73% 80/110 [00:06<00:02, 13.33it/s, loss=1.23, v_num=0, val_loss=1.320, val_auroc_epoch=0.758, train_loss_step=1.070, train_loss_epoch=1.270, train_auroc_epoch=0.769]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 18:  91% 100/110 [00:06<00:00, 15.08it/s, loss=1.23, v_num=0, val_loss=1.320, val_auroc_epoch=0.758, train_loss_step=1.070, train_loss_epoch=1.270, train_auroc_epoch=0.769]\n","Validating:  85% 40/47 [00:01<00:00, 35.37it/s]\u001b[A\n","Epoch 18: 100% 110/110 [00:07<00:00, 14.89it/s, loss=1.19, v_num=0, val_loss=1.290, val_auroc_epoch=0.765, train_loss_step=0.984, train_loss_epoch=1.270, train_auroc_epoch=0.768]\n","Epoch 19:  73% 80/110 [00:05<00:01, 15.09it/s, loss=1.27, v_num=0, val_loss=1.290, val_auroc_epoch=0.765, train_loss_step=1.270, train_loss_epoch=1.270, train_auroc_epoch=0.768]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 19:  91% 100/110 [00:06<00:00, 16.44it/s, loss=1.27, v_num=0, val_loss=1.290, val_auroc_epoch=0.765, train_loss_step=1.270, train_loss_epoch=1.270, train_auroc_epoch=0.768]\n","Validating:  85% 40/47 [00:01<00:00, 28.24it/s]\u001b[A\n","Epoch 19: 100% 110/110 [00:07<00:00, 15.54it/s, loss=1.24, v_num=0, val_loss=1.280, val_auroc_epoch=0.768, train_loss_step=1.300, train_loss_epoch=1.270, train_auroc_epoch=0.771]\n","Epoch 20:  73% 80/110 [00:05<00:02, 13.36it/s, loss=1.25, v_num=0, val_loss=1.280, val_auroc_epoch=0.768, train_loss_step=1.260, train_loss_epoch=1.270, train_auroc_epoch=0.771]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 20:  91% 100/110 [00:06<00:00, 15.11it/s, loss=1.25, v_num=0, val_loss=1.280, val_auroc_epoch=0.768, train_loss_step=1.260, train_loss_epoch=1.270, train_auroc_epoch=0.771]\n","Validating:  85% 40/47 [00:01<00:00, 34.87it/s]\u001b[A\n","Epoch 20: 100% 110/110 [00:07<00:00, 14.87it/s, loss=1.23, v_num=0, val_loss=1.300, val_auroc_epoch=0.769, train_loss_step=1.170, train_loss_epoch=1.240, train_auroc_epoch=0.782]\n","Epoch 21:  73% 80/110 [00:05<00:02, 14.90it/s, loss=1.21, v_num=0, val_loss=1.300, val_auroc_epoch=0.769, train_loss_step=1.310, train_loss_epoch=1.240, train_auroc_epoch=0.782]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 21:  91% 100/110 [00:06<00:00, 16.16it/s, loss=1.21, v_num=0, val_loss=1.300, val_auroc_epoch=0.769, train_loss_step=1.310, train_loss_epoch=1.240, train_auroc_epoch=0.782]\n","Validating:  85% 40/47 [00:01<00:00, 27.11it/s]\u001b[A\n","Epoch 21: 100% 110/110 [00:07<00:00, 15.27it/s, loss=1.2, v_num=0, val_loss=1.280, val_auroc_epoch=0.770, train_loss_step=1.080, train_loss_epoch=1.250, train_auroc_epoch=0.777] \n","Epoch 22:  73% 80/110 [00:05<00:02, 13.50it/s, loss=1.2, v_num=0, val_loss=1.280, val_auroc_epoch=0.770, train_loss_step=1.060, train_loss_epoch=1.250, train_auroc_epoch=0.777]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 22:  91% 100/110 [00:06<00:00, 15.26it/s, loss=1.2, v_num=0, val_loss=1.280, val_auroc_epoch=0.770, train_loss_step=1.060, train_loss_epoch=1.250, train_auroc_epoch=0.777]\n","Validating:  85% 40/47 [00:01<00:00, 34.89it/s]\u001b[A\n","Epoch 22: 100% 110/110 [00:07<00:00, 15.00it/s, loss=1.2, v_num=0, val_loss=1.270, val_auroc_epoch=0.773, train_loss_step=0.959, train_loss_epoch=1.240, train_auroc_epoch=0.782]\n","Epoch 23:  73% 80/110 [00:05<00:02, 14.88it/s, loss=1.23, v_num=0, val_loss=1.270, val_auroc_epoch=0.773, train_loss_step=1.250, train_loss_epoch=1.240, train_auroc_epoch=0.782]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 23:  91% 100/110 [00:06<00:00, 16.23it/s, loss=1.23, v_num=0, val_loss=1.270, val_auroc_epoch=0.773, train_loss_step=1.250, train_loss_epoch=1.240, train_auroc_epoch=0.782]\n","Validating:  85% 40/47 [00:01<00:00, 28.15it/s]\u001b[A\n","Epoch 23: 100% 110/110 [00:07<00:00, 15.38it/s, loss=1.22, v_num=0, val_loss=1.270, val_auroc_epoch=0.776, train_loss_step=1.170, train_loss_epoch=1.230, train_auroc_epoch=0.787]\n","Epoch 24:  73% 80/110 [00:05<00:02, 13.50it/s, loss=1.22, v_num=0, val_loss=1.270, val_auroc_epoch=0.776, train_loss_step=1.370, train_loss_epoch=1.230, train_auroc_epoch=0.787]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 24:  91% 100/110 [00:06<00:00, 15.28it/s, loss=1.22, v_num=0, val_loss=1.270, val_auroc_epoch=0.776, train_loss_step=1.370, train_loss_epoch=1.230, train_auroc_epoch=0.787]\n","Validating:  85% 40/47 [00:01<00:00, 35.49it/s]\u001b[A\n","Epoch 24: 100% 110/110 [00:07<00:00, 15.05it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.775, train_loss_step=0.992, train_loss_epoch=1.230, train_auroc_epoch=0.785]\n","Epoch 25:  73% 80/110 [00:05<00:01, 15.02it/s, loss=1.22, v_num=0, val_loss=1.280, val_auroc_epoch=0.775, train_loss_step=1.280, train_loss_epoch=1.230, train_auroc_epoch=0.785]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 25:  91% 100/110 [00:06<00:00, 16.36it/s, loss=1.22, v_num=0, val_loss=1.280, val_auroc_epoch=0.775, train_loss_step=1.280, train_loss_epoch=1.230, train_auroc_epoch=0.785]\n","Validating:  85% 40/47 [00:01<00:00, 28.13it/s]\u001b[A\n","Epoch 25: 100% 110/110 [00:07<00:00, 15.50it/s, loss=1.2, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.210, train_loss_epoch=1.240, train_auroc_epoch=0.780] \n","Epoch 26:  73% 80/110 [00:05<00:02, 13.71it/s, loss=1.2, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.270, train_loss_epoch=1.240, train_auroc_epoch=0.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 26:  91% 100/110 [00:06<00:00, 15.41it/s, loss=1.2, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.270, train_loss_epoch=1.240, train_auroc_epoch=0.780]\n","Validating:  85% 40/47 [00:01<00:00, 33.50it/s]\u001b[A\n","Epoch 26: 100% 110/110 [00:07<00:00, 15.06it/s, loss=1.17, v_num=0, val_loss=1.260, val_auroc_epoch=0.779, train_loss_step=1.010, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Epoch 27:  73% 80/110 [00:05<00:02, 14.49it/s, loss=1.23, v_num=0, val_loss=1.260, val_auroc_epoch=0.779, train_loss_step=1.440, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 27:  91% 100/110 [00:06<00:00, 15.81it/s, loss=1.23, v_num=0, val_loss=1.260, val_auroc_epoch=0.779, train_loss_step=1.440, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Validating:  85% 40/47 [00:01<00:00, 27.73it/s]\u001b[A\n","Epoch 27: 100% 110/110 [00:07<00:00, 15.02it/s, loss=1.2, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.020, train_loss_epoch=1.200, train_auroc_epoch=0.797] \n","Epoch 28:  73% 80/110 [00:05<00:02, 13.72it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.030, train_loss_epoch=1.200, train_auroc_epoch=0.797]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 28:  91% 100/110 [00:06<00:00, 15.51it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.030, train_loss_epoch=1.200, train_auroc_epoch=0.797]\n","Validating:  85% 40/47 [00:01<00:00, 35.07it/s]\u001b[A\n","Epoch 28: 100% 110/110 [00:07<00:00, 15.21it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.200, train_loss_epoch=1.210, train_auroc_epoch=0.796]\n","Epoch 29:  73% 80/110 [00:05<00:02, 14.50it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.170, train_loss_epoch=1.210, train_auroc_epoch=0.796]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 29:  91% 100/110 [00:06<00:00, 15.86it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.170, train_loss_epoch=1.210, train_auroc_epoch=0.796]\n","Validating:  85% 40/47 [00:01<00:00, 28.18it/s]\u001b[A\n","Epoch 29: 100% 110/110 [00:07<00:00, 15.10it/s, loss=1.13, v_num=0, val_loss=1.260, val_auroc_epoch=0.781, train_loss_step=1.060, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Epoch 30:  73% 80/110 [00:05<00:02, 13.91it/s, loss=1.17, v_num=0, val_loss=1.260, val_auroc_epoch=0.781, train_loss_step=1.180, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 30:  91% 100/110 [00:06<00:00, 15.71it/s, loss=1.17, v_num=0, val_loss=1.260, val_auroc_epoch=0.781, train_loss_step=1.180, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Validating:  85% 40/47 [00:01<00:00, 35.88it/s]\u001b[A\n","Epoch 30: 100% 110/110 [00:07<00:00, 15.40it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Epoch 31:  73% 80/110 [00:05<00:02, 14.39it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 31:  91% 100/110 [00:06<00:00, 15.69it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Validating:  85% 40/47 [00:01<00:00, 27.21it/s]\u001b[A\n","Epoch 31: 100% 110/110 [00:07<00:00, 14.89it/s, loss=1.14, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=0.996, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Epoch 32:  73% 80/110 [00:05<00:02, 13.85it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=1.090, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 32:  91% 100/110 [00:06<00:00, 15.63it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=1.090, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Validating:  85% 40/47 [00:01<00:00, 35.59it/s]\u001b[A\n","Epoch 32: 100% 110/110 [00:07<00:00, 15.36it/s, loss=1.13, v_num=0, val_loss=1.260, val_auroc_epoch=0.780, train_loss_step=1.260, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Epoch 33:  73% 80/110 [00:05<00:02, 14.57it/s, loss=1.16, v_num=0, val_loss=1.260, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 33:  91% 100/110 [00:06<00:00, 15.98it/s, loss=1.16, v_num=0, val_loss=1.260, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.200, train_auroc_epoch=0.798]\n","Validating:  85% 40/47 [00:01<00:00, 28.54it/s]\u001b[A\n","Epoch 33: 100% 110/110 [00:07<00:00, 15.22it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.782, train_loss_step=0.846, train_loss_epoch=1.190, train_auroc_epoch=0.802]\n","Epoch 34:  73% 80/110 [00:05<00:02, 14.16it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.782, train_loss_step=1.070, train_loss_epoch=1.190, train_auroc_epoch=0.802]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 34:  91% 100/110 [00:06<00:00, 15.96it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.782, train_loss_step=1.070, train_loss_epoch=1.190, train_auroc_epoch=0.802]\n","Validating:  85% 40/47 [00:01<00:00, 35.95it/s]\u001b[A\n","Epoch 34: 100% 110/110 [00:07<00:00, 15.67it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.781, train_loss_step=1.070, train_loss_epoch=1.180, train_auroc_epoch=0.807]\n","Epoch 35:  73% 80/110 [00:05<00:02, 14.10it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.781, train_loss_step=1.110, train_loss_epoch=1.180, train_auroc_epoch=0.807]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 35:  91% 100/110 [00:06<00:00, 15.51it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.781, train_loss_step=1.110, train_loss_epoch=1.180, train_auroc_epoch=0.807]\n","Validating:  85% 40/47 [00:01<00:00, 28.45it/s]\u001b[A\n","Epoch 35: 100% 110/110 [00:07<00:00, 14.82it/s, loss=1.15, v_num=0, val_loss=1.260, val_auroc_epoch=0.783, train_loss_step=0.970, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Epoch 36:  73% 80/110 [00:05<00:02, 13.90it/s, loss=1.15, v_num=0, val_loss=1.260, val_auroc_epoch=0.783, train_loss_step=1.110, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 36:  91% 100/110 [00:06<00:00, 15.64it/s, loss=1.15, v_num=0, val_loss=1.260, val_auroc_epoch=0.783, train_loss_step=1.110, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Validating:  85% 40/47 [00:01<00:00, 34.72it/s]\u001b[A\n","Epoch 36: 100% 110/110 [00:07<00:00, 15.31it/s, loss=1.15, v_num=0, val_loss=1.260, val_auroc_epoch=0.785, train_loss_step=0.927, train_loss_epoch=1.180, train_auroc_epoch=0.807]\n","Epoch 37:  73% 80/110 [00:05<00:02, 13.93it/s, loss=1.13, v_num=0, val_loss=1.260, val_auroc_epoch=0.785, train_loss_step=1.130, train_loss_epoch=1.180, train_auroc_epoch=0.807]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 37:  91% 100/110 [00:06<00:00, 15.27it/s, loss=1.13, v_num=0, val_loss=1.260, val_auroc_epoch=0.785, train_loss_step=1.130, train_loss_epoch=1.180, train_auroc_epoch=0.807]\n","Validating:  85% 40/47 [00:01<00:00, 25.21it/s]\u001b[A\n","Epoch 37: 100% 110/110 [00:07<00:00, 14.27it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.782, train_loss_step=0.977, train_loss_epoch=1.170, train_auroc_epoch=0.810]\n","Epoch 38:  73% 80/110 [00:05<00:02, 14.58it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.782, train_loss_step=1.210, train_loss_epoch=1.170, train_auroc_epoch=0.810]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 38:  91% 100/110 [00:06<00:00, 16.33it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.782, train_loss_step=1.210, train_loss_epoch=1.170, train_auroc_epoch=0.810]\n","Validating:  85% 40/47 [00:01<00:00, 34.69it/s]\u001b[A\n","Epoch 38: 100% 110/110 [00:06<00:00, 15.90it/s, loss=1.12, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=0.836, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Epoch 39:  73% 80/110 [00:06<00:02, 12.01it/s, loss=1.15, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=1.140, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 39:  91% 100/110 [00:07<00:00, 12.74it/s, loss=1.15, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=1.140, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Validating:  85% 40/47 [00:02<00:00, 20.37it/s]\u001b[A\n","Epoch 39: 100% 110/110 [00:09<00:00, 12.09it/s, loss=1.15, v_num=0, val_loss=1.280, val_auroc_epoch=0.781, train_loss_step=1.280, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Epoch 40:  73% 80/110 [00:05<00:02, 14.48it/s, loss=1.14, v_num=0, val_loss=1.280, val_auroc_epoch=0.781, train_loss_step=1.090, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 40:  91% 100/110 [00:06<00:00, 16.28it/s, loss=1.14, v_num=0, val_loss=1.280, val_auroc_epoch=0.781, train_loss_step=1.090, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Validating:  85% 40/47 [00:01<00:00, 35.20it/s]\u001b[A\n","Epoch 40: 100% 110/110 [00:06<00:00, 15.90it/s, loss=1.13, v_num=0, val_loss=1.270, val_auroc_epoch=0.782, train_loss_step=0.951, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Epoch 41:  73% 80/110 [00:05<00:02, 13.77it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.782, train_loss_step=1.200, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 41:  91% 100/110 [00:06<00:00, 14.88it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.782, train_loss_step=1.200, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Validating:  85% 40/47 [00:01<00:00, 24.07it/s]\u001b[A\n","Epoch 41: 100% 110/110 [00:07<00:00, 14.19it/s, loss=1.12, v_num=0, val_loss=1.270, val_auroc_epoch=0.782, train_loss_step=1.080, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Epoch 42:  73% 80/110 [00:05<00:01, 15.01it/s, loss=1.17, v_num=0, val_loss=1.270, val_auroc_epoch=0.782, train_loss_step=1.230, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 42:  91% 100/110 [00:05<00:00, 16.76it/s, loss=1.17, v_num=0, val_loss=1.270, val_auroc_epoch=0.782, train_loss_step=1.230, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Validating:  85% 40/47 [00:01<00:00, 34.72it/s]\u001b[A\n","Epoch 42: 100% 110/110 [00:06<00:00, 16.30it/s, loss=1.15, v_num=0, val_loss=1.250, val_auroc_epoch=0.785, train_loss_step=1.020, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Epoch 43:  73% 80/110 [00:06<00:02, 12.59it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.785, train_loss_step=1.130, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 43:  91% 100/110 [00:07<00:00, 14.11it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.785, train_loss_step=1.130, train_loss_epoch=1.180, train_auroc_epoch=0.806]\n","Validating:  85% 40/47 [00:01<00:00, 32.94it/s]\u001b[A\n","Epoch 43: 100% 110/110 [00:07<00:00, 14.01it/s, loss=1.1, v_num=0, val_loss=1.250, val_auroc_epoch=0.785, train_loss_step=0.828, train_loss_epoch=1.170, train_auroc_epoch=0.811] \n","Epoch 44:  73% 80/110 [00:05<00:01, 15.09it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.785, train_loss_step=1.110, train_loss_epoch=1.170, train_auroc_epoch=0.811]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 44:  91% 100/110 [00:05<00:00, 16.86it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.785, train_loss_step=1.110, train_loss_epoch=1.170, train_auroc_epoch=0.811]\n","Validating:  85% 40/47 [00:01<00:00, 35.06it/s]\u001b[A\n","Epoch 44: 100% 110/110 [00:06<00:00, 16.40it/s, loss=1.13, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=0.978, train_loss_epoch=1.150, train_auroc_epoch=0.816]\n","Epoch 45:  73% 80/110 [00:06<00:02, 12.68it/s, loss=1.15, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.816]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 45:  91% 100/110 [00:06<00:00, 14.40it/s, loss=1.15, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.816]\n","Validating:  85% 40/47 [00:01<00:00, 34.56it/s]\u001b[A\n","Epoch 45: 100% 110/110 [00:07<00:00, 14.24it/s, loss=1.13, v_num=0, val_loss=1.250, val_auroc_epoch=0.784, train_loss_step=1.090, train_loss_epoch=1.160, train_auroc_epoch=0.813]\n","Epoch 46:  73% 80/110 [00:05<00:02, 14.13it/s, loss=1.13, v_num=0, val_loss=1.250, val_auroc_epoch=0.784, train_loss_step=1.210, train_loss_epoch=1.160, train_auroc_epoch=0.813]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 46:  91% 100/110 [00:06<00:00, 15.89it/s, loss=1.13, v_num=0, val_loss=1.250, val_auroc_epoch=0.784, train_loss_step=1.210, train_loss_epoch=1.160, train_auroc_epoch=0.813]\n","Validating:  85% 40/47 [00:01<00:00, 34.75it/s]\u001b[A\n","Epoch 46: 100% 110/110 [00:07<00:00, 15.55it/s, loss=1.1, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=1.070, train_loss_epoch=1.150, train_auroc_epoch=0.816] \n","Epoch 47:  73% 80/110 [00:06<00:02, 12.28it/s, loss=1.14, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=1.130, train_loss_epoch=1.150, train_auroc_epoch=0.816]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 47:  91% 100/110 [00:07<00:00, 13.97it/s, loss=1.14, v_num=0, val_loss=1.260, val_auroc_epoch=0.784, train_loss_step=1.130, train_loss_epoch=1.150, train_auroc_epoch=0.816]\n","Validating:  85% 40/47 [00:01<00:00, 33.99it/s]\u001b[A\n","Epoch 47: 100% 110/110 [00:07<00:00, 13.84it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.786, train_loss_step=0.975, train_loss_epoch=1.160, train_auroc_epoch=0.810]\n","Epoch 48:  73% 80/110 [00:05<00:02, 14.87it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.786, train_loss_step=1.040, train_loss_epoch=1.160, train_auroc_epoch=0.810]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 48:  91% 100/110 [00:06<00:00, 16.64it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.786, train_loss_step=1.040, train_loss_epoch=1.160, train_auroc_epoch=0.810]\n","Validating:  85% 40/47 [00:01<00:00, 35.16it/s]\u001b[A\n","Epoch 48: 100% 110/110 [00:06<00:00, 16.21it/s, loss=1.13, v_num=0, val_loss=1.260, val_auroc_epoch=0.786, train_loss_step=1.130, train_loss_epoch=1.140, train_auroc_epoch=0.820]\n","Epoch 49:  73% 80/110 [00:06<00:02, 12.47it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.786, train_loss_step=1.080, train_loss_epoch=1.140, train_auroc_epoch=0.820]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 49:  91% 100/110 [00:07<00:00, 14.17it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.786, train_loss_step=1.080, train_loss_epoch=1.140, train_auroc_epoch=0.820]\n","Validating:  85% 40/47 [00:01<00:00, 34.40it/s]\u001b[A\n","Epoch 49: 100% 110/110 [00:07<00:00, 14.03it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.786, train_loss_step=0.817, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Epoch 50:  73% 80/110 [00:05<00:02, 14.78it/s, loss=1.16, v_num=0, val_loss=1.250, val_auroc_epoch=0.786, train_loss_step=0.973, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 50:  91% 100/110 [00:06<00:00, 16.48it/s, loss=1.16, v_num=0, val_loss=1.250, val_auroc_epoch=0.786, train_loss_step=0.973, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating:  85% 40/47 [00:01<00:00, 30.80it/s]\u001b[A\n","Epoch 50: 100% 110/110 [00:07<00:00, 15.65it/s, loss=1.16, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=1.180, train_loss_epoch=1.150, train_auroc_epoch=0.816]\n","Epoch 51:  73% 80/110 [00:06<00:02, 12.03it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=0.931, train_loss_epoch=1.150, train_auroc_epoch=0.816]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 51:  91% 100/110 [00:07<00:00, 13.61it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=0.931, train_loss_epoch=1.150, train_auroc_epoch=0.816]\n","Validating:  85% 40/47 [00:01<00:00, 31.24it/s]\u001b[A\n","Epoch 51: 100% 110/110 [00:08<00:00, 13.38it/s, loss=1.1, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=0.961, train_loss_epoch=1.150, train_auroc_epoch=0.818] \n","Epoch 52:  73% 80/110 [00:05<00:02, 13.48it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=1.170, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 52:  91% 100/110 [00:06<00:00, 14.83it/s, loss=1.14, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=1.170, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating:  85% 40/47 [00:01<00:00, 26.73it/s]\u001b[A\n","Epoch 52: 100% 110/110 [00:07<00:00, 14.07it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.786, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Epoch 53:  73% 80/110 [00:05<00:02, 14.47it/s, loss=1.12, v_num=0, val_loss=1.270, val_auroc_epoch=0.786, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 53:  91% 100/110 [00:06<00:00, 16.23it/s, loss=1.12, v_num=0, val_loss=1.270, val_auroc_epoch=0.786, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating:  85% 40/47 [00:01<00:00, 34.93it/s]\u001b[A\n","Epoch 53: 100% 110/110 [00:06<00:00, 15.83it/s, loss=1.1, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=0.964, train_loss_epoch=1.140, train_auroc_epoch=0.819] \n","Epoch 54:  73% 80/110 [00:05<00:02, 13.45it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=1.050, train_loss_epoch=1.140, train_auroc_epoch=0.819]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 54:  91% 100/110 [00:06<00:00, 14.64it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=1.050, train_loss_epoch=1.140, train_auroc_epoch=0.819]\n","Validating:  85% 40/47 [00:01<00:00, 24.17it/s]\u001b[A\n","Validating: 100% 47/47 [00:01<00:00, 26.56it/s]\u001b[AEpoch 00055: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 54: 100% 110/110 [00:07<00:00, 13.98it/s, loss=1.09, v_num=0, val_loss=1.270, val_auroc_epoch=0.785, train_loss_step=0.810, train_loss_epoch=1.120, train_auroc_epoch=0.826]\n","Epoch 55:  73% 80/110 [00:05<00:01, 15.01it/s, loss=1.17, v_num=0, val_loss=1.270, val_auroc_epoch=0.785, train_loss_step=1.030, train_loss_epoch=1.120, train_auroc_epoch=0.826]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 55:  91% 100/110 [00:05<00:00, 16.79it/s, loss=1.17, v_num=0, val_loss=1.270, val_auroc_epoch=0.785, train_loss_step=1.030, train_loss_epoch=1.120, train_auroc_epoch=0.826]\n","Validating:  85% 40/47 [00:01<00:00, 33.95it/s]\u001b[A\n","Epoch 55: 100% 110/110 [00:06<00:00, 16.25it/s, loss=1.17, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=0.894, train_loss_epoch=1.130, train_auroc_epoch=0.822]\n","Epoch 56:  73% 80/110 [00:06<00:02, 12.81it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=1.040, train_loss_epoch=1.130, train_auroc_epoch=0.822]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 56:  91% 100/110 [00:07<00:00, 14.12it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=1.040, train_loss_epoch=1.130, train_auroc_epoch=0.822]\n","Validating:  85% 40/47 [00:01<00:00, 30.51it/s]\u001b[A\n","Epoch 56: 100% 110/110 [00:07<00:00, 13.99it/s, loss=1.1, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.180, train_loss_epoch=1.140, train_auroc_epoch=0.822] \n","Epoch 57:  73% 80/110 [00:05<00:01, 15.18it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.140, train_loss_epoch=1.140, train_auroc_epoch=0.822]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 57:  91% 100/110 [00:05<00:00, 16.98it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.140, train_loss_epoch=1.140, train_auroc_epoch=0.822]\n","Validating:  85% 40/47 [00:01<00:00, 35.34it/s]\u001b[A\n","Epoch 57: 100% 110/110 [00:06<00:00, 16.51it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.260, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Epoch 58:  73% 80/110 [00:06<00:02, 12.67it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.060, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 58:  91% 100/110 [00:06<00:00, 14.38it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.060, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating:  85% 40/47 [00:01<00:00, 34.66it/s]\u001b[A\n","Epoch 58: 100% 110/110 [00:07<00:00, 14.20it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.912, train_loss_epoch=1.130, train_auroc_epoch=0.824]\n","Epoch 59:  73% 80/110 [00:05<00:01, 15.18it/s, loss=1.1, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.170, train_loss_epoch=1.130, train_auroc_epoch=0.824]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 59:  91% 100/110 [00:05<00:00, 16.94it/s, loss=1.1, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.170, train_loss_epoch=1.130, train_auroc_epoch=0.824]\n","Validating:  85% 40/47 [00:01<00:00, 34.30it/s]\u001b[A\n","Epoch 59: 100% 110/110 [00:06<00:00, 16.43it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.320, train_loss_epoch=1.120, train_auroc_epoch=0.827]\n","Epoch 60:  73% 80/110 [00:06<00:02, 12.53it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.190, train_loss_epoch=1.120, train_auroc_epoch=0.827]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 60:  91% 100/110 [00:07<00:00, 14.26it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.190, train_loss_epoch=1.120, train_auroc_epoch=0.827]\n","Validating:  85% 40/47 [00:01<00:00, 34.79it/s]\u001b[A\n","Epoch 60: 100% 110/110 [00:07<00:00, 14.12it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.170, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Epoch 61:  73% 80/110 [00:05<00:01, 15.12it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.280, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 61:  91% 100/110 [00:05<00:00, 16.86it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.280, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating:  85% 40/47 [00:01<00:00, 34.77it/s]\u001b[A\n","Epoch 61: 100% 110/110 [00:06<00:00, 16.41it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.869, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Epoch 62:  73% 80/110 [00:06<00:02, 12.50it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 62:  91% 100/110 [00:07<00:00, 14.25it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating:  85% 40/47 [00:01<00:00, 35.41it/s]\u001b[A\n","Epoch 62: 100% 110/110 [00:07<00:00, 14.11it/s, loss=1.1, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.030, train_loss_epoch=1.120, train_auroc_epoch=0.828] \n","Epoch 63:  73% 80/110 [00:05<00:01, 15.01it/s, loss=1.12, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.120, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 63:  91% 100/110 [00:05<00:00, 16.74it/s, loss=1.12, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.120, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating:  85% 40/47 [00:01<00:00, 34.81it/s]\u001b[A\n","Epoch 63: 100% 110/110 [00:06<00:00, 16.27it/s, loss=1.07, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=0.786, train_loss_epoch=1.120, train_auroc_epoch=0.830]\n","Epoch 64:  73% 80/110 [00:06<00:02, 12.59it/s, loss=1.07, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=1.120, train_loss_epoch=1.120, train_auroc_epoch=0.830]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 64:  91% 100/110 [00:06<00:00, 14.32it/s, loss=1.07, v_num=0, val_loss=1.250, val_auroc_epoch=0.787, train_loss_step=1.120, train_loss_epoch=1.120, train_auroc_epoch=0.830]\n","Validating:  85% 40/47 [00:01<00:00, 34.12it/s]\u001b[A\n","Epoch 64: 100% 110/110 [00:07<00:00, 14.14it/s, loss=1.07, v_num=0, val_loss=1.240, val_auroc_epoch=0.789, train_loss_step=1.190, train_loss_epoch=1.120, train_auroc_epoch=0.825]\n","Epoch 65:  73% 80/110 [00:05<00:01, 15.06it/s, loss=1.07, v_num=0, val_loss=1.240, val_auroc_epoch=0.789, train_loss_step=1.010, train_loss_epoch=1.120, train_auroc_epoch=0.825]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 65:  91% 100/110 [00:05<00:00, 16.84it/s, loss=1.07, v_num=0, val_loss=1.240, val_auroc_epoch=0.789, train_loss_step=1.010, train_loss_epoch=1.120, train_auroc_epoch=0.825]\n","Validating:  85% 40/47 [00:01<00:00, 32.33it/s]\u001b[A\n","Epoch 65: 100% 110/110 [00:06<00:00, 16.04it/s, loss=1.07, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.140, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Epoch 66:  73% 80/110 [00:06<00:02, 12.57it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=0.992, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 66:  91% 100/110 [00:06<00:00, 14.32it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=0.992, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating:  85% 40/47 [00:01<00:00, 35.54it/s]\u001b[A\n","Epoch 66: 100% 110/110 [00:07<00:00, 14.10it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.915, train_loss_epoch=1.100, train_auroc_epoch=0.833]\n","Epoch 67:  73% 80/110 [00:06<00:02, 12.06it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.100, train_auroc_epoch=0.833]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 67:  91% 100/110 [00:07<00:00, 13.48it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.100, train_auroc_epoch=0.833]\n","Validating:  85% 40/47 [00:01<00:00, 25.87it/s]\u001b[A\n","Epoch 67: 100% 110/110 [00:08<00:00, 12.86it/s, loss=1.08, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.867, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Epoch 68:  73% 80/110 [00:05<00:02, 14.69it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.835, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 68:  91% 100/110 [00:06<00:00, 16.45it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.835, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating:  85% 40/47 [00:01<00:00, 33.87it/s]\u001b[A\n","Epoch 68: 100% 110/110 [00:06<00:00, 15.97it/s, loss=1.1, v_num=0, val_loss=1.240, val_auroc_epoch=0.789, train_loss_step=0.990, train_loss_epoch=1.120, train_auroc_epoch=0.829] \n","Epoch 69:  73% 80/110 [00:05<00:02, 13.53it/s, loss=1.09, v_num=0, val_loss=1.240, val_auroc_epoch=0.789, train_loss_step=0.875, train_loss_epoch=1.120, train_auroc_epoch=0.829]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 69:  91% 100/110 [00:06<00:00, 14.60it/s, loss=1.09, v_num=0, val_loss=1.240, val_auroc_epoch=0.789, train_loss_step=0.875, train_loss_epoch=1.120, train_auroc_epoch=0.829]\n","Validating:  85% 40/47 [00:01<00:00, 25.50it/s]\u001b[A\n","Epoch 69: 100% 110/110 [00:07<00:00, 14.16it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.995, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Epoch 70:  73% 80/110 [00:05<00:01, 15.00it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 70:  91% 100/110 [00:05<00:00, 16.74it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Validating:  85% 40/47 [00:01<00:00, 34.35it/s]\u001b[A\n","Epoch 70: 100% 110/110 [00:06<00:00, 16.26it/s, loss=1.04, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.678, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 71:  73% 80/110 [00:06<00:02, 12.88it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.260, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 71:  91% 100/110 [00:06<00:00, 14.38it/s, loss=1.11, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.260, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 32.47it/s]\u001b[A\n","Epoch 71: 100% 110/110 [00:07<00:00, 14.23it/s, loss=1.13, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.972, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Epoch 72:  73% 80/110 [00:05<00:01, 15.09it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.200, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 72:  91% 100/110 [00:05<00:00, 16.87it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.200, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating:  85% 40/47 [00:01<00:00, 34.73it/s]\u001b[A\n","Epoch 72: 100% 110/110 [00:06<00:00, 16.38it/s, loss=1.06, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.762, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Epoch 73:  73% 80/110 [00:06<00:02, 12.66it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.940, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 73:  91% 100/110 [00:06<00:00, 14.40it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.940, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating:  85% 40/47 [00:01<00:00, 35.40it/s]\u001b[A\n","Epoch 73: 100% 110/110 [00:07<00:00, 14.25it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.875, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Epoch 74:  73% 80/110 [00:05<00:01, 15.11it/s, loss=1.05, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 74:  91% 100/110 [00:05<00:00, 16.88it/s, loss=1.05, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating:  85% 40/47 [00:01<00:00, 34.88it/s]\u001b[A\n","Epoch 74: 100% 110/110 [00:06<00:00, 16.39it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.070, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Epoch 75:  73% 80/110 [00:06<00:02, 12.57it/s, loss=1.08, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.838, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 75:  91% 100/110 [00:07<00:00, 14.27it/s, loss=1.08, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.838, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating:  85% 40/47 [00:01<00:00, 34.32it/s]\u001b[A\n","Epoch 75: 100% 110/110 [00:07<00:00, 14.10it/s, loss=1.05, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.020, train_loss_epoch=1.090, train_auroc_epoch=0.836]\n","Epoch 76:  73% 80/110 [00:05<00:01, 15.12it/s, loss=1.05, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.943, train_loss_epoch=1.090, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 76:  91% 100/110 [00:05<00:00, 16.87it/s, loss=1.05, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.943, train_loss_epoch=1.090, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 34.09it/s]\u001b[A\n","Epoch 76: 100% 110/110 [00:06<00:00, 16.34it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=0.912, train_loss_epoch=1.090, train_auroc_epoch=0.838]\n","Epoch 77:  73% 80/110 [00:06<00:02, 12.51it/s, loss=1.13, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=1.020, train_loss_epoch=1.090, train_auroc_epoch=0.838]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 77:  91% 100/110 [00:07<00:00, 14.25it/s, loss=1.13, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=1.020, train_loss_epoch=1.090, train_auroc_epoch=0.838]\n","Validating:  85% 40/47 [00:01<00:00, 34.92it/s]\u001b[A\n","Epoch 77: 100% 110/110 [00:07<00:00, 14.11it/s, loss=1.1, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=0.863, train_loss_epoch=1.100, train_auroc_epoch=0.835] \n","Epoch 78:  73% 80/110 [00:05<00:02, 14.90it/s, loss=1.09, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=0.992, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 78:  91% 100/110 [00:06<00:00, 16.60it/s, loss=1.09, v_num=0, val_loss=1.260, val_auroc_epoch=0.787, train_loss_step=0.992, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating:  85% 40/47 [00:01<00:00, 33.72it/s]\u001b[A\n","Epoch 78: 100% 110/110 [00:06<00:00, 15.88it/s, loss=1.09, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.190, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 79:  73% 80/110 [00:06<00:02, 12.90it/s, loss=1.08, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.120, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 79:  91% 100/110 [00:06<00:00, 14.65it/s, loss=1.08, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.120, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 35.07it/s]\u001b[A\n","Validating: 100% 47/47 [00:01<00:00, 35.78it/s]\u001b[AEpoch 00080: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 79: 100% 110/110 [00:07<00:00, 14.43it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.998, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Epoch 80:  73% 80/110 [00:05<00:01, 15.04it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.000, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 80:  91% 100/110 [00:06<00:00, 16.63it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.000, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Validating:  85% 40/47 [00:01<00:00, 29.24it/s]\u001b[A\n","Epoch 80: 100% 110/110 [00:07<00:00, 15.64it/s, loss=1.14, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.160, train_loss_epoch=1.090, train_auroc_epoch=0.835]\n","Epoch 81:  73% 80/110 [00:06<00:02, 13.28it/s, loss=1.09, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.030, train_loss_epoch=1.090, train_auroc_epoch=0.835]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 81:  91% 100/110 [00:06<00:00, 15.04it/s, loss=1.09, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.030, train_loss_epoch=1.090, train_auroc_epoch=0.835]\n","Validating:  85% 40/47 [00:01<00:00, 35.38it/s]\u001b[A\n","Epoch 81: 100% 110/110 [00:07<00:00, 14.84it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.010, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Epoch 82:  73% 80/110 [00:05<00:02, 14.93it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=0.795, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 82:  91% 100/110 [00:06<00:00, 16.27it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=0.795, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Validating:  85% 40/47 [00:01<00:00, 27.60it/s]\u001b[A\n","Epoch 82: 100% 110/110 [00:07<00:00, 15.35it/s, loss=1.05, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=0.976, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Epoch 83:  73% 80/110 [00:05<00:02, 13.75it/s, loss=1.06, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.100, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 83:  91% 100/110 [00:06<00:00, 15.51it/s, loss=1.06, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.100, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Validating:  85% 40/47 [00:01<00:00, 35.38it/s]\u001b[A\n","Epoch 83: 100% 110/110 [00:07<00:00, 15.26it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.785, train_loss_epoch=1.070, train_auroc_epoch=0.842]\n","Epoch 84:  73% 80/110 [00:05<00:02, 14.25it/s, loss=1.1, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.090, train_loss_epoch=1.070, train_auroc_epoch=0.842]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 84:  91% 100/110 [00:06<00:00, 15.55it/s, loss=1.1, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.090, train_loss_epoch=1.070, train_auroc_epoch=0.842]\n","Validating:  85% 40/47 [00:01<00:00, 26.94it/s]\u001b[A\n","Epoch 84: 100% 110/110 [00:07<00:00, 14.67it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.990, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Epoch 85:  73% 80/110 [00:05<00:02, 14.34it/s, loss=1.07, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.953, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 85:  91% 100/110 [00:06<00:00, 16.07it/s, loss=1.07, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.953, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Validating:  85% 40/47 [00:01<00:00, 34.73it/s]\u001b[A\n","Epoch 85: 100% 110/110 [00:06<00:00, 15.72it/s, loss=1.04, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.882, train_loss_epoch=1.070, train_auroc_epoch=0.843]\n","Epoch 86:  73% 80/110 [00:06<00:02, 12.76it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.100, train_loss_epoch=1.070, train_auroc_epoch=0.843]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 86:  91% 100/110 [00:07<00:00, 13.78it/s, loss=1.11, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.100, train_loss_epoch=1.070, train_auroc_epoch=0.843]\n","Validating:  85% 40/47 [00:01<00:00, 25.55it/s]\u001b[A\n","Epoch 86: 100% 110/110 [00:08<00:00, 13.49it/s, loss=1.12, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.250, train_loss_epoch=1.080, train_auroc_epoch=0.839]\n","Epoch 87:  73% 80/110 [00:05<00:01, 15.18it/s, loss=1.09, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.020, train_loss_epoch=1.080, train_auroc_epoch=0.839]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 87:  91% 100/110 [00:05<00:00, 16.95it/s, loss=1.09, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.020, train_loss_epoch=1.080, train_auroc_epoch=0.839]\n","Validating:  85% 40/47 [00:01<00:00, 35.18it/s]\u001b[A\n","Epoch 87: 100% 110/110 [00:06<00:00, 16.52it/s, loss=1.1, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.020, train_loss_epoch=1.090, train_auroc_epoch=0.837] \n","Epoch 88:  73% 80/110 [00:06<00:02, 12.99it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.873, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 88:  91% 100/110 [00:06<00:00, 14.50it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.873, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Validating:  85% 40/47 [00:01<00:00, 32.80it/s]\u001b[A\n","Epoch 88: 100% 110/110 [00:07<00:00, 14.36it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.970, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Epoch 89:  73% 80/110 [00:05<00:02, 14.77it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.110, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 89:  91% 100/110 [00:06<00:00, 16.54it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.110, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Validating:  85% 40/47 [00:01<00:00, 34.94it/s]\u001b[A\n","Epoch 89: 100% 110/110 [00:06<00:00, 16.12it/s, loss=1.07, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.000, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Epoch 90:  73% 80/110 [00:06<00:02, 12.82it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.020, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 90:  91% 100/110 [00:06<00:00, 14.57it/s, loss=1.08, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.020, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Validating:  85% 40/47 [00:01<00:00, 35.37it/s]\u001b[A\n","Validating: 100% 47/47 [00:01<00:00, 36.09it/s]\u001b[AEpoch 00091: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 90: 100% 110/110 [00:07<00:00, 14.40it/s, loss=1.09, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.893, train_loss_epoch=1.080, train_auroc_epoch=0.842]\n","Epoch 91:  73% 80/110 [00:05<00:01, 15.04it/s, loss=1.13, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.080, train_auroc_epoch=0.842]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 91:  91% 100/110 [00:05<00:00, 16.83it/s, loss=1.13, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.080, train_loss_epoch=1.080, train_auroc_epoch=0.842]\n","Validating:  85% 40/47 [00:01<00:00, 35.08it/s]\u001b[A\n","Epoch 91: 100% 110/110 [00:06<00:00, 16.35it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=1.230, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Epoch 92:  73% 80/110 [00:06<00:02, 12.69it/s, loss=1.06, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.932, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 92:  91% 100/110 [00:06<00:00, 14.40it/s, loss=1.06, v_num=0, val_loss=1.250, val_auroc_epoch=0.788, train_loss_step=0.932, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Validating:  85% 40/47 [00:01<00:00, 34.76it/s]\u001b[A\n","Epoch 92: 100% 110/110 [00:07<00:00, 14.21it/s, loss=1.07, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.160, train_loss_epoch=1.070, train_auroc_epoch=0.845]\n","Epoch 93:  73% 80/110 [00:05<00:02, 14.93it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.210, train_loss_epoch=1.070, train_auroc_epoch=0.845]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 93:  91% 100/110 [00:05<00:00, 16.68it/s, loss=1.12, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.210, train_loss_epoch=1.070, train_auroc_epoch=0.845]\n","Validating:  85% 40/47 [00:01<00:00, 34.73it/s]\u001b[A\n","Epoch 93: 100% 110/110 [00:06<00:00, 16.24it/s, loss=1.12, v_num=0, val_loss=1.260, val_auroc_epoch=0.789, train_loss_step=1.110, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Epoch 94:  73% 80/110 [00:06<00:02, 12.66it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.789, train_loss_step=1.090, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 94:  91% 100/110 [00:06<00:00, 14.36it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.789, train_loss_step=1.090, train_loss_epoch=1.080, train_auroc_epoch=0.840]\n","Validating:  85% 40/47 [00:01<00:00, 34.05it/s]\u001b[A\n","Epoch 94: 100% 110/110 [00:07<00:00, 14.00it/s, loss=1.03, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.040, train_loss_epoch=1.070, train_auroc_epoch=0.843]\n","Epoch 95:  73% 80/110 [00:06<00:02, 12.61it/s, loss=1.03, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.000, train_loss_epoch=1.070, train_auroc_epoch=0.843]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 95:  91% 100/110 [00:07<00:00, 14.06it/s, loss=1.03, v_num=0, val_loss=1.250, val_auroc_epoch=0.789, train_loss_step=1.000, train_loss_epoch=1.070, train_auroc_epoch=0.843]\n","Validating:  85% 40/47 [00:01<00:00, 28.22it/s]\u001b[A\n","Epoch 95: 100% 110/110 [00:08<00:00, 13.56it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.789, train_loss_step=0.982, train_loss_epoch=1.060, train_auroc_epoch=0.847]\n","Epoch 96:  73% 80/110 [00:05<00:02, 13.41it/s, loss=1.07, v_num=0, val_loss=1.260, val_auroc_epoch=0.789, train_loss_step=0.966, train_loss_epoch=1.060, train_auroc_epoch=0.847]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 96:  91% 100/110 [00:06<00:00, 15.15it/s, loss=1.07, v_num=0, val_loss=1.260, val_auroc_epoch=0.789, train_loss_step=0.966, train_loss_epoch=1.060, train_auroc_epoch=0.847]\n","Validating:  85% 40/47 [00:01<00:00, 34.17it/s]\u001b[A\n","Epoch 96: 100% 110/110 [00:07<00:00, 14.87it/s, loss=1.07, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.916, train_loss_epoch=1.060, train_auroc_epoch=0.846]\n","Epoch 97:  73% 80/110 [00:05<00:02, 14.21it/s, loss=1.07, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.070, train_loss_epoch=1.060, train_auroc_epoch=0.846]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 97:  91% 100/110 [00:06<00:00, 15.53it/s, loss=1.07, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.070, train_loss_epoch=1.060, train_auroc_epoch=0.846]\n","Validating:  85% 40/47 [00:01<00:00, 27.23it/s]\u001b[A\n","Epoch 97: 100% 110/110 [00:07<00:00, 14.75it/s, loss=1.09, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.170, train_loss_epoch=1.060, train_auroc_epoch=0.847]\n","Epoch 98:  73% 80/110 [00:05<00:02, 14.28it/s, loss=1.08, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.030, train_loss_epoch=1.060, train_auroc_epoch=0.847]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 98:  91% 100/110 [00:06<00:00, 16.06it/s, loss=1.08, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.030, train_loss_epoch=1.060, train_auroc_epoch=0.847]\n","Validating:  85% 40/47 [00:01<00:00, 34.83it/s]\u001b[A\n","Epoch 98: 100% 110/110 [00:07<00:00, 15.68it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=0.869, train_loss_epoch=1.060, train_auroc_epoch=0.846]\n","Epoch 99:  73% 80/110 [00:05<00:02, 14.04it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.260, train_loss_epoch=1.060, train_auroc_epoch=0.846]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 99:  91% 100/110 [00:06<00:00, 15.32it/s, loss=1.05, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.260, train_loss_epoch=1.060, train_auroc_epoch=0.846]\n","Validating:  85% 40/47 [00:01<00:00, 24.76it/s]\u001b[A\n","Epoch 99: 100% 110/110 [00:07<00:00, 14.25it/s, loss=1.06, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.260, train_loss_epoch=1.070, train_auroc_epoch=0.843]\n","Epoch 99: 100% 110/110 [00:07<00:00, 14.23it/s, loss=1.06, v_num=0, val_loss=1.260, val_auroc_epoch=0.788, train_loss_step=1.260, train_loss_epoch=1.070, train_auroc_epoch=0.843]\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 63/63 [00:01<00:00, 34.42it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test_auroc_best_epoch': 0.7790159583091736, 'test_loss': 1.2676830291748047}\n","--------------------------------------------------------------------------------\n","Path to best model found during training: \n","/content/outputs/2023-06-12/03-39-17/lightning_logs/version_0/checkpoints/68-4346.ckpt\n"]}]},{"cell_type":"markdown","source":["## Supervised learning only"],"metadata":{"id":"NyPOPTmDOePz"}},{"cell_type":"code","source":["# bare SUP\n","\n","!python /content/saint/main.py experiment=supervised \\\n","  experiment.model=saint \\\n","  data.data_folder=/content/saint/data \\\n","  data=bank_sup"],"metadata":{"id":"dEoRCf56GlGD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686543793846,"user_tz":-420,"elapsed":173050,"user":{"displayName":"Vincent Tionando","userId":"04311127348620831134"}},"outputId":"dab33158-b267-4918-e746-b1b081ace098"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-12 04:10:41.115013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-12 04:10:42.110239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/main.py:11: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","Global seed set to 1234\n","/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name         | Type             | Params\n","--------------------------------------------------\n","0 | transformer  | Encoder          | 65.0 K\n","1 | embedding    | Embedding        | 3.2 K \n","2 | fc           | Linear           | 165   \n","3 | criterion    | CrossEntropyLoss | 0     \n","4 | train_metric | AUROC            | 0     \n","--------------------------------------------------\n","68.3 K    Trainable params\n","0         Non-trainable params\n","68.3 K    Total params\n","0.273     Total estimated model params size (MB)\n","Global seed set to 1234\n","Epoch 0:  73% 80/110 [00:05<00:02, 13.62it/s, loss=1.52, v_num=0, val_loss=1.680, val_auroc_epoch=0.497, train_loss_step=1.390]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  91% 100/110 [00:06<00:00, 15.40it/s, loss=1.52, v_num=0, val_loss=1.680, val_auroc_epoch=0.497, train_loss_step=1.390]\n","Validating:  85% 40/47 [00:01<00:00, 35.80it/s]\u001b[A\n","Epoch 0: 100% 110/110 [00:07<00:00, 15.16it/s, loss=1.5, v_num=0, val_loss=1.570, val_auroc_epoch=0.575, train_loss_step=1.380, train_loss_epoch=1.590, train_auroc_epoch=0.552]\n","Epoch 1:  73% 80/110 [00:05<00:02, 14.47it/s, loss=1.48, v_num=0, val_loss=1.570, val_auroc_epoch=0.575, train_loss_step=1.330, train_loss_epoch=1.590, train_auroc_epoch=0.552]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 1:  91% 100/110 [00:06<00:00, 15.74it/s, loss=1.48, v_num=0, val_loss=1.570, val_auroc_epoch=0.575, train_loss_step=1.330, train_loss_epoch=1.590, train_auroc_epoch=0.552]\n","Validating:  85% 40/47 [00:01<00:00, 27.20it/s]\u001b[A\n","Epoch 1: 100% 110/110 [00:07<00:00, 14.89it/s, loss=1.47, v_num=0, val_loss=1.530, val_auroc_epoch=0.639, train_loss_step=1.420, train_loss_epoch=1.570, train_auroc_epoch=0.550]\n","Epoch 2:  73% 80/110 [00:05<00:02, 14.11it/s, loss=1.47, v_num=0, val_loss=1.530, val_auroc_epoch=0.639, train_loss_step=1.470, train_loss_epoch=1.570, train_auroc_epoch=0.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 2:  91% 100/110 [00:06<00:00, 15.88it/s, loss=1.47, v_num=0, val_loss=1.530, val_auroc_epoch=0.639, train_loss_step=1.470, train_loss_epoch=1.570, train_auroc_epoch=0.550]\n","Validating:  85% 40/47 [00:01<00:00, 34.84it/s]\u001b[A\n","Epoch 2: 100% 110/110 [00:07<00:00, 15.53it/s, loss=1.45, v_num=0, val_loss=1.480, val_auroc_epoch=0.684, train_loss_step=1.290, train_loss_epoch=1.530, train_auroc_epoch=0.598]\n","Epoch 3:  73% 80/110 [00:05<00:02, 13.80it/s, loss=1.44, v_num=0, val_loss=1.480, val_auroc_epoch=0.684, train_loss_step=1.300, train_loss_epoch=1.530, train_auroc_epoch=0.598]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 3:  91% 100/110 [00:06<00:00, 15.03it/s, loss=1.44, v_num=0, val_loss=1.480, val_auroc_epoch=0.684, train_loss_step=1.300, train_loss_epoch=1.530, train_auroc_epoch=0.598]\n","Validating:  85% 40/47 [00:01<00:00, 24.14it/s]\u001b[A\n","Epoch 3: 100% 110/110 [00:07<00:00, 14.21it/s, loss=1.41, v_num=0, val_loss=1.430, val_auroc_epoch=0.706, train_loss_step=1.290, train_loss_epoch=1.500, train_auroc_epoch=0.636]\n","Epoch 4:  73% 80/110 [00:05<00:02, 14.94it/s, loss=1.42, v_num=0, val_loss=1.430, val_auroc_epoch=0.706, train_loss_step=1.340, train_loss_epoch=1.500, train_auroc_epoch=0.636]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 4:  91% 100/110 [00:05<00:00, 16.72it/s, loss=1.42, v_num=0, val_loss=1.430, val_auroc_epoch=0.706, train_loss_step=1.340, train_loss_epoch=1.500, train_auroc_epoch=0.636]\n","Validating:  85% 40/47 [00:01<00:00, 35.21it/s]\u001b[A\n","Epoch 4: 100% 110/110 [00:06<00:00, 16.29it/s, loss=1.4, v_num=0, val_loss=1.400, val_auroc_epoch=0.720, train_loss_step=1.150, train_loss_epoch=1.460, train_auroc_epoch=0.671] \n","Epoch 5:  73% 80/110 [00:06<00:02, 13.01it/s, loss=1.36, v_num=0, val_loss=1.400, val_auroc_epoch=0.720, train_loss_step=1.280, train_loss_epoch=1.460, train_auroc_epoch=0.671]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 5:  91% 100/110 [00:06<00:00, 14.34it/s, loss=1.36, v_num=0, val_loss=1.400, val_auroc_epoch=0.720, train_loss_step=1.280, train_loss_epoch=1.460, train_auroc_epoch=0.671]\n","Validating:  85% 40/47 [00:01<00:00, 30.82it/s]\u001b[A\n","Epoch 5: 100% 110/110 [00:07<00:00, 14.18it/s, loss=1.34, v_num=0, val_loss=1.370, val_auroc_epoch=0.728, train_loss_step=1.220, train_loss_epoch=1.420, train_auroc_epoch=0.699]\n","Epoch 6:  73% 80/110 [00:06<00:02, 13.26it/s, loss=1.37, v_num=0, val_loss=1.370, val_auroc_epoch=0.728, train_loss_step=1.350, train_loss_epoch=1.420, train_auroc_epoch=0.699]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 6:  91% 100/110 [00:06<00:00, 14.40it/s, loss=1.37, v_num=0, val_loss=1.370, val_auroc_epoch=0.728, train_loss_step=1.350, train_loss_epoch=1.420, train_auroc_epoch=0.699]\n","Validating:  85% 40/47 [00:01<00:00, 28.27it/s]\u001b[A\n","Epoch 6: 100% 110/110 [00:07<00:00, 14.17it/s, loss=1.34, v_num=0, val_loss=1.360, val_auroc_epoch=0.735, train_loss_step=1.190, train_loss_epoch=1.400, train_auroc_epoch=0.707]\n","Epoch 7:  73% 80/110 [00:06<00:02, 12.59it/s, loss=1.36, v_num=0, val_loss=1.360, val_auroc_epoch=0.735, train_loss_step=1.380, train_loss_epoch=1.400, train_auroc_epoch=0.707]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 7:  91% 100/110 [00:06<00:00, 14.30it/s, loss=1.36, v_num=0, val_loss=1.360, val_auroc_epoch=0.735, train_loss_step=1.380, train_loss_epoch=1.400, train_auroc_epoch=0.707]\n","Validating:  85% 40/47 [00:01<00:00, 35.18it/s]\u001b[A\n","Epoch 7: 100% 110/110 [00:07<00:00, 14.19it/s, loss=1.33, v_num=0, val_loss=1.350, val_auroc_epoch=0.741, train_loss_step=1.170, train_loss_epoch=1.390, train_auroc_epoch=0.711]\n","Epoch 8:  73% 80/110 [00:05<00:02, 14.94it/s, loss=1.32, v_num=0, val_loss=1.350, val_auroc_epoch=0.741, train_loss_step=1.140, train_loss_epoch=1.390, train_auroc_epoch=0.711]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 8:  91% 100/110 [00:05<00:00, 16.72it/s, loss=1.32, v_num=0, val_loss=1.350, val_auroc_epoch=0.741, train_loss_step=1.140, train_loss_epoch=1.390, train_auroc_epoch=0.711]\n","Validating:  85% 40/47 [00:01<00:00, 35.46it/s]\u001b[A\n","Epoch 8: 100% 110/110 [00:06<00:00, 16.30it/s, loss=1.3, v_num=0, val_loss=1.350, val_auroc_epoch=0.743, train_loss_step=1.170, train_loss_epoch=1.370, train_auroc_epoch=0.722] \n","Epoch 9:  73% 80/110 [00:06<00:02, 12.63it/s, loss=1.3, v_num=0, val_loss=1.350, val_auroc_epoch=0.743, train_loss_step=1.120, train_loss_epoch=1.370, train_auroc_epoch=0.722]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 9:  91% 100/110 [00:06<00:00, 14.37it/s, loss=1.3, v_num=0, val_loss=1.350, val_auroc_epoch=0.743, train_loss_step=1.120, train_loss_epoch=1.370, train_auroc_epoch=0.722]\n","Validating:  85% 40/47 [00:01<00:00, 34.83it/s]\u001b[A\n","Epoch 9: 100% 110/110 [00:07<00:00, 14.19it/s, loss=1.3, v_num=0, val_loss=1.330, val_auroc_epoch=0.748, train_loss_step=1.110, train_loss_epoch=1.350, train_auroc_epoch=0.736]\n","Epoch 10:  73% 80/110 [00:05<00:02, 14.92it/s, loss=1.32, v_num=0, val_loss=1.330, val_auroc_epoch=0.748, train_loss_step=1.260, train_loss_epoch=1.350, train_auroc_epoch=0.736]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 10:  91% 100/110 [00:06<00:00, 16.66it/s, loss=1.32, v_num=0, val_loss=1.330, val_auroc_epoch=0.748, train_loss_step=1.260, train_loss_epoch=1.350, train_auroc_epoch=0.736]\n","Validating:  85% 40/47 [00:01<00:00, 33.70it/s]\u001b[A\n","Epoch 10: 100% 110/110 [00:06<00:00, 15.90it/s, loss=1.29, v_num=0, val_loss=1.330, val_auroc_epoch=0.749, train_loss_step=1.260, train_loss_epoch=1.350, train_auroc_epoch=0.737]\n","Epoch 11:  73% 80/110 [00:06<00:02, 12.89it/s, loss=1.32, v_num=0, val_loss=1.330, val_auroc_epoch=0.749, train_loss_step=1.190, train_loss_epoch=1.350, train_auroc_epoch=0.737]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 11:  91% 100/110 [00:06<00:00, 14.64it/s, loss=1.32, v_num=0, val_loss=1.330, val_auroc_epoch=0.749, train_loss_step=1.190, train_loss_epoch=1.350, train_auroc_epoch=0.737]\n","Validating:  85% 40/47 [00:01<00:00, 34.88it/s]\u001b[A\n","Epoch 11: 100% 110/110 [00:07<00:00, 14.42it/s, loss=1.3, v_num=0, val_loss=1.320, val_auroc_epoch=0.753, train_loss_step=1.090, train_loss_epoch=1.340, train_auroc_epoch=0.737] \n","Epoch 12:  73% 80/110 [00:05<00:01, 15.04it/s, loss=1.34, v_num=0, val_loss=1.320, val_auroc_epoch=0.753, train_loss_step=1.250, train_loss_epoch=1.340, train_auroc_epoch=0.737]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 12:  91% 100/110 [00:06<00:00, 16.46it/s, loss=1.34, v_num=0, val_loss=1.320, val_auroc_epoch=0.753, train_loss_step=1.250, train_loss_epoch=1.340, train_auroc_epoch=0.737]\n","Validating:  85% 40/47 [00:01<00:00, 29.01it/s]\u001b[A\n","Epoch 12: 100% 110/110 [00:07<00:00, 15.62it/s, loss=1.3, v_num=0, val_loss=1.310, val_auroc_epoch=0.757, train_loss_step=1.040, train_loss_epoch=1.330, train_auroc_epoch=0.747] \n","Epoch 13:  73% 80/110 [00:06<00:02, 12.91it/s, loss=1.28, v_num=0, val_loss=1.310, val_auroc_epoch=0.757, train_loss_step=1.170, train_loss_epoch=1.330, train_auroc_epoch=0.747]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 13:  91% 100/110 [00:06<00:00, 14.62it/s, loss=1.28, v_num=0, val_loss=1.310, val_auroc_epoch=0.757, train_loss_step=1.170, train_loss_epoch=1.330, train_auroc_epoch=0.747]\n","Validating:  85% 40/47 [00:01<00:00, 34.57it/s]\u001b[A\n","Epoch 13: 100% 110/110 [00:07<00:00, 14.43it/s, loss=1.28, v_num=0, val_loss=1.320, val_auroc_epoch=0.756, train_loss_step=1.420, train_loss_epoch=1.330, train_auroc_epoch=0.747]\n","Epoch 14:  73% 80/110 [00:05<00:02, 14.15it/s, loss=1.27, v_num=0, val_loss=1.320, val_auroc_epoch=0.756, train_loss_step=1.160, train_loss_epoch=1.330, train_auroc_epoch=0.747]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 14:  91% 100/110 [00:06<00:00, 15.48it/s, loss=1.27, v_num=0, val_loss=1.320, val_auroc_epoch=0.756, train_loss_step=1.160, train_loss_epoch=1.330, train_auroc_epoch=0.747]\n","Validating:  85% 40/47 [00:01<00:00, 25.68it/s]\u001b[A\n","Epoch 14: 100% 110/110 [00:07<00:00, 14.47it/s, loss=1.26, v_num=0, val_loss=1.310, val_auroc_epoch=0.761, train_loss_step=1.050, train_loss_epoch=1.320, train_auroc_epoch=0.752]\n","Epoch 15:  73% 80/110 [00:05<00:02, 14.42it/s, loss=1.27, v_num=0, val_loss=1.310, val_auroc_epoch=0.761, train_loss_step=1.290, train_loss_epoch=1.320, train_auroc_epoch=0.752]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 15:  91% 100/110 [00:06<00:00, 16.20it/s, loss=1.27, v_num=0, val_loss=1.310, val_auroc_epoch=0.761, train_loss_step=1.290, train_loss_epoch=1.320, train_auroc_epoch=0.752]\n","Validating:  85% 40/47 [00:01<00:00, 34.94it/s]\u001b[A\n","Epoch 15: 100% 110/110 [00:06<00:00, 15.81it/s, loss=1.25, v_num=0, val_loss=1.300, val_auroc_epoch=0.763, train_loss_step=1.030, train_loss_epoch=1.310, train_auroc_epoch=0.755]\n","Epoch 16:  73% 80/110 [00:06<00:02, 13.24it/s, loss=1.27, v_num=0, val_loss=1.300, val_auroc_epoch=0.763, train_loss_step=1.260, train_loss_epoch=1.310, train_auroc_epoch=0.755]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 16:  91% 100/110 [00:06<00:00, 14.30it/s, loss=1.27, v_num=0, val_loss=1.300, val_auroc_epoch=0.763, train_loss_step=1.260, train_loss_epoch=1.310, train_auroc_epoch=0.755]\n","Validating:  85% 40/47 [00:01<00:00, 25.43it/s]\u001b[A\n","Epoch 16: 100% 110/110 [00:07<00:00, 13.89it/s, loss=1.24, v_num=0, val_loss=1.300, val_auroc_epoch=0.765, train_loss_step=1.270, train_loss_epoch=1.290, train_auroc_epoch=0.764]\n","Epoch 17:  73% 80/110 [00:05<00:02, 14.71it/s, loss=1.24, v_num=0, val_loss=1.300, val_auroc_epoch=0.765, train_loss_step=1.010, train_loss_epoch=1.290, train_auroc_epoch=0.764]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 17:  91% 100/110 [00:06<00:00, 16.44it/s, loss=1.24, v_num=0, val_loss=1.300, val_auroc_epoch=0.765, train_loss_step=1.010, train_loss_epoch=1.290, train_auroc_epoch=0.764]\n","Validating:  85% 40/47 [00:01<00:00, 34.46it/s]\u001b[A\n","Epoch 17: 100% 110/110 [00:06<00:00, 16.01it/s, loss=1.21, v_num=0, val_loss=1.320, val_auroc_epoch=0.761, train_loss_step=1.260, train_loss_epoch=1.270, train_auroc_epoch=0.772]\n","Epoch 18:  73% 80/110 [00:06<00:02, 12.40it/s, loss=1.25, v_num=0, val_loss=1.320, val_auroc_epoch=0.761, train_loss_step=1.090, train_loss_epoch=1.270, train_auroc_epoch=0.772]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 18:  91% 100/110 [00:07<00:00, 14.10it/s, loss=1.25, v_num=0, val_loss=1.320, val_auroc_epoch=0.761, train_loss_step=1.090, train_loss_epoch=1.270, train_auroc_epoch=0.772]\n","Validating:  85% 40/47 [00:01<00:00, 34.71it/s]\u001b[A\n","Epoch 18: 100% 110/110 [00:07<00:00, 13.97it/s, loss=1.22, v_num=0, val_loss=1.300, val_auroc_epoch=0.765, train_loss_step=1.140, train_loss_epoch=1.280, train_auroc_epoch=0.767]\n","Epoch 19:  73% 80/110 [00:05<00:02, 14.86it/s, loss=1.25, v_num=0, val_loss=1.300, val_auroc_epoch=0.765, train_loss_step=1.350, train_loss_epoch=1.280, train_auroc_epoch=0.767]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 19:  91% 100/110 [00:06<00:00, 16.60it/s, loss=1.25, v_num=0, val_loss=1.300, val_auroc_epoch=0.765, train_loss_step=1.350, train_loss_epoch=1.280, train_auroc_epoch=0.767]\n","Validating:  85% 40/47 [00:01<00:00, 34.57it/s]\u001b[A\n","Epoch 19: 100% 110/110 [00:06<00:00, 16.15it/s, loss=1.21, v_num=0, val_loss=1.300, val_auroc_epoch=0.767, train_loss_step=1.120, train_loss_epoch=1.270, train_auroc_epoch=0.771]\n","Epoch 20:  73% 80/110 [00:06<00:02, 12.59it/s, loss=1.25, v_num=0, val_loss=1.300, val_auroc_epoch=0.767, train_loss_step=1.160, train_loss_epoch=1.270, train_auroc_epoch=0.771]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 20:  91% 100/110 [00:06<00:00, 14.30it/s, loss=1.25, v_num=0, val_loss=1.300, val_auroc_epoch=0.767, train_loss_step=1.160, train_loss_epoch=1.270, train_auroc_epoch=0.771]\n","Validating:  85% 40/47 [00:01<00:00, 34.96it/s]\u001b[A\n","Epoch 20: 100% 110/110 [00:07<00:00, 14.15it/s, loss=1.23, v_num=0, val_loss=1.320, val_auroc_epoch=0.765, train_loss_step=1.260, train_loss_epoch=1.260, train_auroc_epoch=0.776]\n","Epoch 21:  73% 80/110 [00:05<00:02, 14.87it/s, loss=1.21, v_num=0, val_loss=1.320, val_auroc_epoch=0.765, train_loss_step=1.340, train_loss_epoch=1.260, train_auroc_epoch=0.776]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 21:  91% 100/110 [00:06<00:00, 16.61it/s, loss=1.21, v_num=0, val_loss=1.320, val_auroc_epoch=0.765, train_loss_step=1.340, train_loss_epoch=1.260, train_auroc_epoch=0.776]\n","Validating:  85% 40/47 [00:01<00:00, 34.60it/s]\u001b[A\n","Epoch 21: 100% 110/110 [00:06<00:00, 16.17it/s, loss=1.19, v_num=0, val_loss=1.300, val_auroc_epoch=0.768, train_loss_step=0.962, train_loss_epoch=1.260, train_auroc_epoch=0.774]\n","Epoch 22:  73% 80/110 [00:06<00:02, 12.63it/s, loss=1.22, v_num=0, val_loss=1.300, val_auroc_epoch=0.768, train_loss_step=1.110, train_loss_epoch=1.260, train_auroc_epoch=0.774]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 22:  91% 100/110 [00:06<00:00, 14.31it/s, loss=1.22, v_num=0, val_loss=1.300, val_auroc_epoch=0.768, train_loss_step=1.110, train_loss_epoch=1.260, train_auroc_epoch=0.774]\n","Validating:  85% 40/47 [00:01<00:00, 34.17it/s]\u001b[A\n","Epoch 22: 100% 110/110 [00:07<00:00, 14.15it/s, loss=1.23, v_num=0, val_loss=1.270, val_auroc_epoch=0.772, train_loss_step=0.933, train_loss_epoch=1.250, train_auroc_epoch=0.780]\n","Epoch 23:  73% 80/110 [00:05<00:01, 15.11it/s, loss=1.24, v_num=0, val_loss=1.270, val_auroc_epoch=0.772, train_loss_step=1.190, train_loss_epoch=1.250, train_auroc_epoch=0.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 23:  91% 100/110 [00:05<00:00, 16.85it/s, loss=1.24, v_num=0, val_loss=1.270, val_auroc_epoch=0.772, train_loss_step=1.190, train_loss_epoch=1.250, train_auroc_epoch=0.780]\n","Validating:  85% 40/47 [00:01<00:00, 34.56it/s]\u001b[A\n","Epoch 23: 100% 110/110 [00:06<00:00, 16.22it/s, loss=1.23, v_num=0, val_loss=1.280, val_auroc_epoch=0.772, train_loss_step=1.130, train_loss_epoch=1.250, train_auroc_epoch=0.780]\n","Epoch 24:  73% 80/110 [00:06<00:02, 12.67it/s, loss=1.23, v_num=0, val_loss=1.280, val_auroc_epoch=0.772, train_loss_step=1.370, train_loss_epoch=1.250, train_auroc_epoch=0.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 24:  91% 100/110 [00:06<00:00, 14.35it/s, loss=1.23, v_num=0, val_loss=1.280, val_auroc_epoch=0.772, train_loss_step=1.370, train_loss_epoch=1.250, train_auroc_epoch=0.780]\n","Validating:  85% 40/47 [00:01<00:00, 34.44it/s]\u001b[A\n","Epoch 24: 100% 110/110 [00:07<00:00, 14.20it/s, loss=1.18, v_num=0, val_loss=1.290, val_auroc_epoch=0.773, train_loss_step=0.964, train_loss_epoch=1.250, train_auroc_epoch=0.783]\n","Epoch 25:  73% 80/110 [00:05<00:02, 14.96it/s, loss=1.22, v_num=0, val_loss=1.290, val_auroc_epoch=0.773, train_loss_step=1.170, train_loss_epoch=1.250, train_auroc_epoch=0.783]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 25:  91% 100/110 [00:06<00:00, 16.56it/s, loss=1.22, v_num=0, val_loss=1.290, val_auroc_epoch=0.773, train_loss_step=1.170, train_loss_epoch=1.250, train_auroc_epoch=0.783]\n","Validating:  85% 40/47 [00:01<00:00, 29.47it/s]\u001b[A\n","Epoch 25: 100% 110/110 [00:07<00:00, 15.63it/s, loss=1.21, v_num=0, val_loss=1.280, val_auroc_epoch=0.773, train_loss_step=1.220, train_loss_epoch=1.250, train_auroc_epoch=0.781]\n","Epoch 26:  73% 80/110 [00:06<00:02, 13.07it/s, loss=1.2, v_num=0, val_loss=1.280, val_auroc_epoch=0.773, train_loss_step=1.220, train_loss_epoch=1.250, train_auroc_epoch=0.781]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 26:  91% 100/110 [00:06<00:00, 14.77it/s, loss=1.2, v_num=0, val_loss=1.280, val_auroc_epoch=0.773, train_loss_step=1.220, train_loss_epoch=1.250, train_auroc_epoch=0.781]\n","Validating:  85% 40/47 [00:01<00:00, 34.37it/s]\u001b[A\n","Epoch 26: 100% 110/110 [00:07<00:00, 14.56it/s, loss=1.17, v_num=0, val_loss=1.270, val_auroc_epoch=0.775, train_loss_step=0.926, train_loss_epoch=1.230, train_auroc_epoch=0.790]\n","Epoch 27:  73% 80/110 [00:05<00:02, 14.30it/s, loss=1.23, v_num=0, val_loss=1.270, val_auroc_epoch=0.775, train_loss_step=1.520, train_loss_epoch=1.230, train_auroc_epoch=0.790]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 27:  91% 100/110 [00:06<00:00, 15.54it/s, loss=1.23, v_num=0, val_loss=1.270, val_auroc_epoch=0.775, train_loss_step=1.520, train_loss_epoch=1.230, train_auroc_epoch=0.790]\n","Validating:  85% 40/47 [00:01<00:00, 27.05it/s]\u001b[A\n","Epoch 27: 100% 110/110 [00:07<00:00, 14.76it/s, loss=1.21, v_num=0, val_loss=1.300, val_auroc_epoch=0.772, train_loss_step=1.050, train_loss_epoch=1.210, train_auroc_epoch=0.794]\n","Epoch 28:  73% 80/110 [00:05<00:02, 13.70it/s, loss=1.16, v_num=0, val_loss=1.300, val_auroc_epoch=0.772, train_loss_step=1.060, train_loss_epoch=1.210, train_auroc_epoch=0.794]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 28:  91% 100/110 [00:06<00:00, 15.46it/s, loss=1.16, v_num=0, val_loss=1.300, val_auroc_epoch=0.772, train_loss_step=1.060, train_loss_epoch=1.210, train_auroc_epoch=0.794]\n","Validating:  85% 40/47 [00:01<00:00, 34.26it/s]\u001b[A\n","Epoch 28: 100% 110/110 [00:07<00:00, 15.11it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.773, train_loss_step=1.350, train_loss_epoch=1.230, train_auroc_epoch=0.790]\n","Epoch 29:  73% 80/110 [00:05<00:02, 13.58it/s, loss=1.19, v_num=0, val_loss=1.280, val_auroc_epoch=0.773, train_loss_step=1.080, train_loss_epoch=1.230, train_auroc_epoch=0.790]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 29:  91% 100/110 [00:06<00:00, 14.83it/s, loss=1.19, v_num=0, val_loss=1.280, val_auroc_epoch=0.773, train_loss_step=1.080, train_loss_epoch=1.230, train_auroc_epoch=0.790]\n","Validating:  85% 40/47 [00:01<00:00, 24.30it/s]\u001b[A\n","Epoch 29: 100% 110/110 [00:07<00:00, 14.00it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.775, train_loss_step=0.949, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Epoch 30:  73% 80/110 [00:05<00:02, 14.87it/s, loss=1.18, v_num=0, val_loss=1.270, val_auroc_epoch=0.775, train_loss_step=1.220, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 30:  91% 100/110 [00:06<00:00, 16.60it/s, loss=1.18, v_num=0, val_loss=1.270, val_auroc_epoch=0.775, train_loss_step=1.220, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Validating:  85% 40/47 [00:01<00:00, 34.79it/s]\u001b[A\n","Epoch 30: 100% 110/110 [00:06<00:00, 16.17it/s, loss=1.17, v_num=0, val_loss=1.290, val_auroc_epoch=0.773, train_loss_step=1.110, train_loss_epoch=1.210, train_auroc_epoch=0.798]\n","Epoch 31:  73% 80/110 [00:06<00:02, 12.20it/s, loss=1.16, v_num=0, val_loss=1.290, val_auroc_epoch=0.773, train_loss_step=1.080, train_loss_epoch=1.210, train_auroc_epoch=0.798]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 31:  91% 100/110 [00:07<00:00, 13.75it/s, loss=1.16, v_num=0, val_loss=1.290, val_auroc_epoch=0.773, train_loss_step=1.080, train_loss_epoch=1.210, train_auroc_epoch=0.798]\n","Validating:  85% 40/47 [00:01<00:00, 32.80it/s]\u001b[A\n","Epoch 31: 100% 110/110 [00:08<00:00, 13.63it/s, loss=1.15, v_num=0, val_loss=1.290, val_auroc_epoch=0.774, train_loss_step=1.080, train_loss_epoch=1.210, train_auroc_epoch=0.798]\n","Epoch 32:  73% 80/110 [00:05<00:02, 14.68it/s, loss=1.16, v_num=0, val_loss=1.290, val_auroc_epoch=0.774, train_loss_step=0.977, train_loss_epoch=1.210, train_auroc_epoch=0.798]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 32:  91% 100/110 [00:06<00:00, 16.43it/s, loss=1.16, v_num=0, val_loss=1.290, val_auroc_epoch=0.774, train_loss_step=0.977, train_loss_epoch=1.210, train_auroc_epoch=0.798]\n","Validating:  85% 40/47 [00:01<00:00, 34.57it/s]\u001b[A\n","Epoch 32: 100% 110/110 [00:06<00:00, 16.00it/s, loss=1.14, v_num=0, val_loss=1.280, val_auroc_epoch=0.774, train_loss_step=1.140, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Epoch 33:  73% 80/110 [00:06<00:02, 12.53it/s, loss=1.18, v_num=0, val_loss=1.280, val_auroc_epoch=0.774, train_loss_step=1.100, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 33:  91% 100/110 [00:07<00:00, 14.22it/s, loss=1.18, v_num=0, val_loss=1.280, val_auroc_epoch=0.774, train_loss_step=1.100, train_loss_epoch=1.220, train_auroc_epoch=0.793]\n","Validating:  85% 40/47 [00:01<00:00, 34.17it/s]\u001b[A\n","Epoch 33: 100% 110/110 [00:07<00:00, 14.04it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=0.963, train_loss_epoch=1.210, train_auroc_epoch=0.795]\n","Epoch 34:  73% 80/110 [00:06<00:02, 12.67it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=1.160, train_loss_epoch=1.210, train_auroc_epoch=0.795]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 34:  91% 100/110 [00:07<00:00, 14.02it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=1.160, train_loss_epoch=1.210, train_auroc_epoch=0.795]\n","Validating:  85% 40/47 [00:01<00:00, 30.65it/s]\u001b[A\n","Epoch 34: 100% 110/110 [00:07<00:00, 13.88it/s, loss=1.15, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=0.971, train_loss_epoch=1.190, train_auroc_epoch=0.803]\n","Epoch 35:  73% 80/110 [00:06<00:02, 12.53it/s, loss=1.19, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=1.180, train_loss_epoch=1.190, train_auroc_epoch=0.803]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 35:  91% 100/110 [00:07<00:00, 14.24it/s, loss=1.19, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=1.180, train_loss_epoch=1.190, train_auroc_epoch=0.803]\n","Validating:  85% 40/47 [00:01<00:00, 34.77it/s]\u001b[A\n","Epoch 35: 100% 110/110 [00:07<00:00, 14.10it/s, loss=1.19, v_num=0, val_loss=1.290, val_auroc_epoch=0.775, train_loss_step=1.030, train_loss_epoch=1.200, train_auroc_epoch=0.802]\n","Epoch 36:  73% 80/110 [00:05<00:02, 14.84it/s, loss=1.16, v_num=0, val_loss=1.290, val_auroc_epoch=0.775, train_loss_step=1.070, train_loss_epoch=1.200, train_auroc_epoch=0.802]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 36:  91% 100/110 [00:06<00:00, 16.48it/s, loss=1.16, v_num=0, val_loss=1.290, val_auroc_epoch=0.775, train_loss_step=1.070, train_loss_epoch=1.200, train_auroc_epoch=0.802]\n","Validating:  85% 40/47 [00:01<00:00, 29.22it/s]\u001b[A\n","Epoch 36: 100% 110/110 [00:07<00:00, 15.47it/s, loss=1.15, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=0.959, train_loss_epoch=1.190, train_auroc_epoch=0.804]\n","Epoch 37:  73% 80/110 [00:06<00:02, 13.20it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=1.010, train_loss_epoch=1.190, train_auroc_epoch=0.804]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 37:  91% 100/110 [00:06<00:00, 14.95it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=1.010, train_loss_epoch=1.190, train_auroc_epoch=0.804]\n","Validating:  85% 40/47 [00:01<00:00, 34.92it/s]\u001b[A\n","Epoch 37: 100% 110/110 [00:07<00:00, 14.73it/s, loss=1.12, v_num=0, val_loss=1.300, val_auroc_epoch=0.774, train_loss_step=1.140, train_loss_epoch=1.170, train_auroc_epoch=0.812]\n","Epoch 38:  73% 80/110 [00:05<00:02, 14.56it/s, loss=1.19, v_num=0, val_loss=1.300, val_auroc_epoch=0.774, train_loss_step=1.120, train_loss_epoch=1.170, train_auroc_epoch=0.812]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 38:  91% 100/110 [00:06<00:00, 15.88it/s, loss=1.19, v_num=0, val_loss=1.300, val_auroc_epoch=0.774, train_loss_step=1.120, train_loss_epoch=1.170, train_auroc_epoch=0.812]\n","Validating:  85% 40/47 [00:01<00:00, 27.20it/s]\u001b[A\n","Epoch 38: 100% 110/110 [00:07<00:00, 15.06it/s, loss=1.15, v_num=0, val_loss=1.280, val_auroc_epoch=0.775, train_loss_step=0.750, train_loss_epoch=1.200, train_auroc_epoch=0.801]\n","Epoch 39:  73% 80/110 [00:05<00:02, 13.74it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.775, train_loss_step=1.140, train_loss_epoch=1.200, train_auroc_epoch=0.801]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 39:  91% 100/110 [00:06<00:00, 15.46it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.775, train_loss_step=1.140, train_loss_epoch=1.200, train_auroc_epoch=0.801]\n","Validating:  85% 40/47 [00:01<00:00, 34.72it/s]\u001b[A\n","Epoch 39: 100% 110/110 [00:07<00:00, 15.19it/s, loss=1.17, v_num=0, val_loss=1.300, val_auroc_epoch=0.774, train_loss_step=1.280, train_loss_epoch=1.190, train_auroc_epoch=0.805]\n","Epoch 40:  73% 80/110 [00:05<00:02, 14.02it/s, loss=1.15, v_num=0, val_loss=1.300, val_auroc_epoch=0.774, train_loss_step=1.050, train_loss_epoch=1.190, train_auroc_epoch=0.805]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 40:  91% 100/110 [00:06<00:00, 15.36it/s, loss=1.15, v_num=0, val_loss=1.300, val_auroc_epoch=0.774, train_loss_step=1.050, train_loss_epoch=1.190, train_auroc_epoch=0.805]\n","Validating:  85% 40/47 [00:01<00:00, 25.78it/s]\u001b[A\n","Epoch 40: 100% 110/110 [00:07<00:00, 14.40it/s, loss=1.15, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=0.824, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Epoch 41:  73% 80/110 [00:05<00:02, 14.69it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=1.200, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 41:  91% 100/110 [00:06<00:00, 16.44it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=1.200, train_loss_epoch=1.180, train_auroc_epoch=0.805]\n","Validating:  85% 40/47 [00:01<00:00, 34.87it/s]\u001b[A\n","Epoch 41: 100% 110/110 [00:06<00:00, 16.03it/s, loss=1.14, v_num=0, val_loss=1.290, val_auroc_epoch=0.775, train_loss_step=1.190, train_loss_epoch=1.190, train_auroc_epoch=0.806]\n","Epoch 42:  73% 80/110 [00:06<00:02, 13.14it/s, loss=1.18, v_num=0, val_loss=1.290, val_auroc_epoch=0.775, train_loss_step=1.180, train_loss_epoch=1.190, train_auroc_epoch=0.806]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 42:  91% 100/110 [00:07<00:00, 14.25it/s, loss=1.18, v_num=0, val_loss=1.290, val_auroc_epoch=0.775, train_loss_step=1.180, train_loss_epoch=1.190, train_auroc_epoch=0.806]\n","Validating:  85% 40/47 [00:01<00:00, 26.49it/s]\u001b[A\n","Epoch 42: 100% 110/110 [00:07<00:00, 13.92it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=1.110, train_loss_epoch=1.190, train_auroc_epoch=0.806]\n","Epoch 43:  73% 80/110 [00:05<00:02, 14.76it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=1.290, train_loss_epoch=1.190, train_auroc_epoch=0.806]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 43:  91% 100/110 [00:06<00:00, 16.55it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.777, train_loss_step=1.290, train_loss_epoch=1.190, train_auroc_epoch=0.806]\n","Validating:  85% 40/47 [00:01<00:00, 35.33it/s]\u001b[A\n","Epoch 43: 100% 110/110 [00:06<00:00, 16.12it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=0.900, train_loss_epoch=1.150, train_auroc_epoch=0.817]\n","Epoch 44:  73% 80/110 [00:06<00:02, 12.61it/s, loss=1.16, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.817]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 44:  91% 100/110 [00:06<00:00, 14.30it/s, loss=1.16, v_num=0, val_loss=1.280, val_auroc_epoch=0.776, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.817]\n","Validating:  85% 40/47 [00:01<00:00, 34.43it/s]\u001b[A\n","Validating: 100% 47/47 [00:01<00:00, 34.86it/s]\u001b[AEpoch 00045: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 44: 100% 110/110 [00:07<00:00, 14.11it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.777, train_loss_step=1.010, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Epoch 45:  73% 80/110 [00:05<00:02, 14.92it/s, loss=1.16, v_num=0, val_loss=1.280, val_auroc_epoch=0.777, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 45:  91% 100/110 [00:06<00:00, 16.60it/s, loss=1.16, v_num=0, val_loss=1.280, val_auroc_epoch=0.777, train_loss_step=1.110, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating:  85% 40/47 [00:01<00:00, 34.38it/s]\u001b[A\n","Epoch 45: 100% 110/110 [00:06<00:00, 16.19it/s, loss=1.16, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.380, train_loss_epoch=1.160, train_auroc_epoch=0.816]\n","Epoch 46:  73% 80/110 [00:06<00:02, 12.59it/s, loss=1.12, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.030, train_loss_epoch=1.160, train_auroc_epoch=0.816]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 46:  91% 100/110 [00:06<00:00, 14.30it/s, loss=1.12, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.030, train_loss_epoch=1.160, train_auroc_epoch=0.816]\n","Validating:  85% 40/47 [00:01<00:00, 35.08it/s]\u001b[A\n","Epoch 46: 100% 110/110 [00:07<00:00, 14.19it/s, loss=1.1, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.130, train_loss_epoch=1.130, train_auroc_epoch=0.825] \n","Epoch 47:  73% 80/110 [00:05<00:01, 15.00it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.090, train_loss_epoch=1.130, train_auroc_epoch=0.825]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 47:  91% 100/110 [00:05<00:00, 16.75it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.090, train_loss_epoch=1.130, train_auroc_epoch=0.825]\n","Validating:  85% 40/47 [00:01<00:00, 34.85it/s]\u001b[A\n","Epoch 47: 100% 110/110 [00:06<00:00, 16.28it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.030, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Epoch 48:  73% 80/110 [00:06<00:02, 12.49it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.100, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 48:  91% 100/110 [00:07<00:00, 14.21it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.100, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Validating:  85% 40/47 [00:01<00:00, 34.76it/s]\u001b[A\n","Epoch 48: 100% 110/110 [00:07<00:00, 14.07it/s, loss=1.14, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.120, train_loss_epoch=1.140, train_auroc_epoch=0.822]\n","Epoch 49:  73% 80/110 [00:05<00:02, 14.87it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.050, train_loss_epoch=1.140, train_auroc_epoch=0.822]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 49:  91% 100/110 [00:06<00:00, 16.60it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.050, train_loss_epoch=1.140, train_auroc_epoch=0.822]\n","Validating:  85% 40/47 [00:01<00:00, 33.58it/s]\u001b[A\n","Epoch 49: 100% 110/110 [00:06<00:00, 16.09it/s, loss=1.08, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=0.981, train_loss_epoch=1.130, train_auroc_epoch=0.826]\n","Epoch 50:  73% 80/110 [00:06<00:02, 12.04it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=0.954, train_loss_epoch=1.130, train_auroc_epoch=0.826]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 50:  91% 100/110 [00:07<00:00, 13.72it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=0.954, train_loss_epoch=1.130, train_auroc_epoch=0.826]\n","Validating:  85% 40/47 [00:01<00:00, 34.71it/s]\u001b[A\n","Epoch 50: 100% 110/110 [00:08<00:00, 13.62it/s, loss=1.17, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.230, train_loss_epoch=1.130, train_auroc_epoch=0.826]\n","Epoch 51:  73% 80/110 [00:05<00:02, 14.88it/s, loss=1.12, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=0.971, train_loss_epoch=1.130, train_auroc_epoch=0.826]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 51:  91% 100/110 [00:06<00:00, 16.44it/s, loss=1.12, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=0.971, train_loss_epoch=1.130, train_auroc_epoch=0.826]\n","Validating:  85% 40/47 [00:01<00:00, 29.93it/s]\u001b[A\n","Epoch 51: 100% 110/110 [00:07<00:00, 15.56it/s, loss=1.11, v_num=0, val_loss=1.270, val_auroc_epoch=0.780, train_loss_step=1.060, train_loss_epoch=1.140, train_auroc_epoch=0.822]\n","Epoch 52:  73% 80/110 [00:06<00:02, 13.27it/s, loss=1.12, v_num=0, val_loss=1.270, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.140, train_auroc_epoch=0.822]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 52:  91% 100/110 [00:06<00:00, 14.98it/s, loss=1.12, v_num=0, val_loss=1.270, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.140, train_auroc_epoch=0.822]\n","Validating:  85% 40/47 [00:01<00:00, 34.32it/s]\u001b[A\n","Epoch 52: 100% 110/110 [00:07<00:00, 14.73it/s, loss=1.12, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=0.980, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Epoch 53:  73% 80/110 [00:05<00:02, 14.42it/s, loss=1.12, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.020, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 53:  91% 100/110 [00:06<00:00, 15.70it/s, loss=1.12, v_num=0, val_loss=1.280, val_auroc_epoch=0.778, train_loss_step=1.020, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating:  85% 40/47 [00:01<00:00, 27.63it/s]\u001b[A\n","Epoch 53: 100% 110/110 [00:07<00:00, 14.92it/s, loss=1.11, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.110, train_loss_epoch=1.130, train_auroc_epoch=0.823]\n","Epoch 54:  73% 80/110 [00:05<00:02, 13.92it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.130, train_loss_epoch=1.130, train_auroc_epoch=0.823]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 54:  91% 100/110 [00:06<00:00, 15.69it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.779, train_loss_step=1.130, train_loss_epoch=1.130, train_auroc_epoch=0.823]\n","Validating:  85% 40/47 [00:01<00:00, 34.96it/s]\u001b[A\n","Epoch 54: 100% 110/110 [00:07<00:00, 15.36it/s, loss=1.12, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=1.040, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Epoch 55:  73% 80/110 [00:05<00:02, 13.93it/s, loss=1.18, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=1.150, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 55:  91% 100/110 [00:06<00:00, 15.25it/s, loss=1.18, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=1.150, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Validating:  85% 40/47 [00:01<00:00, 25.57it/s]\u001b[A\n","Epoch 55: 100% 110/110 [00:07<00:00, 14.30it/s, loss=1.17, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=0.894, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Epoch 56:  73% 80/110 [00:05<00:02, 14.64it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=1.070, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 56:  91% 100/110 [00:06<00:00, 16.37it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=1.070, train_loss_epoch=1.140, train_auroc_epoch=0.821]\n","Validating:  85% 40/47 [00:01<00:00, 34.40it/s]\u001b[A\n","Epoch 56: 100% 110/110 [00:06<00:00, 15.95it/s, loss=1.09, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=1.060, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Epoch 57:  73% 80/110 [00:06<00:02, 13.28it/s, loss=1.15, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=1.150, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 57:  91% 100/110 [00:06<00:00, 14.32it/s, loss=1.15, v_num=0, val_loss=1.280, val_auroc_epoch=0.779, train_loss_step=1.150, train_loss_epoch=1.150, train_auroc_epoch=0.818]\n","Validating:  85% 40/47 [00:01<00:00, 26.08it/s]\u001b[A\n","Validating: 100% 47/47 [00:01<00:00, 28.36it/s]\u001b[AEpoch 00058: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 57: 100% 110/110 [00:07<00:00, 13.99it/s, loss=1.14, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.270, train_loss_epoch=1.130, train_auroc_epoch=0.825]\n","Epoch 58:  73% 80/110 [00:05<00:02, 14.70it/s, loss=1.14, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.110, train_loss_epoch=1.130, train_auroc_epoch=0.825]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 58:  91% 100/110 [00:06<00:00, 16.42it/s, loss=1.14, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.110, train_loss_epoch=1.130, train_auroc_epoch=0.825]\n","Validating:  85% 40/47 [00:01<00:00, 34.66it/s]\u001b[A\n","Epoch 58: 100% 110/110 [00:06<00:00, 16.02it/s, loss=1.14, v_num=0, val_loss=1.270, val_auroc_epoch=0.781, train_loss_step=0.942, train_loss_epoch=1.140, train_auroc_epoch=0.824]\n","Epoch 59:  73% 80/110 [00:06<00:02, 12.44it/s, loss=1.08, v_num=0, val_loss=1.270, val_auroc_epoch=0.781, train_loss_step=1.330, train_loss_epoch=1.140, train_auroc_epoch=0.824]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 59:  91% 100/110 [00:07<00:00, 14.08it/s, loss=1.08, v_num=0, val_loss=1.270, val_auroc_epoch=0.781, train_loss_step=1.330, train_loss_epoch=1.140, train_auroc_epoch=0.824]\n","Validating:  85% 40/47 [00:01<00:00, 33.77it/s]\u001b[A\n","Epoch 59: 100% 110/110 [00:07<00:00, 13.91it/s, loss=1.09, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.240, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Epoch 60:  73% 80/110 [00:05<00:02, 14.77it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.340, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 60:  91% 100/110 [00:06<00:00, 16.48it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.340, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating:  85% 40/47 [00:01<00:00, 34.17it/s]\u001b[A\n","Epoch 60: 100% 110/110 [00:06<00:00, 16.01it/s, loss=1.12, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.270, train_loss_epoch=1.120, train_auroc_epoch=0.829]\n","Epoch 61:  73% 80/110 [00:06<00:02, 12.24it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.170, train_loss_epoch=1.120, train_auroc_epoch=0.829]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 61:  91% 100/110 [00:07<00:00, 13.94it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.170, train_loss_epoch=1.120, train_auroc_epoch=0.829]\n","Validating:  85% 40/47 [00:01<00:00, 34.03it/s]\u001b[A\n","Epoch 61: 100% 110/110 [00:07<00:00, 13.77it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.919, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Epoch 62:  73% 80/110 [00:05<00:02, 13.69it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.210, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 62:  91% 100/110 [00:06<00:00, 15.05it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.210, train_loss_epoch=1.120, train_auroc_epoch=0.828]\n","Validating:  85% 40/47 [00:01<00:00, 25.61it/s]\u001b[A\n","Epoch 62: 100% 110/110 [00:07<00:00, 14.03it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.030, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Epoch 63:  73% 80/110 [00:06<00:02, 11.81it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.100, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 63:  91% 100/110 [00:07<00:00, 13.45it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.100, train_loss_epoch=1.110, train_auroc_epoch=0.831]\n","Validating:  85% 40/47 [00:01<00:00, 34.10it/s]\u001b[A\n","Epoch 63: 100% 110/110 [00:08<00:00, 13.36it/s, loss=1.07, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.934, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Epoch 64:  73% 80/110 [00:05<00:02, 14.64it/s, loss=1.09, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.140, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 64:  91% 100/110 [00:06<00:00, 15.89it/s, loss=1.09, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.140, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating:  85% 40/47 [00:01<00:00, 27.03it/s]\u001b[A\n","Epoch 64: 100% 110/110 [00:07<00:00, 15.03it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.440, train_loss_epoch=1.110, train_auroc_epoch=0.833] \n","Epoch 65:  73% 80/110 [00:06<00:02, 13.12it/s, loss=1.09, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 65:  91% 100/110 [00:06<00:00, 14.84it/s, loss=1.09, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Validating:  85% 40/47 [00:01<00:00, 34.11it/s]\u001b[A\n","Epoch 65: 100% 110/110 [00:07<00:00, 14.54it/s, loss=1.07, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.020, train_loss_epoch=1.110, train_auroc_epoch=0.829]\n","Epoch 66:  73% 80/110 [00:05<00:02, 13.54it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.100, train_loss_epoch=1.110, train_auroc_epoch=0.829]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 66:  91% 100/110 [00:06<00:00, 14.84it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.100, train_loss_epoch=1.110, train_auroc_epoch=0.829]\n","Validating:  85% 40/47 [00:01<00:00, 25.19it/s]\u001b[A\n","Epoch 66: 100% 110/110 [00:07<00:00, 13.96it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.781, train_loss_step=0.913, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Epoch 67:  73% 80/110 [00:05<00:02, 14.39it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.781, train_loss_step=1.240, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 67:  91% 100/110 [00:06<00:00, 16.12it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.781, train_loss_step=1.240, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating:  85% 40/47 [00:01<00:00, 34.10it/s]\u001b[A\n","Epoch 67: 100% 110/110 [00:07<00:00, 15.71it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.956, train_loss_epoch=1.100, train_auroc_epoch=0.836] \n","Epoch 68:  73% 80/110 [00:05<00:02, 13.41it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.992, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 68:  91% 100/110 [00:06<00:00, 14.43it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.992, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 25.30it/s]\u001b[A\n","Validating: 100% 47/47 [00:01<00:00, 27.50it/s]\u001b[AEpoch 00069: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 68: 100% 110/110 [00:07<00:00, 13.99it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.861, train_loss_epoch=1.120, train_auroc_epoch=0.830] \n","Epoch 69:  73% 80/110 [00:05<00:02, 14.71it/s, loss=1.07, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.880, train_loss_epoch=1.120, train_auroc_epoch=0.830]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 69:  91% 100/110 [00:06<00:00, 16.45it/s, loss=1.07, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.880, train_loss_epoch=1.120, train_auroc_epoch=0.830]\n","Validating:  85% 40/47 [00:01<00:00, 34.81it/s]\u001b[A\n","Epoch 69: 100% 110/110 [00:06<00:00, 16.04it/s, loss=1.05, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.792, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 70:  73% 80/110 [00:06<00:02, 11.96it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.120, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 70:  91% 100/110 [00:07<00:00, 13.63it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.120, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 34.18it/s]\u001b[A\n","Epoch 70: 100% 110/110 [00:08<00:00, 13.53it/s, loss=1.04, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.771, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Epoch 71:  73% 80/110 [00:05<00:02, 14.66it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.210, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 71:  91% 100/110 [00:06<00:00, 16.29it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.210, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Validating:  85% 40/47 [00:01<00:00, 33.05it/s]\u001b[A\n","Epoch 71: 100% 110/110 [00:06<00:00, 15.82it/s, loss=1.13, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.010, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 72:  73% 80/110 [00:06<00:02, 12.42it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.985, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 72:  91% 100/110 [00:07<00:00, 14.12it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.985, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 34.87it/s]\u001b[A\n","Epoch 72: 100% 110/110 [00:07<00:00, 13.98it/s, loss=1.06, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.957, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Epoch 73:  73% 80/110 [00:05<00:02, 14.88it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.030, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 73:  91% 100/110 [00:06<00:00, 16.56it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.030, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Validating:  85% 40/47 [00:01<00:00, 33.92it/s]\u001b[A\n","Epoch 73: 100% 110/110 [00:06<00:00, 16.11it/s, loss=1.09, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.896, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Epoch 74:  73% 80/110 [00:06<00:02, 12.34it/s, loss=1.05, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.140, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 74:  91% 100/110 [00:07<00:00, 14.04it/s, loss=1.05, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.140, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Validating:  85% 40/47 [00:01<00:00, 34.21it/s]\u001b[A\n","Epoch 74: 100% 110/110 [00:07<00:00, 13.88it/s, loss=1.05, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.160, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Epoch 75:  73% 80/110 [00:05<00:02, 14.81it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.000, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 75:  91% 100/110 [00:06<00:00, 16.51it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.000, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating:  85% 40/47 [00:01<00:00, 32.00it/s]\u001b[A\n","Epoch 75: 100% 110/110 [00:07<00:00, 15.66it/s, loss=1.05, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.846, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 76:  73% 80/110 [00:06<00:02, 12.84it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 76:  91% 100/110 [00:06<00:00, 14.57it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 35.13it/s]\u001b[A\n","Epoch 76: 100% 110/110 [00:07<00:00, 14.42it/s, loss=1.09, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=1.150, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Epoch 77:  73% 80/110 [00:05<00:02, 14.89it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=1.130, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 77:  91% 100/110 [00:06<00:00, 16.22it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=1.130, train_loss_epoch=1.090, train_auroc_epoch=0.839]\n","Validating:  85% 40/47 [00:01<00:00, 27.83it/s]\u001b[A\n","Epoch 77: 100% 110/110 [00:07<00:00, 15.31it/s, loss=1.08, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.040, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Epoch 78:  73% 80/110 [00:06<00:02, 13.28it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.060, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 78:  91% 100/110 [00:06<00:00, 15.00it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.060, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Validating:  85% 40/47 [00:01<00:00, 34.44it/s]\u001b[A\n","Epoch 78: 100% 110/110 [00:07<00:00, 14.74it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.210, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Epoch 79:  73% 80/110 [00:05<00:02, 14.39it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.020, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 79:  91% 100/110 [00:06<00:00, 15.62it/s, loss=1.11, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=1.020, train_loss_epoch=1.110, train_auroc_epoch=0.832]\n","Validating:  85% 40/47 [00:01<00:00, 26.93it/s]\u001b[A\n","Validating: 100% 47/47 [00:01<00:00, 27.30it/s]\u001b[AEpoch 00080: reducing learning rate of group 0 to 6.2500e-06.\n","Epoch 79: 100% 110/110 [00:07<00:00, 14.76it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=0.996, train_loss_epoch=1.100, train_auroc_epoch=0.836] \n","Epoch 80:  73% 80/110 [00:05<00:02, 13.97it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=1.000, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 80:  91% 100/110 [00:06<00:00, 15.71it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.779, train_loss_step=1.000, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 34.73it/s]\u001b[A\n","Epoch 80: 100% 110/110 [00:07<00:00, 15.38it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.040, train_loss_epoch=1.100, train_auroc_epoch=0.833]\n","Epoch 81:  73% 80/110 [00:05<00:02, 13.72it/s, loss=1.11, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.140, train_loss_epoch=1.100, train_auroc_epoch=0.833]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 81:  91% 100/110 [00:06<00:00, 14.92it/s, loss=1.11, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.140, train_loss_epoch=1.100, train_auroc_epoch=0.833]\n","Validating:  85% 40/47 [00:01<00:00, 23.73it/s]\u001b[A\n","Epoch 81: 100% 110/110 [00:07<00:00, 13.96it/s, loss=1.09, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.895, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Epoch 82:  73% 80/110 [00:05<00:02, 14.50it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.835, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 82:  91% 100/110 [00:06<00:00, 16.21it/s, loss=1.1, v_num=0, val_loss=1.280, val_auroc_epoch=0.780, train_loss_step=0.835, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Validating:  85% 40/47 [00:01<00:00, 34.25it/s]\u001b[A\n","Epoch 82: 100% 110/110 [00:06<00:00, 15.81it/s, loss=1.07, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.933, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Epoch 83:  73% 80/110 [00:06<00:02, 12.94it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.160, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 83:  91% 100/110 [00:07<00:00, 14.13it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.160, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating:  85% 40/47 [00:01<00:00, 29.19it/s]\u001b[A\n","Epoch 83: 100% 110/110 [00:07<00:00, 13.96it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.788, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 84:  73% 80/110 [00:05<00:02, 14.70it/s, loss=1.06, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.080, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 84:  91% 100/110 [00:06<00:00, 16.43it/s, loss=1.06, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.080, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 34.15it/s]\u001b[A\n","Epoch 84: 100% 110/110 [00:06<00:00, 16.00it/s, loss=1.06, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.920, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Epoch 85:  73% 80/110 [00:06<00:02, 12.53it/s, loss=1.09, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.040, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 85:  91% 100/110 [00:07<00:00, 14.22it/s, loss=1.09, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.040, train_loss_epoch=1.080, train_auroc_epoch=0.841]\n","Validating:  85% 40/47 [00:01<00:00, 34.56it/s]\u001b[A\n","Epoch 85: 100% 110/110 [00:07<00:00, 14.08it/s, loss=1.07, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.010, train_loss_epoch=1.100, train_auroc_epoch=0.834]\n","Epoch 86:  73% 80/110 [00:05<00:02, 14.68it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.210, train_loss_epoch=1.100, train_auroc_epoch=0.834]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 86:  91% 100/110 [00:06<00:00, 16.38it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.210, train_loss_epoch=1.100, train_auroc_epoch=0.834]\n","Validating:  85% 40/47 [00:01<00:00, 34.44it/s]\u001b[A\n","Epoch 86: 100% 110/110 [00:06<00:00, 15.98it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.170, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Epoch 87:  73% 80/110 [00:06<00:02, 12.57it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.200, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 87:  91% 100/110 [00:06<00:00, 14.29it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.200, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Validating:  85% 40/47 [00:01<00:00, 34.57it/s]\u001b[A\n","Epoch 87: 100% 110/110 [00:07<00:00, 14.13it/s, loss=1.11, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 88:  73% 80/110 [00:05<00:02, 14.73it/s, loss=1.11, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.843, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 88:  91% 100/110 [00:06<00:00, 16.43it/s, loss=1.11, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.843, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 34.37it/s]\u001b[A\n","Epoch 88: 100% 110/110 [00:06<00:00, 16.00it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.930, train_loss_epoch=1.090, train_auroc_epoch=0.838] \n","Epoch 89:  73% 80/110 [00:06<00:02, 12.49it/s, loss=1.09, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.080, train_loss_epoch=1.090, train_auroc_epoch=0.838]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 89:  91% 100/110 [00:07<00:00, 14.19it/s, loss=1.09, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.080, train_loss_epoch=1.090, train_auroc_epoch=0.838]\n","Validating:  85% 40/47 [00:01<00:00, 34.42it/s]\u001b[A\n","Epoch 89: 100% 110/110 [00:07<00:00, 14.04it/s, loss=1.07, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.915, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Epoch 90:  73% 80/110 [00:05<00:02, 13.44it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 90:  91% 100/110 [00:06<00:00, 14.46it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Validating:  85% 40/47 [00:01<00:00, 22.04it/s]\u001b[A\n","Validating: 100% 47/47 [00:02<00:00, 21.25it/s]\u001b[AEpoch 00091: reducing learning rate of group 0 to 3.1250e-06.\n","Epoch 90: 100% 110/110 [00:08<00:00, 13.27it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.070, train_loss_epoch=1.090, train_auroc_epoch=0.837] \n","Epoch 91:  73% 80/110 [00:06<00:02, 11.96it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.140, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 91:  91% 100/110 [00:07<00:00, 13.64it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.140, train_loss_epoch=1.090, train_auroc_epoch=0.837]\n","Validating:  85% 40/47 [00:01<00:00, 33.99it/s]\u001b[A\n","Epoch 91: 100% 110/110 [00:08<00:00, 13.52it/s, loss=1.15, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.510, train_loss_epoch=1.100, train_auroc_epoch=0.834]\n","Epoch 92:  73% 80/110 [00:05<00:02, 13.64it/s, loss=1.06, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.888, train_loss_epoch=1.100, train_auroc_epoch=0.834]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 92:  91% 100/110 [00:06<00:00, 14.90it/s, loss=1.06, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.888, train_loss_epoch=1.100, train_auroc_epoch=0.834]\n","Validating:  85% 40/47 [00:01<00:00, 25.99it/s]\u001b[A\n","Epoch 92: 100% 110/110 [00:07<00:00, 14.02it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.170, train_loss_epoch=1.090, train_auroc_epoch=0.840]\n","Epoch 93:  73% 80/110 [00:05<00:02, 14.11it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.230, train_loss_epoch=1.090, train_auroc_epoch=0.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 93:  91% 100/110 [00:06<00:00, 15.86it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.230, train_loss_epoch=1.090, train_auroc_epoch=0.840]\n","Validating:  85% 40/47 [00:01<00:00, 34.83it/s]\u001b[A\n","Epoch 93: 100% 110/110 [00:07<00:00, 15.51it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.320, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Epoch 94:  73% 80/110 [00:06<00:02, 13.22it/s, loss=1.07, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.200, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 94:  91% 100/110 [00:06<00:00, 14.31it/s, loss=1.07, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.200, train_loss_epoch=1.100, train_auroc_epoch=0.835]\n","Validating:  85% 40/47 [00:01<00:00, 25.35it/s]\u001b[A\n","Epoch 94: 100% 110/110 [00:07<00:00, 13.87it/s, loss=1.06, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Epoch 95:  73% 80/110 [00:05<00:02, 14.98it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.030, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 95:  91% 100/110 [00:05<00:00, 16.72it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.030, train_loss_epoch=1.100, train_auroc_epoch=0.837]\n","Validating:  85% 40/47 [00:01<00:00, 34.36it/s]\u001b[A\n","Epoch 95: 100% 110/110 [00:06<00:00, 16.24it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.020, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 96:  73% 80/110 [00:06<00:02, 12.18it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 96:  91% 100/110 [00:07<00:00, 13.86it/s, loss=1.08, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.090, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 34.36it/s]\u001b[A\n","Epoch 96: 100% 110/110 [00:08<00:00, 13.74it/s, loss=1.09, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.010, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Epoch 97:  73% 80/110 [00:05<00:02, 14.84it/s, loss=1.11, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.996, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 97:  91% 100/110 [00:06<00:00, 16.57it/s, loss=1.11, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=0.996, train_loss_epoch=1.100, train_auroc_epoch=0.836]\n","Validating:  85% 40/47 [00:01<00:00, 34.34it/s]\u001b[A\n","Epoch 97: 100% 110/110 [00:06<00:00, 16.11it/s, loss=1.11, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.120, train_loss_epoch=1.090, train_auroc_epoch=0.840]\n","Epoch 98:  73% 80/110 [00:06<00:02, 12.32it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.170, train_loss_epoch=1.090, train_auroc_epoch=0.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 98:  91% 100/110 [00:07<00:00, 14.03it/s, loss=1.12, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.170, train_loss_epoch=1.090, train_auroc_epoch=0.840]\n","Validating:  85% 40/47 [00:01<00:00, 34.38it/s]\u001b[A\n","Epoch 98: 100% 110/110 [00:07<00:00, 13.88it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.050, train_loss_epoch=1.090, train_auroc_epoch=0.838] \n","Epoch 99:  73% 80/110 [00:05<00:02, 14.81it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.230, train_loss_epoch=1.090, train_auroc_epoch=0.838]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/47 [00:00<?, ?it/s]\u001b[A\n","Epoch 99:  91% 100/110 [00:06<00:00, 16.56it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.230, train_loss_epoch=1.090, train_auroc_epoch=0.838]\n","Validating:  85% 40/47 [00:01<00:00, 33.94it/s]\u001b[A\n","Epoch 99: 100% 110/110 [00:06<00:00, 16.08it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.270, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","Epoch 99: 100% 110/110 [00:06<00:00, 16.06it/s, loss=1.1, v_num=0, val_loss=1.290, val_auroc_epoch=0.780, train_loss_step=1.270, train_loss_epoch=1.110, train_auroc_epoch=0.833]\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 63/63 [00:02<00:00, 29.47it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test_auroc_best_epoch': 0.7717625498771667, 'test_loss': 1.2778334617614746}\n","--------------------------------------------------------------------------------\n","Path to best model found during training: \n","/content/outputs/2023-06-12/04-10-44/lightning_logs/version_0/checkpoints/46-2960.ckpt\n"]}]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"gvrjFwH2JpA-"}},{"cell_type":"code","source":["pretrained_checkpoint = \"/content/outputs/2023-06-11/20-03-11/lightning_logs/version_0/checkpoints/56-398.ckpt\""],"metadata":{"id":"pbNGb90_qzMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python /content/saint/predict.py experiment=predict \\\n","  experiment.model=saint \\\n","  data=bank_sup \\\n","  data.data_folder=/content/saint/data \\\n","  experiment.pretrained_checkpoint={pretrained_checkpoint} \\\n","  experiment.pred_sav_path=/content/predict.csv"],"metadata":{"id":"2BkcD_bOHJlX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686514897120,"user_tz":-420,"elapsed":10723,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"82f1be31-e3c8-4d4d-93ae-063477d8e21e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-11 20:21:32.260458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-11 20:21:33.540769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/content/saint/predict.py:15: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"configs\", config_name=\"config\")\n","/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","{'seed': 1234, 'transformer': {'num_layers': 6, 'num_heads': 8, 'dropout': 0.1, 'dropout_ff': 0.1, 'embed_dim': 32, 'd_ff': 32, 'cls_token_idx': 0}, 'augmentation': {'prob_cutmix': 0.3, 'alpha': 0.2, 'lambda_pt': 10}, 'optimizer': {'temperature': 0.7, 'proj_head_dim': 128, 'beta_1': 0.9, 'beta_2': 0.99, 'lr': 0.0001, 'weight_decay': 0.01, 'optim': 'adamw', 'metric': 'auroc'}, 'preproc': {'data_folder': None, 'train_split': 0.65, 'validation_split': 0.15, 'test_split': 0.2, 'num_supervised_train_data': None}, 'callback': {'monitor': 'val_loss', 'mode': 'min', 'auto_insert_metric_name': False}, 'trainer': {'max_epochs': 100, 'deterministic': True, 'default_root_dir': None}, 'dataloader': {'shuffle_val': False, 'train_bs': 32, 'val_bs': 32, 'test_bs': 16, 'num_workers': 2, 'pin_memory': False}, 'metric': '${optimizer.metric}', 'print_config': False, 'experiment': {'model': 'saint', 'task': 'classification', 'pretrained_checkpoint': '/content/outputs/2023-06-11/20-03-11/lightning_logs/version_0/checkpoints/56-398.ckpt', 'num_output': 1, 'pred_sav_path': '/content/predict.csv', 'save_prediction': True, 'id_col': 'index', 'target_col': 'target'}, 'data': {'data_folder': '/content/saint/data', 'data_paths': {'train_csv_path': '${data.data_folder}/train.csv', 'train_y_csv_path': '${data.data_folder}/train_y.csv', 'val_csv_path': '${data.data_folder}/val.csv', 'val_y_csv_path': '${data.data_folder}/val_y.csv', 'test_csv_path': '${data.data_folder}/test.csv', 'test_y_csv_path': '${data.data_folder}/test_y.csv'}, 'data_stats': {'no_cat': 1, 'no_num': 49, 'cats': [1]}}}\n","/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)\n","Prediction finished,  csv saved at /content/predict.csv\n"]}]},{"cell_type":"code","source":["pred = pd.read_csv(\"/content/predict.csv\")\n","pred['target'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkms-sdgJFYA","executionInfo":{"status":"ok","timestamp":1686515329523,"user_tz":-420,"elapsed":1064,"user":{"displayName":"Mar Ammar","userId":"11100875506678644540"}},"outputId":"e5726bc0-423d-45da-d0f0-3dd93a68dac3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    160\n","1     40\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":[],"metadata":{"id":"V9pdLifpJ0ug"},"execution_count":null,"outputs":[]}]}